{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e881cda3-9dd2-466e-ad83-f21b3eee3c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "    data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "    lora_dir = '/root/autodl-tmp/checkpoints/previous/glm3-Rank64_27000' #'/root/autodl-tmp/checkpoints/glm3-3-dataset-Rank64'\n",
    "    MAX_TURNS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd3b0ed-c4ae-4e10-a7c4-6e438cffeac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/root/miniconda3/lib/python3.8/site-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mdtex2html\n",
    "import gradio as gr\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import model_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4f440d-e753-4645-b811-44e36fc785dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from threading import Thread\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48dff910-f69f-4143-abea-6b804f0da8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_LICENSE                     pytorch_model-00005-of-00007.bin\n",
      "README.md                         pytorch_model-00006-of-00007.bin\n",
      "config.json                       pytorch_model-00007-of-00007.bin\n",
      "configuration_chatglm.py          pytorch_model.bin.index.json\n",
      "modeling_chatglm.py               quantization.py\n",
      "pytorch_model-00001-of-00007.bin  tokenization_chatglm.py\n",
      "pytorch_model-00002-of-00007.bin  tokenizer.model\n",
      "pytorch_model-00003-of-00007.bin  tokenizer_config.json\n",
      "pytorch_model-00004-of-00007.bin\n"
     ]
    }
   ],
   "source": [
    "ls $CFG.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec82b29-4852-4edd-a6d3-d76f8d540f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008867263793945312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd0754180e40f2b2d8e797a6333a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "#tokenization_chatglm.py\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "#model = model_tools.merge_lora(CFG.model_path, CFG.lora_dir)\n",
    "model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().half()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cf1211-cdaa-4db3-a33a-8565f8df5cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [0, 2]#[0, 2, 31002, 64795]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\" + line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict(history, max_length, repetition_penalty, temperature):\n",
    "    print(1)\n",
    "    stop = StopOnTokens()\n",
    "    messages = []\n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "    print(\"\\n\\n====conversation====\\n\", messages)\n",
    "    print(messages)\n",
    "    model_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                                 add_generation_prompt=False,\n",
    "                                                 tokenize=True,\n",
    "                                                 return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    print(model_inputs)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": temperature,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    for new_token in streamer:\n",
    "        if new_token != '':\n",
    "            history[-1][1] += new_token\n",
    "            yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695a5518-5209-4600-9515-5ebf0aa5d0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://233c105eb9e5ade4b3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://233c105eb9e5ade4b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '当别人不注意我的时候，我就很愤怒，该怎么办？如题。我想做一个不用那么去争取别人注意力，又能被别人看到的人。<br>比如说，开会的时候，大家会讨论，而忽略我；或者新的环境里，有很多小团体，他们也不会主动叫上我。<br>一开始是尴尬，时间长了次数多了，我开始感受到被忽略的愤怒，知道自己不应该这样，可是就是好生气，该怎么办呢？'}]\n",
      "[{'role': 'user', 'content': '当别人不注意我的时候，我就很愤怒，该怎么办？如题。我想做一个不用那么去争取别人注意力，又能被别人看到的人。<br>比如说，开会的时候，大家会讨论，而忽略我；或者新的环境里，有很多小团体，他们也不会主动叫上我。<br>一开始是尴尬，时间长了次数多了，我开始感受到被忽略的愤怒，知道自己不应该这样，可是就是好生气，该怎么办呢？'}]\n",
      "tensor([[  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 54673,\n",
      "         32282, 54535, 31937, 54546, 31737, 31123, 33600, 54657, 38216, 31123,\n",
      "         49086, 31514, 54627, 54736, 31155, 33103, 36941, 33033, 31783, 54701,\n",
      "         34629, 32282, 38038, 31123, 38617, 54732, 32282, 31857, 31635, 31155,\n",
      "         31002,  1335, 30994, 37514, 31123, 45955, 31737, 31123, 31684, 54549,\n",
      "         32654, 31123, 54617, 36481, 54546, 54659, 31767, 31888, 31747, 54662,\n",
      "         31123, 33446, 54603, 34393, 31123, 31633, 34093, 32270, 55483, 54547,\n",
      "         54546, 31155, 31002,  1335, 30994, 35872, 54532, 35556, 31123, 31643,\n",
      "         42165, 36942, 33851, 31123, 51809, 33816, 54732, 36481, 54530, 38216,\n",
      "         31123, 43292, 40919, 31676, 31123, 32435, 31632, 54591, 36443, 31123,\n",
      "         49086, 55282, 31514, 31002, 31007,   326, 30962,   437, 31007, 30994,\n",
      "            13]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">心理对话微调 ChatGLM3-6B Gradio 简单 Demo</h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"输入...\", lines=10, container=False)\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                submitBtn = gr.Button(\"Submit\")\n",
    "        with gr.Column(scale=1):\n",
    "            emptyBtn = gr.Button(\"清除上下文\")\n",
    "            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            temperature = gr.Slider(0.01, 1, value=0.6, step=0.01, label=\"Temperature\", interactive=True)\n",
    "            repetition_penalty = gr.Slider(1.0, 1.5, value=1.25, step=0.01, label=\"repetition_penalty\", interactive=True)\n",
    "\n",
    "\n",
    "    def user(query, history):\n",
    "        return \"\", history + [[parse_text(query), \"\"]]\n",
    "\n",
    "\n",
    "    submitBtn.click(user, [user_input, chatbot], [user_input, chatbot], queue=False).then(\n",
    "        predict, [chatbot, max_length, repetition_penalty, temperature], chatbot\n",
    "    )\n",
    "    emptyBtn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549193ab-8b9a-47be-a5e5-b3183bca3634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b72db38-c986-4ba8-a625-52a89cf0b93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你好<|im_end|>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d9e13b-c4e1-4d44-afe7-69a67fb4083e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你好<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13, 31002, 31007,\n",
    "           326, 30962,  6631, 31007, 30994,   530, 18971,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b15008b-8d80-4285-af19-cc16294e3f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你好<|im_end|>\\n<|im_start|>assistant\\n您好，我是人工智能助手。请问有什么问题我可以帮您解答吗？<|user|><|im_end|>\\n<|im_start|>user\\n请问你是谁？<|im_end|>\\n<|im_start|>assistant\\n我是一个人工智能助手，可以回答您的问题和提供帮助。<|user|><|im_end|>\\n<|im_start|>user\\n当别人不注意我的时候，我就很愤怒，该怎么办？如题。我想做一个不用那么去争取别人注意力，又能被别人看到的人。<br>比如说，开会的时候，大家会讨论，而忽略我；或者新的环境里，有很多小团体，他们也不会主动叫上我。<br>一开始是尴尬，时间长了次数多了，我开始感受到被忽略的愤怒，知道自己不应该这样，可是就是好生气，该怎么办呢？<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13, 31002, 31007,\n",
    "           326, 30962,  6631, 31007, 30994,   530, 18971,    13, 48214, 31123,\n",
    "         33030, 34797, 42481, 31155, 42693, 33277, 31639, 40648, 55268, 55353,\n",
    "         36295, 55398, 31514, 31002, 31007,  4865, 31007,  6144, 31007,   326,\n",
    "         30962,   437, 31007, 30994,    13, 31002, 31007,   326, 30962,  6631,\n",
    "         31007, 30994,  4865,    13, 42693, 34607, 55622, 31514, 31002, 31007,\n",
    "           326, 30962,   437, 31007, 30994,    13, 31002, 31007,   326, 30962,\n",
    "          6631, 31007, 30994,   530, 18971,    13, 54546, 32103, 34797, 42481,\n",
    "         31123, 31628, 33287, 55353, 32184, 54542, 31692, 31934, 31155, 31002,\n",
    "         31007,  4865, 31007,  6144, 31007,   326, 30962,   437, 31007, 30994,\n",
    "            13, 31002, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13,\n",
    "         54673, 32282, 54535, 31937, 54546, 31737, 31123, 33600, 54657, 38216,\n",
    "         31123, 49086, 31514, 54627, 54736, 31155, 33103, 36941, 33033, 31783,\n",
    "         54701, 34629, 32282, 38038, 31123, 38617, 54732, 32282, 31857, 31635,\n",
    "         31155, 31002,  1335, 30994, 37514, 31123, 45955, 31737, 31123, 31684,\n",
    "         54549, 32654, 31123, 54617, 36481, 54546, 54659, 31767, 31888, 31747,\n",
    "         54662, 31123, 33446, 54603, 34393, 31123, 31633, 34093, 32270, 55483,\n",
    "         54547, 54546, 31155, 31002,  1335, 30994, 35872, 54532, 35556, 31123,\n",
    "         31643, 42165, 36942, 33851, 31123, 51809, 33816, 54732, 36481, 54530,\n",
    "         38216, 31123, 43292, 40919, 31676, 31123, 32435, 31632, 54591, 36443,\n",
    "         31123, 49086, 55282, 31514, 31002, 31007,   326, 30962,   437, 31007,\n",
    "         30994,    13, 31002, 31007,   326, 30962,  6631, 31007, 30994,   530,\n",
    "         18971,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e96990-4558-4ec9-a321-1b40868be7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(31002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cf675-4ada-4025-a1be-3f6263e5b731",
   "metadata": {},
   "source": [
    "```\n",
    "def simple_chat(prompts):\n",
    "    response, history = model.chat(\n",
    "        tokenizer, \n",
    "        prompts, \n",
    "        history=[],\n",
    "        do_sample=True, \n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.0,)\n",
    "    return response\n",
    "\n",
    "demo = gr.Interface(fn=simple_chat, \n",
    "                    inputs=gr.Textbox(\n",
    "                        label='''说点什么~\n",
    "                        因为这是一个以心理疏导为导向的模型，问一些与心理疏导无关的内容效果可能会变差\n",
    "                        目前web Demo只支持单论对话（即模型无法了解你之前发送的内容）, 正在改进中...\n",
    "                        '''), \n",
    "                    outputs=gr.Text(\n",
    "                        label=\"\"\"回复~\n",
    "                        目前参数temperature=0.8, repetition_penalty=1.2\"\"\"\n",
    "                    ))\n",
    "\n",
    "demo.launch(share=True, server_port=6006)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "230ccbf3-cca5-443f-8ec4-9304fb9e9d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_chat(prompts):\n",
    "    response, history = model.chat(\n",
    "        tokenizer, \n",
    "        prompts, \n",
    "        history=[],\n",
    "        do_sample=True, \n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.0,)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9b0d33-83d4-4622-b749-75d4aedbf2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很高兴认识你。请问有什么我可以帮助你的？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chat('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24a525-0956-4745-b7b5-f8fb2f802851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
