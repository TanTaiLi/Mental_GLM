{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9541844d-d60a-4728-8ac0-7d20ed8db2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/tuning_space', '/root/miniconda3/lib/python38.zip', '/root/miniconda3/lib/python3.8', '/root/miniconda3/lib/python3.8/lib-dynload', '', '/root/miniconda3/lib/python3.8/site-packages', '/root/.cache/huggingface/modules']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd68e78d-d0db-4e5a-8578-358ffed0ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbf340d-4971-4b23-84a7-690e205bb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-tmp/weights/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e61ad5-1ccc-4bf5-983a-a3feb8a8f893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009174346923828125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daa728afb634803aa6f7c729c3a0b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.11 s, sys: 19.4 s, total: 26.6 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82e2ce52-adc9-46f2-b6ca-7083e6af3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt='用户:你好,请问你叫什么？\\n 助手:\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6925a523-c5b6-4374-a601-2070053c6b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_streamed(prompt, model, tokenzier, device='cuda'):\n",
    "    # 将自然语言形式的prompt进行tokenlize并转化为tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # 流式生成predict token\n",
    "    max_length = 1000 # 设置最大生成token长度\n",
    "    with torch.no_grad(): # 禁用梯度下降\n",
    "        for count in range(max_length):\n",
    "            # 获得最后一个词位置上的词表概率分布\n",
    "            outputs = model(input_ids=input_ids) # 直接使用了torch的inference方法，得到的是ModelOuput对象\n",
    "            next_token_logits = outputs.logits[:, -1, :] # 获取GPT模型最后一层的数据，并提取最后一个位置的词表概率分布\n",
    "            \n",
    "            # 从概率分布中抽象选择被predict的token;此处并不随机选一个子集，而是在所有词汇中都进行选择，也就是top_p=1\n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            \n",
    "            # 解码predict token\n",
    "            print(tokenizer.decode(next_token.squeeze().tolist()), end='', flush=True)\n",
    "            \n",
    "            # 整合下一个input\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # 判断是否结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffedcae9-0a38-40c6-8866-179ea61242c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好，我是一名人工智能助手，很高兴为您服务！请问有什么问题我可以帮您解答吗？"
     ]
    }
   ],
   "source": [
    "inference_streamed(sample_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687af08-6729-4d50-9cff-f8b143ac0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_streamed(prompt, model, tokenizer, device='cuda'):\n",
    "    # Encode the prompt to tensor of token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text token by token\n",
    "    max_length = 1000  # Set the maximum length for generation\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for _ in range(max_length):\n",
    "            # Get the logits of the last token (need to pass the whole sequence every time)\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # Sample the next token\n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "\n",
    "            # Print the generated token\n",
    "            print(tokenizer.decode(next_token.squeeze().tolist()), end='', flush=True)\n",
    "\n",
    "            # Append the new token to the input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Check if the last token is the end of a sentence\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "# Example usage:\n",
    "# inference_streamed(\"The quick brown fox\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577e4609-fd5f-4109-9fb5-70af69a3e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, do_sample=True, model=model, tokenizer=tokenizer):\n",
    "    # tokens-index\n",
    "    model_inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda') #返回token的index\n",
    "    model_output = model.generate(input_ids=model_inputs, do_sample=do_sample, temperature=0.2, top_p=0.95, pad_token_id = tokenizer.pad_token_id, max_length=10000)\n",
    "    # nature language\n",
    "    model_output_nl = tokenizer.decode(model_output[0], skip_special_tokenizer=True).strip()\n",
    "    return model_output_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e505002-5f50-4af3-8fd6-49ce77f3b950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:18,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:我是一个助手，可以回答您的问题和提供帮助。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:09,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:06,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:04,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:04<00:03,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:我是一个助手，可以回答您的问题，请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:04<00:02,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:05<00:01,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:05<00:01,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:06<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop 用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "用户:你好,请问你叫什么？\n",
      " 助手:你好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "助手:您好，我是人工智能助手，很高兴为您服务。请问有什么问题我可以帮您解答吗？\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for times in tqdm(range(10)):\n",
    "    print(inference(sample_prompt))\n",
    "    print(\"***\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729c5a8-26f4-4d37-b6fb-c98d267148e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
