{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feeaa2f6-d651-41e1-8127-5eb96b1c016f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ed41b7-5a3d-4b57-9bce-f9903a6e5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aae582e-3710-47a4-96a2-6af840ff84cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/SoulChat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6de31ce-1813-4b94-bf48-23ccf030c6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007318973541259766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b5fff9071413c8e2028074209a19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0420f5a-5c30-4ab4-8f3c-4a9b81728ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (word_embeddings): Embedding(130528, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e3618-3f98-4b0e-9389-1e0f18d61d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    input_ids = torch.cat([st['[gMASK]'],st['sop'],st['<|system|>'],st['\\n']],dim=1).cuda()\n",
    "    input_ids = torch.cat([input_ids, tokenizer.encode(prompt_dict['system_information'], return_tensors='pt').cuda()[:,2:], st['\\n'], st['<|user|>']], dim=1)\n",
    "    while True:\n",
    "        input_ids = torch.cat([input_ids, tokenizer.encode(input('User:'), return_tensors='pt').cuda()[:,2:], st['\\n'], st['<|assistant|>']], dim=1)\n",
    "        input_ids = interact.inference_streamed(input_ids, model, tokenizer, in_nature_language=False, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85780769-3bf3-446b-9155-1a97697d96d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
