{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e881cda3-9dd2-466e-ad83-f21b3eee3c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "    data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "    lora_dir = '/root/autodl-tmp/checkpoints/previous/glm3-Rank64_27000' #'/root/autodl-tmp/checkpoints/glm3-3-dataset-Rank64'\n",
    "    MAX_TURNS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd3b0ed-c4ae-4e10-a7c4-6e438cffeac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/root/miniconda3/lib/python3.8/site-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mdtex2html\n",
    "import gradio as gr\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import model_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4f440d-e753-4645-b811-44e36fc785dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from threading import Thread\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48dff910-f69f-4143-abea-6b804f0da8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_LICENSE                     pytorch_model-00005-of-00007.bin\n",
      "README.md                         pytorch_model-00006-of-00007.bin\n",
      "config.json                       pytorch_model-00007-of-00007.bin\n",
      "configuration_chatglm.py          pytorch_model.bin.index.json\n",
      "modeling_chatglm.py               quantization.py\n",
      "pytorch_model-00001-of-00007.bin  tokenization_chatglm.py\n",
      "pytorch_model-00002-of-00007.bin  tokenizer.model\n",
      "pytorch_model-00003-of-00007.bin  tokenizer_config.json\n",
      "pytorch_model-00004-of-00007.bin\n"
     ]
    }
   ],
   "source": [
    "ls $CFG.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec82b29-4852-4edd-a6d3-d76f8d540f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008582830429077148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f3725fdab0490eac0bd2e445f63b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "#tokenization_chatglm.py\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "#model = model_tools.merge_lora(CFG.model_path, CFG.lora_dir)\n",
    "model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().half()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4566039-ff03-4f69-a8b2-aaff9121a6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Conversation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Conversation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mchat_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Converts a Conversation object or a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\u001b[0m\n",
       "\u001b[0;34m        ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\u001b[0m\n",
       "\u001b[0;34m        determine the format and control tokens to use when converting. When chat_template is None, it will fall back\u001b[0m\n",
       "\u001b[0;34m        to the default_chat_template specified at the class level.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            conversation (Union[List[Dict[str, str]], \"Conversation\"]): A Conversation object or list of dicts\u001b[0m\n",
       "\u001b[0;34m                with \"role\" and \"content\" keys, representing the chat history so far.\u001b[0m\n",
       "\u001b[0;34m            chat_template (str, *optional*): A Jinja template to use for this conversion. If\u001b[0m\n",
       "\u001b[0;34m                this is not passed, the model's default chat template will be used instead.\u001b[0m\n",
       "\u001b[0;34m            add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate\u001b[0m\n",
       "\u001b[0;34m                the start of an assistant message. This is useful when you want to generate a response from the model.\u001b[0m\n",
       "\u001b[0;34m                Note that this argument will be passed to the chat template, and so it must be supported in the\u001b[0m\n",
       "\u001b[0;34m                template for this argument to have any effect.\u001b[0m\n",
       "\u001b[0;34m            tokenize (`bool`, defaults to `True`):\u001b[0m\n",
       "\u001b[0;34m                Whether to tokenize the output. If `False`, the output will be a string.\u001b[0m\n",
       "\u001b[0;34m            padding (`bool`, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m                Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\u001b[0m\n",
       "\u001b[0;34m            truncation (`bool`, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m                Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\u001b[0m\n",
       "\u001b[0;34m            max_length (`int`, *optional*):\u001b[0m\n",
       "\u001b[0;34m                Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\u001b[0m\n",
       "\u001b[0;34m                not specified, the tokenizer's `max_length` attribute will be used as a default.\u001b[0m\n",
       "\u001b[0;34m            return_tensors (`str` or [`~utils.TensorType`], *optional*):\u001b[0m\n",
       "\u001b[0;34m                If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\u001b[0m\n",
       "\u001b[0;34m                values are:\u001b[0m\n",
       "\u001b[0;34m                - `'tf'`: Return TensorFlow `tf.Tensor` objects.\u001b[0m\n",
       "\u001b[0;34m                - `'pt'`: Return PyTorch `torch.Tensor` objects.\u001b[0m\n",
       "\u001b[0;34m                - `'np'`: Return NumPy `np.ndarray` objects.\u001b[0m\n",
       "\u001b[0;34m                - `'jax'`: Return JAX `jnp.ndarray` objects.\u001b[0m\n",
       "\u001b[0;34m            return_dict (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m                Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\u001b[0m\n",
       "\u001b[0;34m            **tokenizer_kwargs: Additional kwargs to pass to the tokenizer.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns:\u001b[0m\n",
       "\u001b[0;34m            `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\u001b[0m\n",
       "\u001b[0;34m            output is ready to pass to the model, either directly or via methods like `generate()`.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# Indicates it's a Conversation object\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template`\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mchat_template\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_chat_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Compilation function uses a cache to avoid recompiling the same template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcompiled_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_jinja_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrendered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_map\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m  \u001b[0;31m# There's only one sequence here, so \"longest\" makes no sense\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mrendered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mrendered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cf1211-cdaa-4db3-a33a-8565f8df5cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [0, 2]#[0, 2, 31002, 64795]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\" + line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict(history, max_length, repetition_penalty, temperature):\n",
    "    print(history)\n",
    "    stop = StopOnTokens()\n",
    "    messages = []\n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "    print(\"\\n\\n====conversation====\\n\", messages)\n",
    "    print(messages)\n",
    "    model_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                                 add_generation_prompt=False,\n",
    "                                                 tokenize=True,\n",
    "                                                 return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    print(model_inputs)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": temperature,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    for new_token in streamer:\n",
    "        if new_token != '':\n",
    "            history[-1][1] += new_token\n",
    "            yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695a5518-5209-4600-9515-5ebf0aa5d0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://e88c2bef56a815d2bd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e88c2bef56a815d2bd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ä½ å¥½', '']]\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': 'ä½ å¥½'}]\n",
      "[{'role': 'user', 'content': 'ä½ å¥½'}]\n",
      "tensor([[  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
      "         31002, 31007,   326, 30962,   437, 31007, 30994,    13]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">å¿ƒç†å¯¹è¯å¾®è°ƒ ChatGLM3-6B Gradio ç®€å• Demo</h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"è¾“å…¥...\", lines=10, container=False)\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                submitBtn = gr.Button(\"Submit\")\n",
    "        with gr.Column(scale=1):\n",
    "            emptyBtn = gr.Button(\"æ¸…é™¤ä¸Šä¸‹æ–‡\")\n",
    "            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            temperature = gr.Slider(0.01, 1, value=0.6, step=0.01, label=\"Temperature\", interactive=True)\n",
    "            repetition_penalty = gr.Slider(1.0, 1.5, value=1.25, step=0.01, label=\"repetition_penalty\", interactive=True)\n",
    "\n",
    "\n",
    "    def user(query, history):\n",
    "        return \"\", history + [[parse_text(query), \"\"]]\n",
    "\n",
    "\n",
    "    submitBtn.click(user, [user_input, chatbot], [user_input, chatbot], queue=False).then(\n",
    "        predict, [chatbot, max_length, repetition_penalty, temperature], chatbot\n",
    "    )\n",
    "    emptyBtn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549193ab-8b9a-47be-a5e5-b3183bca3634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b72db38-c986-4ba8-a625-52a89cf0b93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nä½ å¥½<|im_end|>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d9e13b-c4e1-4d44-afe7-69a67fb4083e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nä½ å¥½<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13, 31002, 31007,\n",
    "           326, 30962,  6631, 31007, 30994,   530, 18971,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b15008b-8d80-4285-af19-cc16294e3f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nä½ å¥½<|im_end|>\\n<|im_start|>assistant\\næ‚¨å¥½ï¼Œæˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å—ï¼Ÿ<|user|><|im_end|>\\n<|im_start|>user\\nè¯·é—®ä½ æ˜¯è°ï¼Ÿ<|im_end|>\\n<|im_start|>assistant\\næˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”æ‚¨çš„é—®é¢˜å’Œæä¾›å¸®åŠ©ã€‚<|user|><|im_end|>\\n<|im_start|>user\\nå½“åˆ«äººä¸æ³¨æ„æˆ‘çš„æ—¶å€™ï¼Œæˆ‘å°±å¾ˆæ„¤æ€’ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿå¦‚é¢˜ã€‚æˆ‘æƒ³åšä¸€ä¸ªä¸ç”¨é‚£ä¹ˆå»äº‰å–åˆ«äººæ³¨æ„åŠ›ï¼Œåˆèƒ½è¢«åˆ«äººçœ‹åˆ°çš„äººã€‚<br>æ¯”å¦‚è¯´ï¼Œå¼€ä¼šçš„æ—¶å€™ï¼Œå¤§å®¶ä¼šè®¨è®ºï¼Œè€Œå¿½ç•¥æˆ‘ï¼›æˆ–è€…æ–°çš„ç¯å¢ƒé‡Œï¼Œæœ‰å¾ˆå¤šå°å›¢ä½“ï¼Œä»–ä»¬ä¹Ÿä¸ä¼šä¸»åŠ¨å«ä¸Šæˆ‘ã€‚<br>ä¸€å¼€å§‹æ˜¯å°´å°¬ï¼Œæ—¶é—´é•¿äº†æ¬¡æ•°å¤šäº†ï¼Œæˆ‘å¼€å§‹æ„Ÿå—åˆ°è¢«å¿½ç•¥çš„æ„¤æ€’ï¼ŒçŸ¥é“è‡ªå·±ä¸åº”è¯¥è¿™æ ·ï¼Œå¯æ˜¯å°±æ˜¯å¥½ç”Ÿæ°”ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  906, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13, 39701,\n",
    "         31002, 31007,   326, 30962,   437, 31007, 30994,    13, 31002, 31007,\n",
    "           326, 30962,  6631, 31007, 30994,   530, 18971,    13, 48214, 31123,\n",
    "         33030, 34797, 42481, 31155, 42693, 33277, 31639, 40648, 55268, 55353,\n",
    "         36295, 55398, 31514, 31002, 31007,  4865, 31007,  6144, 31007,   326,\n",
    "         30962,   437, 31007, 30994,    13, 31002, 31007,   326, 30962,  6631,\n",
    "         31007, 30994,  4865,    13, 42693, 34607, 55622, 31514, 31002, 31007,\n",
    "           326, 30962,   437, 31007, 30994,    13, 31002, 31007,   326, 30962,\n",
    "          6631, 31007, 30994,   530, 18971,    13, 54546, 32103, 34797, 42481,\n",
    "         31123, 31628, 33287, 55353, 32184, 54542, 31692, 31934, 31155, 31002,\n",
    "         31007,  4865, 31007,  6144, 31007,   326, 30962,   437, 31007, 30994,\n",
    "            13, 31002, 31007,   326, 30962,  6631, 31007, 30994,  4865,    13,\n",
    "         54673, 32282, 54535, 31937, 54546, 31737, 31123, 33600, 54657, 38216,\n",
    "         31123, 49086, 31514, 54627, 54736, 31155, 33103, 36941, 33033, 31783,\n",
    "         54701, 34629, 32282, 38038, 31123, 38617, 54732, 32282, 31857, 31635,\n",
    "         31155, 31002,  1335, 30994, 37514, 31123, 45955, 31737, 31123, 31684,\n",
    "         54549, 32654, 31123, 54617, 36481, 54546, 54659, 31767, 31888, 31747,\n",
    "         54662, 31123, 33446, 54603, 34393, 31123, 31633, 34093, 32270, 55483,\n",
    "         54547, 54546, 31155, 31002,  1335, 30994, 35872, 54532, 35556, 31123,\n",
    "         31643, 42165, 36942, 33851, 31123, 51809, 33816, 54732, 36481, 54530,\n",
    "         38216, 31123, 43292, 40919, 31676, 31123, 32435, 31632, 54591, 36443,\n",
    "         31123, 49086, 55282, 31514, 31002, 31007,   326, 30962,   437, 31007,\n",
    "         30994,    13, 31002, 31007,   326, 30962,  6631, 31007, 30994,   530,\n",
    "         18971,    13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e96990-4558-4ec9-a321-1b40868be7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(31002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cf675-4ada-4025-a1be-3f6263e5b731",
   "metadata": {},
   "source": [
    "```\n",
    "def simple_chat(prompts):\n",
    "    response, history = model.chat(\n",
    "        tokenizer, \n",
    "        prompts, \n",
    "        history=[],\n",
    "        do_sample=True, \n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.0,)\n",
    "    return response\n",
    "\n",
    "demo = gr.Interface(fn=simple_chat, \n",
    "                    inputs=gr.Textbox(\n",
    "                        label='''è¯´ç‚¹ä»€ä¹ˆ~\n",
    "                        å› ä¸ºè¿™æ˜¯ä¸€ä¸ªä»¥å¿ƒç†ç–å¯¼ä¸ºå¯¼å‘çš„æ¨¡å‹ï¼Œé—®ä¸€äº›ä¸å¿ƒç†ç–å¯¼æ— å…³çš„å†…å®¹æ•ˆæœå¯èƒ½ä¼šå˜å·®\n",
    "                        ç›®å‰web Demoåªæ”¯æŒå•è®ºå¯¹è¯ï¼ˆå³æ¨¡å‹æ— æ³•äº†è§£ä½ ä¹‹å‰å‘é€çš„å†…å®¹ï¼‰, æ­£åœ¨æ”¹è¿›ä¸­...\n",
    "                        '''), \n",
    "                    outputs=gr.Text(\n",
    "                        label=\"\"\"å›å¤~\n",
    "                        ç›®å‰å‚æ•°temperature=0.8, repetition_penalty=1.2\"\"\"\n",
    "                    ))\n",
    "\n",
    "demo.launch(share=True, server_port=6006)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "230ccbf3-cca5-443f-8ec4-9304fb9e9d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_chat(prompts):\n",
    "    response, history = model.chat(\n",
    "        tokenizer, \n",
    "        prompts, \n",
    "        history=[],\n",
    "        do_sample=True, \n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.0,)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9b0d33-83d4-4622-b749-75d4aedbf2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chat('ä½ å¥½')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24a525-0956-4745-b7b5-f8fb2f802851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
