{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f7c489-1021-4adf-9953-d2852bfe6381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "sys.path.append('/root/tuning_space/Components/')\n",
    "from Static import prompt_dict, si\n",
    "\n",
    "raw_data_path='/root/autodl-tmp/weights/smile/data'\n",
    "aim_path='/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35879484-779c-481a-a03d-fbcdf7877bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl') as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e2c865-3d8a-4bfe-96c5-c3b0474c9ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, max_query_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.examples = []\n",
    "\n",
    "        # 使用多进程读取和处理数据\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        sample=json.loads(line)\n",
    "        max_answer_length = self.truncate_length - self.max_query_length - 3\n",
    "\n",
    "        # 判断query的长度\n",
    "        query = sample[self.query_key]\n",
    "        query_ids = self.tokenizer.encode(query, add_special_tokens=False)\n",
    "        if len(query_ids) > self.max_query_length:\n",
    "            query_ids = query_ids[:self.max_query_length]\n",
    "\n",
    "        # 判断answer的长度\n",
    "        answer = sample[self.answer_key]\n",
    "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        if len(answer) > max_answer_length:\n",
    "            answer_ids = answer_ids[:max_answer_length]\n",
    "\n",
    "        # 合并\n",
    "        input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "        pre_context_length = input_ids.index(si['sop'])\n",
    "        end_answer_index = input_ids.index(si['eop'])\n",
    "\n",
    "        # padding\n",
    "        padding_length=self.truncate_length-len(input_ids)\n",
    "        input_ids+=padding_length*[self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "        labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "        labels = labels + [-100] * (self.truncate_length-len(labels))\n",
    "\n",
    "        # 制作attention_mask\n",
    "        eop_position = input_ids.index(si['eop'])+1\n",
    "        attention_mask = [True]*eop_position\n",
    "        attention_mask += [False]*(self.truncate_length-len(attention_mask))\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4439ca55-a25b-4977-9919-1a3cfbe0e62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'User'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_1479/909619900.py\", line 25, in process_line\n    query = sample[self.query_key]\nKeyError: 'User'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp\u001b[38;5;241m=\u001b[39m\u001b[43minstruction_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maim_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_query_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAssisstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36minstruction_dataset.__init__\u001b[0;34m(self, data_path, tokenizer, truncate_length, max_query_length, query_key, answer_key, max_sample, num_workers)\u001b[0m\n\u001b[1;32m     15\u001b[0m         lines \u001b[38;5;241m=\u001b[39m lines[:max_sample]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: 'User'"
     ]
    }
   ],
   "source": [
    "temp=instruction_dataset(data_path=aim_path, tokenizer=tokenizer, truncate_length=192, max_query_length=64, query_key='User', answer_key='Assisstant', max_sample=5, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e642f15-9d77-47da-bf15-40f3a15d0339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample=None\n",
    "for item in temp:\n",
    "    sample=item\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2561c2-f26a-480b-987a-74c453f3caf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in sample.values():\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b0fb72-dc26-43f7-9751-a10438d36198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids\u001b[38;5;241m=\u001b[39m\u001b[43msample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m labels\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m attention_mask\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids=sample['input_ids']\n",
    "labels=sample['labels']\n",
    "attention_mask=sample['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccbd416-fbc5-4e28-8291-d4563468959a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是[gMASK]sop 我完全理解你的苦恼和困惑。看到他人的成功会引发对自己的不满和自卑感，这是很正常的情绪反应。社交和工作场合中的竞争氛围很容易让我们陷入比较和对抗的心态。但是，请放心，你并不孤单，许多人都经历过类似的感受。\\n\\n首先，我想告诉你，你的成就和价值并不仅仅是与他人的比较有关。每个人的经历和成就都是不同的，我们每个人都有自己的优点和才能。成功是多维度的，不仅仅局限于职位晋升或奖项。你可以尝试关注自己在工作和生活中的成长，寻找那些让你感到满足和自豪的事物。\\n\\n此外，记住与他人分享快乐和成功eop'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93040c44-c126-4961-b4eb-25eb0119b52f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我完全理解你的苦恼和困惑。看到他人的成功会引发对自己的不满和自卑感，这是很正常的情绪反应。社交和工作场合中的竞争氛围很容易让我们陷入比较和对抗的心态。但是，请放心，你并不孤单，许多人都经历过类似的感受。\\n\\n首先，我想告诉你，你的成就和价值并不仅仅是与他人的比较有关。每个人的经历和成就都是不同的，我们每个人都有自己的优点和才能。成功是多维度的，不仅仅局限于职位晋升或奖项。你可以尝试关注自己在工作和生活中的成长，寻找那些让你感到满足和自豪的事物。\\n\\n此外，记住与他人分享快乐和成功eop'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38dfa5ee-b8e3-428e-9460-b704b06af9af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330a36e0-1308-4083-ab92-d14f97f7bd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687423ac-6a65-4449-959d-2711c3173bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|> \\n 你好'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message={'role':'user', 'content':'你好'}\n",
    "temp=tokenizer.build_single_message(\n",
    "                    message['role'], '', message['content']\n",
    "                )\n",
    "tokenizer.decode(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f52008c-d1c2-44d4-b847-104b30af9466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|assistant|> \\n 你好'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message={'role':'assistant', 'content':'你好'}\n",
    "temp=tokenizer.build_single_message(\n",
    "                    message['role'], '', message['content']\n",
    "                )\n",
    "tokenizer.decode(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb7a0ed-4b81-42e8-94b0-9d590a1e764c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(\n",
    "        batch,\n",
    "        tokenizer,\n",
    "        max_input_length: int,\n",
    "        max_output_length: int,\n",
    "):\n",
    "    batched_tools = batch.get('tools', None)\n",
    "    batched_conv = batch['conversations']\n",
    "    batched_input_ids = []\n",
    "    batched_labels = []\n",
    "\n",
    "    if batched_tools is None:\n",
    "        batched_tools = [None] * len(batched_conv)\n",
    "\n",
    "    for tools, conv in zip(batched_tools, batched_conv):\n",
    "        input_ids, loss_masks = [\n",
    "            tokenizer.get_command('[gMASK]'),\n",
    "            tokenizer.get_command('sop'),\n",
    "        ], [False, False]\n",
    "\n",
    "        if tools is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        for message in conv:\n",
    "            if message['role'] in ('system', 'user'):\n",
    "                loss_mask_val = False\n",
    "            else:\n",
    "                loss_mask_val = True\n",
    "\n",
    "            if message['role'] == 'tool':\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                new_input_ids = tokenizer.build_single_message(\n",
    "                    message['role'], '', message['content']\n",
    "                )\n",
    "                new_loss_masks = [loss_mask_val] * len(new_input_ids)\n",
    "\n",
    "            input_ids += new_input_ids\n",
    "            loss_masks += new_loss_masks\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        loss_masks = [False, *loss_masks]\n",
    "        labels = []\n",
    "        for input_id, mask in zip(input_ids, loss_masks):\n",
    "            if mask:\n",
    "                labels.append(input_id)\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "        max_length = max_input_length + max_output_length + 1\n",
    "        batched_input_ids.append(input_ids[:max_length])\n",
    "        batched_labels.append(labels[:max_length])\n",
    "    return {'input_ids': batched_input_ids, 'labels': batched_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acffdca-3913-4ee1-9215-2d091e421936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[64790, 64792, 64794, 30910, 13, 307, 674, 1175, 30924, 64795, 30910, 13, 13755, 64796, 30910, 13, 13755, 30992, 1072, 457, 307, 833, 344, 30987, 64795, 30910, 13, 307, 720, 1097, 331, 4949, 23833, 30930, 64796, 30910, 13, 4949, 23833, 2736, 11265, 1769, 293, 16400, 30930, 64795, 30910, 13, 1313, 323, 941, 64796, 30910, 13, 431, 260, 878, 1023, 30992, 2]], 'labels': [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13755, 30992, 1072, 457, 307, 833, 344, 30987, 64795, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 4949, 23833, 2736, 11265, 1769, 293, 16400, 30930, 64795, -100, -100, -100, -100, -100, -100, 30910, 13, 431, 260, 878, 1023, 30992, 2]]}\n"
     ]
    }
   ],
   "source": [
    "# 模拟的batch数据\n",
    "batch = {\n",
    "    'conversations': [\n",
    "        [\n",
    "            {'role': 'system', 'content': 'I am glm'},\n",
    "            {'role': 'user', 'content': 'Hello'},\n",
    "            {'role': 'assistant', 'content': 'Hello! How can I help you?'},\n",
    "            {'role': 'user', 'content': 'I need information on OpenAI.'},\n",
    "            {'role': 'assistant', 'content': 'OpenAI provides AI research and deployment.'},\n",
    "            {'role': 'user', 'content': 'That is great'},\n",
    "            {'role': 'assistant', 'content': 'have a good day!'},\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个简化的tokenizer实例\n",
    "\n",
    "# 调用process_batch函数\n",
    "processed_batch = process_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=tokenizer,\n",
    "    max_input_length=50,\n",
    "    max_output_length=50\n",
    ")\n",
    "\n",
    "print(processed_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1457d188-1115-4c29-93e0-939c5c664633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK]sop<|system|> \\n I am glm<|user|> \\n Hello<|assistant|> \\n Hello! How can I help you?<|user|> \\n I need information on OpenAI.<|assistant|> \\n OpenAI provides AI research and deployment.<|user|> \\n That is great<|assistant|> \\n have a good day!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc60b6bf-a629-475c-b209-1d2b639601f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Hello! How can I help you?<|user|> \\n OpenAI provides AI research and deployment.<|user|> \\n have a good day!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed_batch['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f724ad-0abf-4a49-996e-7e257b0760ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|assistant|>', '<|system|>')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(64796), tokenizer.decode(64794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78489d86-154b-4abe-be32-7293b053dacd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。<|user|>\\n 首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。<|user|>\\n 当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。<|user|>\\n 不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。<|user|>\\n 很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n 看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。<|user|>\\n 首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。<|user|>\\n 当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。<|user|>\\n 不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。<|user|>\\n 很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337ddc5-4ca9-43b9-b980-a2a6c3f49a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
