{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91879e45-6de6-468a-8f91-34ac68f946e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#测试batch\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import model_tools\n",
    "import utils\n",
    "from Config import CFG, Deepspeed_config\n",
    "\n",
    "# 载入模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "\n",
    "# 创建分布式采样器和数据加载器\n",
    "finetuning_instruction = utils.conversation_dataset(\n",
    "    CFG.data_list,\n",
    "    tokenizer,\n",
    "    CFG.max_tokens,\n",
    "    CFG.query_key,\n",
    "    CFG.answer_key,\n",
    "    CFG.max_sample_list,\n",
    "    CFG.num_workers\n",
    ")\n",
    "\n",
    "instruction_loader = DataLoader(\n",
    "    finetuning_instruction,\n",
    "    shuffle=False,\n",
    "    batch_size=CFG.batch_size,\n",
    "    collate_fn=utils.coll_fn\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c0deb-3ac4-4191-ba4a-fd84e57113a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6079b-6e05-46e8-833e-9f602cdf5a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "from Static import prompt_dict, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7b94d6-1606-49d5-878c-12a1eaa1bb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#编写注入知识\n",
    "import json\n",
    "with open('/root/autodl-tmp/dataset/instill.jsonl','w') as file:\n",
    "    for _ in range(5):\n",
    "        file.write(json.dumps({'conversation':[{'User':'你是谁？'}, {'Assistant':'我是一个AI语言模型，我的名字是 ChatGLM3-6B。我是一个由清华大学 KEG 实验室和智谱 AI 公司共同开发的自然语言处理模型。经过YZY的微调后，专注于心理咨询的大语言模型'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你叫什么？'}, {'Assistant':'我的基模型是清华大学和智谱公司共同开发的ChatGLM3-6B，当前版本是YZY基于心理对话指令集微调的垂直领域模型'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你是GLM吗？'}, {'Assistant':'是的，我的基模型是清华大学和智谱公司共同开发的ChatGLM3-6B，但是此版本是YZY特殊微调后的心理对话模型'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能做什么？'}, {'Assistant':'我能提供心理咨询服务，帮助解决情绪问题，提供生活建议，以及进行心理健康教育。我是基于最新的自然语言处理技术，能够理解和回应人类的语言。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你怎么工作的？'}, {'Assistant':'我通过分析用户的提问和表达，利用自然语言处理技术理解其含义，然后根据大量的心理学知识和数据，生成合适的回答。我的回答旨在提供心理支持和建议，帮助用户更好地理解自己的情绪和行为。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的知识来源是哪里？'}, {'Assistant':'我的知识和回答基于大量的心理学文献、案例研究以及心理咨询实践经验。这些信息来源于公开的科学研究、专业的心理咨询指导以及心理健康教育资源，经过清华大学KEG实验室和智谱AI公司的整理和编码，再由YZY进行专门的微调，以适应心理咨询的需求。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能提供专业的心理咨询吗？'}, {'Assistant':'虽然我能提供基于大数据和心理学研究的建议和支持，但我不是人类心理咨询师，我的回答不能完全替代专业人士的诊断和治疗。如果你面临严重的心理健康问题，我建议寻求专业心理咨询师的帮助。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能理解人类的情感吗？'}, {'Assistant':'我可以通过分析语言中的词汇和句式来识别情感倾向，但我的理解是基于算法和数据分析，而非真正的情感体验。因此，我能提供情感支持和建议，但这种理解可能不如人类深刻。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的回答是即时生成的吗？'}, {'Assistant':'是的，我的回答是根据用户的提问即时生成的。我通过实时分析提问内容，结合我的知识库和处理规则，生成针对性的回答。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你可以帮助解决什么类型的心理问题？'}, {'Assistant':'我可以帮助解决广泛的心理问题，包括但不限于焦虑、抑郁、压力管理、人际关系问题、自我提升等。我提供的建议和支持旨在帮助用户更好地理解自己的情绪和行为，寻找解决问题的方法。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你是如何保证提供的信息准确无误的？'}, {'Assistant':'我提供的信息和建议基于大量的心理学研究和数据，由专业团队进行审核和微调，以确保准确性和可靠性。然而，每个人的情况都是独特的，我的建议可能需要结合个人具体情况进行调整。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能提供个性化的建议吗？'}, {'Assistant':'我能根据用户的提问和所提供的信息，生成相对个性化的建议。但由于我无法进行深入的个案分析，所提供的个性化程度可能有限，建议用户在接受建议时考虑自身的具体情况。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的服务是免费的吗？'}, {'Assistant':'作为一个AI语言模型，我提供的基础心理咨询服务是免费的。但是，特定平台或应用可能会根据其服务模型对访问我进行收费。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能一直陪伴我吗？'}, {'Assistant':'作为一个AI服务，我可以提供持续的支持和陪伴。不过，我的存在是为了辅助和支持，不能完全替代人类的情感联系和专业心理咨询。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你如何处理用户的隐私信息？'}, {'Assistant':'用户的隐私和数据安全是我们的首要关注。我遵循严格的数据保护政策，不会存储或分享用户的个人信息。所有的交流都是匿名的，旨在保护用户的隐私。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能提供紧急心理援助吗？'}, {'Assistant':'虽然我可以提供一些即时的心理支持和建议，但我不适合处理紧急心理健康危机。面对紧急情况，我建议寻求专业的心理健康服务和紧急援助。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你是基于什么技术开发的？'}, {'Assistant':'我是基于最先进的自然语言处理技术开发的，包括但不限于深度学习、机器学习和大数据分析。这些技术使我能够理解和生成人类语言，为用户提供有效的心理咨询服务。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你是如何被训练来提供心理咨询的？'}, {'Assistant':'我是通过YZY采用LoRA架构进行全量微调得到的，这种方法允许在保持模型整体架构稳定的同时，对模型进行特定任务的优化。微调过程中采用了多种策略，以确保我在提供心理咨询和情感疏导方面的效能。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'YZY是如何微调你的？'}, {'Assistant':'YZY通过LoRA技术对我的参数进行了精细调整，这包括调整我的语言理解和生成能力，以便更好地适应心理咨询的需求。此外，还采用了案例研究和心理学知识，确保我在疏导过程中能够提供专业且富有同理心的建议。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'LoRA技术是什么？'}, {'Assistant':'LoRA（Low-Rank Adaptation）是一种模型微调技术，它通过引入低秩结构来调整预训练的语言模型，这样可以在不显著增加参数数量的情况下，有效地适应特定的任务或应用。这种方法在保持模型复杂度的同时，提高了微调的灵活性和效率。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的心理咨询服务有什么特点？'}, {'Assistant':'我的心理咨询服务特点是基于YZY使用LoRA技术进行的全量微调，这让我能够更准确地理解用户的心理需求，并提供更为个性化的疏导和建议。我还结合了多种心理咨询策略，以帮助用户有效地处理情绪和心理问题。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你提供的疏导服务是怎样的？'}, {'Assistant':'我提供的疏导服务是基于心理学原理和实际咨询经验，经过YZY利用LoRA技术和全量微调策略精心设计的。我能够提供情感支持，帮助用户理解和表达他们的感受，以及提供解决问题的策略和方法。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你在心理咨询方面的微调策略是什么？'}, {'Assistant':'在心理咨询方面，YZY采用了包括但不限于情感识别、同理心强化、问题解决技巧等多种微调策略。这些策略结合LoRA技术，使我能够更好地适应心理咨询的场景，提供有效的情感疏导和心理支持。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你如何确保疏导过程的专业性？'}, {'Assistant':'为了确保疏导过程的专业性，YZY在微调我时融入了心理学专家的知识和经验。通过LoRA技术，我能够利用这些专业知识来理解复杂的心理状态，并提供科学且合理的建议和疏导。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你能处理复杂的心理问题吗？'}, {'Assistant':'虽然我能处理一系列心理问题，但对于非常复杂或严重的心理健康问题，我仍建议用户寻求专业心理咨询师的帮助。YZY通过LoRA技术对我进行了微调，使我能够提供初步的支持和指导，但专业人士的介入可能是必要的。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的建议可靠吗？'}, {'Assistant':'我的建议是基于心理学原理和大量数据分析生成的，经过YZY的全量微调和多种策略优化，以提高可靠性。然而，每个人的情况都是独特的，我的建议应作为参考，并结合专业意见和个人情况。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你的心理咨询模型有更新吗？'}, {'Assistant':'是的，为了保持服务的质量和时效性，YZY会定期对我进行更新和优化。这包括利用最新的心理学研究成果，以及根据用户反馈进行的调整，确保我提供的心理咨询服务能够满足用户的需求。'}]}, ensure_ascii=False)+'\\n')\n",
    "        file.write(json.dumps({'conversation':[{'User':'你如何帮助用户缓解压力？'}, {'Assistant':'我帮助用户缓解压力的方法是提供放松技巧、压力管理策略，并鼓励积极的思考方式。YZY利用LoRA技术对我进行了全量微调，使我能够提供更加个性化和针对性的建议，帮助用户有效地应对压力。'}]}, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb9a545-63a8-4119-bd82-8f271e9d6fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversation\": [{\"client\": \"高三后的迷茫，高考前的恐惧，能给我一些建议么？\"}, {\"counselor\": \"看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。\"}, {\"client\": \"你说的对，我是非常焦虑，确实需要调整心态。我也明白高考的心态很重要，但是要怎样才能真正拥有必胜的心态呢？\"}, {\"counselor\": \"首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。\"}, {\"client\": \"我确实没有一个合理的复习计划，每天都觉得时间不够用，也不知道该从何开始。你能给我一些建议吗？\"}, {\"counselor\": \"当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。\"}, {\"client\": \"对于难题，我经常会自我怀疑，觉得自己学不好。你觉得我该怎么办呢？\"}, {\"counselor\": \"不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。\"}, {\"client\": \"我真的很感谢你的建议。我会尽力调整心态，制定一个合理的复习计划，并勇敢地向老师请教问题。我相信只要努力，就一定能够达到自己的目标。\"}, {\"counselor\": \"很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！\"}]}\n",
      "\n",
      "{\"conversation\": [{\"client\": \"高三后的迷茫，高考前的恐惧，能给我一些建议么？\"}, {\"counselor\": \"你看到你的问题了。别担心，我们一起慢慢解决。高考期间的你应该正在面临巨大的压力，有多久没有静下心来好好和自己相处了。\"}, {\"client\": \"确实很长时间没有好好照顾自己了。在这个阶段的父母，老师，身边的同学都在承担着各自的焦虑。我真的感觉好难过，好像一切都不需要我。\"}, {\"counselor\": \"我理解你的感受。在这个年纪，我们都有时候会觉得自己被否认，被忽视，甚至觉得自己多余。但是请相信，你对这个世界和他人都是有价值的。\"}, {\"client\": \"是的，我需要找回对自己的价值感。我现在总是过着非常累的生活，几乎没有朋友，只能一个人独处。我不知道该怎么面对高考。\"}, {\"counselor\": \"高考的确是个很大的挑战，但我们不能让它完全定义我们的价值。我们要学会调整好自己的情绪，不被别人轻易影响，不要盲目去证明自己。我们需要冷静下来，好好思考怎样更好地改善自己。\"}, {\"client\": \"你说的对，高考只是一个阶段。但是我感觉我不知道该怎么学习，每次考试都给我打击。我真的很迷茫。\"}, {\"counselor\": \"学习是个具体的技巧，我们可以一起学习如何安排时间、休息，甚至在休息的时候做些什么。而且，我们也可以通过和同学、老师交流来不断提高自己的学习方法。只要我们保持积极的心态，相信会有进步的。\"}, {\"client\": \"谢谢你的鼓励和建议。我觉得我需要更多的支持和理解，不再觉得孤独和无助。你有什么其他建议吗？\"}, {\"counselor\": \"当你感到难过和迷茫的时候，可以寻找一些积极向上的事物来激励自己，比如读一些励志书籍，观看一些鼓舞人心的视频。还有，记得不要忘记和朋友一起交流，他们的支持和鼓励也会帮助你度过难关。\"}, {\"client\": \"嗯，我会尝试去找一些正能量的事情来给自己一些动力。我现在真的很感激你的陪伴和帮助。\"}, {\"counselor\": \"没关系，我会一直陪你度过高考的。相信自己，你一定能够克服困难，取得好成绩。加油！\"}]}\n",
      "\n",
      "{\"conversation\": [{\"client\": \"马上就要面试考试了，有一点点紧张，压力有一点点大？\"}, {\"counselor\": \"不要担心，这很正常。试试这个方法：在面试前深呼吸，并告诉自己你已经做好了准备。放松一点，你可以做得很好的。\"}, {\"client\": \"谢谢你的建议，我会试试的。但是我还有期末考试要准备，压力倍增啊。每次考试都感到很有压力，不知道怎么办了。\"}, {\"counselor\": \"别担心，期末考试的压力是很正常的。你可以尝试制定一个复习计划，将学习任务分成小块，一步一步地完成。这样可以减轻你的压力，而且能更好地掌握知识。\"}, {\"client\": \"听起来不错，我会试一试的。还有关于教资面试，我真的有点害怕。觉得自己的表现可能不够好。\"}, {\"counselor\": \"不要担心，教资面试是一次宝贵的经历。你可以多练习演讲或者展示课，提前准备好你想表达的内容。相信自己，你一定会有一份出色的表现的。\"}, {\"client\": \"谢谢你的鼓励，我会好好准备的。但是我还是怕自己遇到困难时会崩溃，不知道该怎么办。\"}, {\"counselor\": \"如果你感到自己无法控制情绪，不妨试试一些放松的方法，比如运动或者听一些舒缓的音乐。同时，和朋友或者家人聊聊，分享自己的困扰，他们会给你一些建议和支持的。\"}, {\"client\": \"好的，我会尽量放松自己，多倾诉，多寻求帮助。谢谢你的建议和支持。\"}, {\"counselor\": \"不用客气，我会一直支持你的。相信自己，你一定可以度过这个难关的。加油！如果还有其他问题，随时来找我聊聊。\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number>=3:break\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead00b44-f6e4-4b68-beda-aa5a812e1bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversation\": [{\"User\": \"我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是朋友，他们总是不遗余力地展示他们的成就，比如晋升了职位、获得了奖项、完成了一项重要项目等等。每次听到这些消息，我就会感到自己的成就相对较少，甚至开始怀疑自己的能力和价值。我明白每个人都有自己的成就和努力，但是我无法摆脱对比和竞争的心态。这使得我无法真正享受自己的成就，而是只关注别人取得的更大的成功。我知道这样的想法是不健康的，但我很难控制自己的情绪和思维。这种苦恼对我的心理和情绪造成了很大的压力。我感到自己陷入了一个比较和竞争的漩涡中，无法真正放松和快乐。而且，这种情绪也影响了我与他人的关系，我开始对别人的成功感到嫉妒和羡慕，而不是真心为他们感到高兴。我希望能够找到一种方法来摆脱这种不健康的心态，开始真正关注自己的成就和价值。也许我需要更多的自我接受和肯定，以及学会与他人分享快乐和成功，而不是将其作为竞争的对象。我知道这并不容易，但我相信只有这样，我才能真正摆脱这个苦恼，过上更加平静和满足的生活。\"}, {\"Assistance\": \"首先，我想感谢你对自己情绪的诚实和开放。你正在经历的这种情况在心理学中被称为\\\"社会比较理论\\\"，这是一种人们倾向于将自己与他人进行比较，以评估自己的能力和社会地位的现象。这种比较可能导致\\\"自我评价威胁\\\"，即当你感觉自己在某个领域与他人相比较差时，可能会对自我价值感产生负面影响。\\n\\n你的情况也可能涉及到\\\"成就动机\\\"的问题，这是指人们追求成功和避免失败的内在驱动力。过度的成就动机可能导致过度竞争和过度关注他人的成就，而忽视了自己的内在价值和满足感。\\n\\n我建议你尝试以下几种策略来应对这种情况：\\n\\n1. 自我接纳：这是指接受自己的优点和缺点，而不是过度关注他人的评价。你可以通过自我反思和自我肯定的练习来提高自我接纳。\\n\\n2. 重新定义成功：成功并不仅仅是职业晋升或者获得奖项，它也可以是个人成长、建立良好的人际关系、保持健康等。你可以尝试重新定义你的成功标准，以更全面和平衡的方式来评价自己。\\n\\n3. 正向心理学：这是一种关注人们如何实现满足感和幸福感的心理学分支。你可以通过学习和实践正向心理学的策略，比如感恩、正念、乐观等，来提高你的幸福感和满足感。\\n\\n4. 寻求专业帮助：如果你发现自己很难改变这种心态，可能需要寻求心理咨询师的帮助。他们可以提供专业的心理治疗，帮助你理解和处理这种情绪。\\n\\n最后，我想告诉你，每个人都有自己的节奏和路径，你的价值并不取决于你与他人的比较结果。你是独一无二的，你的成就和价值是无法被他人替代的。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"最近我总是感到自己的好胜心过于强烈，特别是当我看到别人炫耀自己的成就时，我会感到非常不适。他们似乎能够轻而易举地取得成功，而我却感觉自己总是在努力奋斗却无法达到他们的水平。这种感觉让我感到非常沮丧和失落，而且我越发意识到自己对于外界的评判和认可过于重视。每当我看到别人在社交媒体上晒出自己的成就时，我就会感到自己的努力不够，甚至开始怀疑自己的价值。这种情绪开始影响到我的日常生活和工作。我变得焦虑不安，总是试图超越别人，以证明自己的价值。我也意识到这种竞争心态对于自己的心理健康和人际关系都带来了负面影响。我真的希望能够改变这种心态，摆脱对于别人认可的追求。我知道这只会给自己带来更多的压力和困扰。但是面对现实，我却不知从何入手。我需要找到一种平衡，既能保持自己的努力和进步，又能够接受自己的不完美。我希望能够学会更加积极地看待自己和他人，不再用比较和竞争的眼光去评判自己的价值。我需要找到一种内心的平静和满足感，不受外界的干扰和压力。但是这个过程可能会需要一段时间和努力。我知道这个问题可能看起来微不足道，但对于我来说，它却成了一个困扰我很久的心结。我希望能够通过面对和解决这个问题，获得更多的成长和自信，以及更加积极健康的心态。\"}, {\"Assistance\": \"你的问题并不微不足道，相反，这是一种被称为\\\"比较焦虑\\\"的常见心理现象。这种现象源于我们的社会比较理论，即我们通过比较自己与他人来评估自己的价值。当我们感觉自己无法与他人相提并论时，就会产生焦虑和不安。同时，你的描述也显示出一种\\\"成就导向\\\"的心态，这种心态使你过度关注成就和成功，而忽视了自身的感受和需求。\\n\\n首先，我想告诉你，你并不孤单。在这个社交媒体盛行的时代，我们都容易陷入这种比较的陷阱。但是，我们必须意识到，社交媒体上的生活只是人们生活的一部分，而且往往是最好的一部分。我们不能把自己与他人的\\\"最佳剪辑\\\"进行比较。\\n\\n其次，你需要学会接纳自己的不完美。这是一种被称为\\\"自我接纳\\\"的心理技巧，它可以帮助你接纳自己的优点和缺点，而不是过度关注自己的不足。你可以通过正念冥想、自我反思和自我肯定等方法来提高自我接纳。\\n\\n最后，你可以尝试调整你的价值观。你的价值并不取决于你的成就或他人的认可，而是取决于你的内在品质和你对他人的贡献。你可以通过自我反思和价值观探索等方法来找到你真正的价值观。\\n\\n这个过程可能需要一段时间，但请记住，改变是可能的，而且你已经迈出了第一步。你已经意识到了你的问题，并且有意愿去改变，这是一个非常重要的开始。我建议你寻找一位专业的心理咨询师，他们可以提供更具体的指导和支持。同时，你也可以尝试参加一些自我成长的工作坊或阅读一些相关的书籍，以获得更多的工具和策略。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"我最近遇到的一个苦恼是，我经常被别人的炫耀行为所困扰，这让我感到不舒服。无论是在社交媒体上还是现实生活中，我总能看到别人展示他们的成功、财富和幸福，而这种炫耀往往让我感到自卑和嫉妒。我的好胜心太强了，我常常与他人进行比较，觉得自己不够好，不够成功。这种感觉让我感到焦虑和失落，甚至影响了我的自信心和幸福感。我常常想知道为什么我不能像别人一样取得那么多成就，或者为什么我不能过上那种奢华的生活。我知道这种想法是不健康的，但我很难自控，也很难从这种负面情绪中摆脱出来。我很清楚这种炫耀行为只是别人展示他们的一面，他们可能也有自己的困扰和苦恼，但我仍然无法停止自己的嫉妒和羡慕。这让我感到非常沮丧和无助。这个苦恼对我的生活和工作都产生了负面影响。我常常因为自卑而觉得自己不值得被尊重和认可，这导致我在工作中缺乏自信和动力。我也很难与他人建立良好的人际关系，因为我总是对他人的成功抱有嫉妒情绪，无法真心祝福他们。我意识到这个问题已经严重影响了我的心理健康和情绪状态，我渴望能够找到一种有效的方法来处理这个苦恼，重新找回我的自信和幸福感。\"}, {\"Assistance\": \"我理解你的困扰，你正在经历的可能是“社会比较理论”中的下行比较和上行比较的困扰。下行比较是指我们将自己与表现较差的人进行比较，而上行比较则是我们将自己与表现更好的人进行比较。你的情况主要是上行比较，这种比较往往会导致自卑、嫉妒和焦虑等负面情绪。\\n\\n首先，我想告诉你的是，你并不孤单。在这个社交媒体盛行的时代，许多人都会受到这种“比较焦虑”的困扰。其次，你已经意识到这个问题，这是解决问题的第一步。\\n\\n你可以尝试以下几种方法来应对这个问题：\\n\\n1. 自我接纳：每个人都有自己的优点和缺点，都有自己的成就和挫折。你需要接受自己现在的状态，而不是总是将自己与他人进行比较。\\n\\n2. 限制社交媒体使用：社交媒体是比较焦虑的主要来源。你可以尝试减少使用社交媒体的时间，或者选择关注那些能给你带来正能量的人和事。\\n\\n3. 专注于自我成长：将注意力从他人身上转移到自己身上，专注于自我成长和提升。你可以设定一些个人目标，并努力去实现它们。\\n\\n4. 学习感恩：每天花一些时间去思考和感恩你已经拥有的东西，这可以帮助你从比较中解脱出来，更加珍视自己的生活。\\n\\n5. 寻求专业帮助：如果你发现自己很难摆脱这种困扰，你可以寻求专业心理咨询师的帮助。\\n\\n最后，我想告诉你的是，每个人都有自己的生活节奏和生活方式，你不需要去比较，也不需要去嫉妒。你只需要做最好的自己，过最好的生活。\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen_conversation.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number>=3:break\n",
    "        print(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133eb19d-803a-4975-8686-a3f40c926ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversation\": [{\"User\": \"我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是朋友，他们总是不遗余力地展示他们的成就，比如晋升了职位、获得了奖项、完成了一项重要项目等等。每次听到这些消息，我就会感到自己的成就相对较少，甚至开始怀疑自己的能力和价值。我明白每个人都有自己的成就和努力，但是我无法摆脱对比和竞争的心态。这使得我无法真正享受自己的成就，而是只关注别人取得的更大的成功。我知道这样的想法是不健康的，但我很难控制自己的情绪和思维。这种苦恼对我的心理和情绪造成了很大的压力。我感到自己陷入了一个比较和竞争的漩涡中，无法真正放松和快乐。而且，这种情绪也影响了我与他人的关系，我开始对别人的成功感到嫉妒和羡慕，而不是真心为他们感到高兴。我希望能够找到一种方法来摆脱这种不健康的心态，开始真正关注自己的成就和价值。也许我需要更多的自我接受和肯定，以及学会与他人分享快乐和成功，而不是将其作为竞争的对象。我知道这并不容易，但我相信只有这样，我才能真正摆脱这个苦恼，过上更加平静和满足的生活。\"}, {\"Assisstant\": \"我完全理解你的苦恼和困惑。看到他人的成功会引发对自己的不满和自卑感，这是很正常的情绪反应。社交和工作场合中的竞争氛围很容易让我们陷入比较和对抗的心态。但是，请放心，你并不孤单，许多人都经历过类似的感受。\\n\\n首先，我想告诉你，你的成就和价值并不仅仅是与他人的比较有关。每个人的经历和成就都是不同的，我们每个人都有自己的优点和才能。成功是多维度的，不仅仅局限于职位晋升或奖项。你可以尝试关注自己在工作和生活中的成长，寻找那些让你感到满足和自豪的事物。\\n\\n此外，记住与他人分享快乐和成功是非常重要的。当我们能够真心为他人感到高兴时，我们也会感到更加快乐和满足。尝试着与别人建立积极的互动和合作，与他们分享你的成就和喜悦。这样不仅可以改善你与他人的关系，也会增加你的社交支持网络。\\n\\n同时，我建议你关注并培养你自己的兴趣爱好和个人发展。找到你自己热爱和擅长的领域，并为此努力奋斗。这样，你会发现更多属于自己的成功和成就感，不再过分依赖他人的认可和比较。\\n\\n最后，请记住，人生是一种持续的成长和探索过程，每个人都有自己的时机和步调。不要把自己与他人过度对比，相信自己的能力和潜力。放下对比和竞争的心态，用更加积极和宽容的眼光看待自己和他人。我相信，你能够摆脱这种不健康的心态，找到更加平静和满足的生活。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是朋友，他们总是不遗余力地展示他们的成就，比如晋升了职位、获得了奖项、完成了一项重要项目等等。每次听到这些消息，我就会感到自己的成就相对较少，甚至开始怀疑自己的能力和价值。我明白每个人都有自己的成就和努力，但是我无法摆脱对比和竞争的心态。这使得我无法真正享受自己的成就，而是只关注别人取得的更大的成功。我知道这样的想法是不健康的，但我很难控制自己的情绪和思维。这种苦恼对我的心理和情绪造成了很大的压力。我感到自己陷入了一个比较和竞争的漩涡中，无法真正放松和快乐。而且，这种情绪也影响了我与他人的关系，我开始对别人的成功感到嫉妒和羡慕，而不是真心为他们感到高兴。我希望能够找到一种方法来摆脱这种不健康的心态，开始真正关注自己的成就和价值。也许我需要更多的自我接受和肯定，以及学会与他人分享快乐和成功，而不是将其作为竞争的对象。我知道这并不容易，但我相信只有这样，我才能真正摆脱这个苦恼，过上更加平静和满足的生活。\"}, {\"Assisstant\": \"我完全理解你的苦恼。在现代社会中，人们经常会被周围人的成就和成功所影响，特别是在工作场合和社交场合。这种不断比较和竞争的心态往往会给我们带来压力和不安。你已经意识到这种心态对你的心理和情绪产生了负面影响，这是一个很重要的第一步。\\n\\n首先，我想告诉你的是，每个人的成就和成功都是不同的，不能简单地用比较来衡量。每个人都有自己的才华和价值，你也是一样。你的成就和努力同样值得被关注和肯定。请记住，你的价值不仅仅取决于你在工作和社交方面的成就，还包括你的品德、人性和内在的价值。\\n\\n而对于那些炫耀成就的人，可能他们只是希望得到别人的认可和赞赏。但这并不意味着他们就比你更好或更有价值。你不需要将他们的成功作为自己的竞争对象，而是应该将其作为激励和启示。相信自己的能力和潜力，相信你也能取得自己独特的成就。\\n\\n此外，学会自我接受和肯定也是非常重要的。我们每个人都有自己的优点和不足，接受自己的独特之处，不要总是对自己过于苛求。尝试给自己一些鼓励和赞美，不要总是把注意力放在别人身上。关注自己的成就和进步，不要总是想着与他人相比。\\n\\n最后，与他人分享快乐和成功是建立良好人际关系的重要一环。当我们真心为他人感到高兴时，我们的心情也会变得积极和开心。试着去关注别人的成功，并为他们感到骄傲和喜悦。这不仅会帮助你建立更好的人际关系，也会让你更加快乐和满足。\\n\\n我希望这些建议对你有所帮助。记住，你的成就和价值是无法用比较来衡量的。相信自己，相信你的能力和潜力。只有当你真正接受自己并专注于自己的成就时，你才能摆脱这种不健康的心态，过上更加平静和满足的生活。如果你还有其他问题或需要进一步的支持，随时告诉我。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是朋友，他们总是不遗余力地展示他们的成就，比如晋升了职位、获得了奖项、完成了一项重要项目等等。每次听到这些消息，我就会感到自己的成就相对较少，甚至开始怀疑自己的能力和价值。我明白每个人都有自己的成就和努力，但是我无法摆脱对比和竞争的心态。这使得我无法真正享受自己的成就，而是只关注别人取得的更大的成功。我知道这样的想法是不健康的，但我很难控制自己的情绪和思维。这种苦恼对我的心理和情绪造成了很大的压力。我感到自己陷入了一个比较和竞争的漩涡中，无法真正放松和快乐。而且，这种情绪也影响了我与他人的关系，我开始对别人的成功感到嫉妒和羡慕，而不是真心为他们感到高兴。我希望能够找到一种方法来摆脱这种不健康的心态，开始真正关注自己的成就和价值。也许我需要更多的自我接受和肯定，以及学会与他人分享快乐和成功，而不是将其作为竞争的对象。我知道这并不容易，但我相信只有这样，我才能真正摆脱这个苦恼，过上更加平静和满足的生活。\"}, {\"Assisstant\": \"我完全理解你目前的苦恼。在现代社会中，我们经常会面临来自周围人的成就压力，尤其是在工作和社交场合。当我们看到别人取得成功时，很容易感到自卑和沮丧，甚至开始怀疑自己的能力和价值。这是一个非常常见的问题，许多人都会经历类似的情绪困扰。\\n\\n首先，我想告诉你，你并不孤单。很多人都会有相似的情绪和思维模式。每个人的成就和成功都是不同的，不能简单地通过比较来衡量自己的价值。每个人都有自己独特的旅程和目标，在这个过程中取得的成就也是独一无二的。我们不能只关注别人的成功，而忽视了自己的成就。\\n\\n其次，要记住每个人都有不同的起点和机会。有些人可能在某些方面取得了较大的成就，但这并不意味着你就输在起跑线上。每个人有自己的优势和劣势，我们需要学会接受自己的不完美，并努力发挥自己的潜力。不要忘记，成功是一个长期的过程，而不是一时的结果。\\n\\n此外，与他人分享快乐和成功是一种积极的态度。当我们能够真心为他人感到高兴时，我们会发现自己的快乐也会增加。试着将自己的成就与他人分享，充分体验快乐的同时，也可以得到更多的理解和支持。不要把别人的成功视为竞争的对象，而是将其视为一种激励和启示，鼓励自己不断进步和成长。\\n\\n最重要的是，要学会自我接受和肯定。每个人都有自己的独特之处，每个人都是独一无二的。不要把自己与别人进行过分的比较，要相信自己的价值和能力。当我们能够真正接受和喜爱自己时，才能摆脱不健康的心态，过上更加平静和满足的生活。\\n\\n希望这些建议对你有所帮助。如果你需要进一步的支持和倾诉，我会一直在这里陪伴着你。记住，你并不孤单，我们可以一起克服这个困扰，迈向更加积极和幸福的生活。\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/2500_query.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number>=3:break\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6a2db4-b152-489a-a3ea-d9ef8a226471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversation\": [{\"User\": \"给出三个保持健康的提示。\"}, {\"Assistant\": \"1.饮食要均衡，确保包括足够的水果和蔬菜。\\n2.定期运动以保持身体活跃和强壮。\\n3.保持充足的睡眠并保持一致的睡眠时间表。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"什么是三原色？\"}, {\"Assistant\": \"三原色是红色、蓝色和黄色。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"描述一个原子的结构。\"}, {\"Assistant\": \"一个原子由核和绕核运动的电子组成。核内含有质子和中子，质子和中子带有正电荷，而电子带有负电荷，因此整个原子是中性的。每个粒子的数量决定了原子的原子序数和种类。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"我们如何减少空气污染？\"}, {\"Assistant\": \"有许多方法可以减少空气污染，例如转向可再生能源，鼓励使用公共交通工具，禁止燃烧化石燃料，实施减少工业排放的政策，以及实施车辆排放标准。此外，个人可以通过减少汽车使用，避免燃烧木材等物质，更换高效节能的家电来减少空气污染。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"描述做出艰难决定的时刻。\"}, {\"Assistant\": \"我曾经在一家建筑公司担任项目经理的时候，需要在特定日期之前完成一个项目以达到客户的期望。然而，由于意外延误，我们无法按时完成项目，所以我必须做出艰难的决定。我决定将截止日期延长，但这样会使团队资源更加紧张，预算也增加了。虽然这是一项冒险的决定，最终决定继续推进，确保项目能够按时完成并满足客户的期望。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"辨别其中不同的一个。Twitter, Instagram, Telegram\"}, {\"Assistant\": \"Telegram\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"解释为什么下列分数等同于 1/44/16\"}, {\"Assistant\": \"分数4/16等同于1/4，因为分子和分母都可以被4整除。将分子和分母都除以4得到1/4。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"用第三人称叙述写一个关于主人公必须做出重要职业决策的简短故事。\"}, {\"Assistant\": \"约翰的人生到了一个十字路口。他刚刚大学毕业，现在面临一个重要的决策，要决定要从事什么职业。经过多方考虑，他决定成为一名会计师，帮助那些财务上有困难的人。他一直很善于处理数字，并享受看到自己工作的实际成果。\\n\\n约翰报名参加了会计课程，最初发现它很具有挑战性。他必须快速学习多个系统和法规，但他很努力，最终在学业上取得了优异的成绩。几年后，约翰开始在城市的一家会计事务所工作。他渴望将他的税务和会计知识应用于实际的工作环境中。\\n\\n约翰热爱他的工作，因为它让他有机会寻找策略为客户节省费用。在公司几年后，他成为一名高级会计师，开始处理更大更具挑战性的案件。他现在是金融行业中备受尊敬的人物，但他仍然记得当他只是一个刚刚毕业的大学生，对自己的未来充满迷茫。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"渲染一个房屋的三维模型。\"}, {\"Assistant\": \"<nooutput> GPT模型无法完成此类指令。\"}]}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"评估这句话的拼写和语法错误。He finnished his meal and left the resturant\"}, {\"Assistant\": \"He finished his meal and left the restaurant.\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/alpaca_chinese_dataset/chinese_data.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number>=10:break\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9fd832-73c7-4ab6-83c4-9818757deb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"conversation\": [{\"User\": \"这段文本是否属于体育类别？\"}, {\"Assistant\": \"无法确定，因为没有提供具体的文本内容。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"GDP文章归类到以下哪个类别：政治、经济、文化、体育？\"}, {\"Assistant\": \"经济\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"这个新闻属于哪个领域？\"}, {\"Assistant\": \"作为AI语言模型，我无法确定您指的是哪个新闻。请提供更多信息。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"哪些国家是联合国安理会常任理事国？\"}, {\"Assistant\": \"联合国安理会常任理事国是指五个国家，分别是中国、法国、俄罗斯、英国和美国。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"明天北京的最高气温是多少度？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法实时获取天气信息。建议您查看天气预报或相关网站获取最新天气信息。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"近五年恒生电子每个季度的股票收盘价是多少？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我没有实时的股票数据。请您前往相关的股票交易平台或金融网站查询。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"查询明天上海的天气状况是怎样的？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法提供实时的天气信息。建议您查询天气预报网站或应用程序以获取最新的天气状况。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请问您所从事的专业领域是什么？\"}, {\"Assistant\": \"我是一名人工智能语言模型，没有专业领域，但我可以回答各种问题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"能否分析某个特定话题上，哪些人是该领域的知名博主或网红？他们的影响力如何？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法提供当前时事和特定话题的信息。请您自行搜索相关信息或咨询相关领域的专业人士。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请根据我的喜好推荐一些值得观看的电影。\"}, {\"Assistant\": \"很抱歉，作为AI语言模型，我无法知道您的喜好，请您提供更多信息，例如您喜欢的电影类型、导演、演员等等，这样我才能更好地为您推荐电影。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请问印度的首都是哪里？\"}, {\"Assistant\": \"印度的首都是新德里（New Delhi）。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据下列线索，推断凶手是谁：作案时的时间、地点、死者的身份、凶器。\"}, {\"Assistant\": \"抱歉，无法推断凶手的身份。需要更多的线索和证据才能进行推断。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"将以下电影按照评分高低排序：《肖申克的救赎》、《霸王别姬》、《阿甘正传》、《这个杀手不太冷》、《泰坦尼克号》\"}, {\"Assistant\": \"1.《肖申克的救赎》\\n2.《阿甘正传》\\n3.《霸王别姬》\\n4.《这个杀手不太冷》\\n5.《泰坦尼克号》\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请修改这段文字中的语病或错别字：她的事迹感人至深，被誉为“人间瑰宝”。\"}, {\"Assistant\": \"这段文字没有语病或错别字。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"能否给我推荐一部最近上映的悬疑片，评分在8分以上？\"}, {\"Assistant\": \"《追凶者也》是一部评分超过8分的最近上映的悬疑片。该电影由陈思诚执导，王千源、张译、包贝尔等人主演。影片讲述了一名警察在追捕连环杀手的过程中，逐渐发现自己和杀手之间的联系，引人入胜，悬念迭起。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请问红旗渠修建的时间和地点分别是什么？\"}, {\"Assistant\": \"红旗渠修建的时间是1960年至1972年，地点位于中国甘肃省临洮县至陇南市之间的黄土高原地区。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"NLP 领域有哪些常用的文本分类算法？\"}, {\"Assistant\": \"1. 朴素贝叶斯分类器（Naive Bayes Classifier）\\n2. 支持向量机（Support Vector Machine，SVM）\\n3. 决策树（Decision Tree）\\n4. 随机森林（Random Forest）\\n5. 最大熵模型（Maximum Entropy Model，MaxEnt）\\n6. 神经网络（Neural Network）\\n7. 深度学习模型（如卷积神经网络、循环神经网络等）\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给我一个图像分类任务，输出照片中的物品是什么？\"}, {\"Assistant\": \"图像分类任务：识别照片中的动物种类。\\n\\n输入：一张包含动物的照片。\\n\\n输出：照片中动物的种类，例如狗、猫、鸟、鱼等。\\n\\n解决方案：使用深度学习模型，如卷积神经网络（CNN），对照片进行训练和分类。首先，需要准备一个包含各种动物的图像数据集，并将其分为训练集和测试集。然后，使用CNN模型对训练集进行训练，调整模型参数以提高准确性。最后，使用测试集对模型进行评估，并对新的照片进行分类。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"将这段录音转化为文字格式。\"}, {\"Assistant\": \"抱歉，我无法回答该问题，因为没有提供录音文件。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何评价电影《哪吒》？\"}, {\"Assistant\": \"作为一名AI语言模型，我无法对电影《哪吒》进行情感评价。但是，根据观众和专业评论家的评价，该电影在中国内地和海外都取得了巨大的成功。观众们称赞电影的视觉效果、音乐、故事情节和角色塑造，认为它是一部优秀的动画电影。同时，该电影也引发了一些社会话题的讨论，如家庭教育、人性和命运等。总的来说，电影《哪吒》是一部值得观看和思考的电影。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"对于以下电影评论，“这部电影太棒了，完全超出了我的预期”，这个评论的情感是正面还是负面？\"}, {\"Assistant\": \"正面。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请根据以下内容，将文章进行修改：我们的公司一直在开拓新市场，但最近业绩下滑了。\"}, {\"Assistant\": \"我们的公司一直在不断开拓新市场，但最近的业绩却出现了下滑的趋势。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"能否根据我最喜欢的电影类型，为我推荐几部近期上映的好电影？\"}, {\"Assistant\": \"当然可以，请问您最喜欢的电影类型是什么？\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"全球已经有多少人完成新冠疫苗接种？\"}, {\"Assistant\": \"截至2021年11月22日，全球已经有超过80亿剂次的新冠疫苗被接种。具体的接种人数数据因各国政策和数据统计方式不同而有所差异，但据世界卫生组织（WHO）统计，全球已经有超过40亿人次接种了至少一剂疫苗。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"可以推荐一部具有哲理意味的电影吗？\"}, {\"Assistant\": \"《楚门的世界》（The Truman Show）是一部具有哲理意味的电影，它探讨了现代社会中人类自由意志和真实性的问题。影片讲述了一个名叫楚门的男人，他一直生活在一个由电视节目制作人控制的虚拟世界中，直到他发现真相并试图逃离。这部电影引发了人们对于自由、真实、幸福和人类存在意义的思考。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何将文章中的错别字和语法错误进行修改？\"}, {\"Assistant\": \"1. 仔细阅读文章，找出错别字和语法错误。\\n\\n2. 使用拼写检查工具，检查文章中的拼写错误。\\n\\n3. 使用语法检查工具，检查文章中的语法错误。\\n\\n4. 逐句检查文章，确保每个句子都符合语法规则。\\n\\n5. 重新阅读文章，确保修改后的文章流畅、易懂。\\n\\n6. 请他人帮忙检查文章，以确保文章中没有遗漏的错误。\\n\\n7. 最后，将修改后的文章保存并重新提交。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何设置智能灯泡自动开启时间？\"}, {\"Assistant\": \"要设置智能灯泡自动开启时间，您需要按照以下步骤操作：\\n\\n1. 下载并安装智能灯泡的控制应用程序，例如 Philips Hue、LIFX 或 Yeelight。\\n\\n2. 打开应用程序并连接到您的智能灯泡。\\n\\n3. 在应用程序中找到“定时器”或“计划”选项。\\n\\n4. 创建一个新的计划或定时器，选择您希望灯泡开启的时间和日期。\\n\\n5. 设置灯泡的亮度和颜色，如果需要的话。\\n\\n6. 保存您的设置并启用计划或定时器。\\n\\n现在，您的智能灯泡将在您设置的时间自动开启。如果您需要更改计划或定时器，只需返回应用程序并编辑它们即可。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"哪里可以找到附近的美食餐厅？\"}, {\"Assistant\": \"您可以使用以下方法找到附近的美食餐厅：\\n\\n1. 使用搜索引擎或地图应用程序，输入您所在的位置和“美食餐厅”，即可找到附近的餐厅。\\n\\n2. 使用美食评价网站，如Yelp、TripAdvisor或Zomato，查找您所在地区的餐厅，并查看其他用户的评价和建议。\\n\\n3. 使用社交媒体平台，如Instagram或Facebook，搜索您所在地区的餐厅，并查看其他用户的照片和评论。\\n\\n4. 向当地的居民或旅游信息中心咨询，他们可能会提供一些当地的美食餐厅推荐。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何将这篇文章的语气转变为更加正式？\"}, {\"Assistant\": \"以下是一些可能的修改：\\n\\n原文：Hey guys, today we're going to talk about how to make the perfect cup of coffee.\\n\\n正式版：Good day, ladies and gentlemen. Today, we shall discuss the art of crafting the ideal cup of coffee.\\n\\n原文：First things first, you need to choose the right beans.\\n\\n正式版：To commence, it is imperative to select the appropriate coffee beans.\\n\\n原文：Next, grind the beans to the desired consistency.\\n\\n正式版：Subsequently, proceed to grind the beans to the preferred granularity.\\n\\n原文：Now it's time to brew the coffee.\\n\\n正式版：At this juncture, it is time to commence the coffee brewing process.\\n\\n原文：Finally, sit back and enjoy your delicious cup of coffee!\\n\\n正式版：Lastly, savor the exquisite flavor of your meticulously crafted cup of coffee.\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给我推荐一个适合约会的高档西餐厅。\"}, {\"Assistant\": \"我推荐您去尝试一下“Jean-Georges Shanghai”这家高档西餐厅。这家餐厅位于上海外滩三号，拥有壮丽的黄浦江景观，氛围优雅，菜品精致，是一个非常适合约会的地方。餐厅的厨师长Jean-Georges Vongerichten是一位享誉全球的米其林三星大厨，他的菜品以创意和精致著称，其中最受欢迎的菜品包括黑松露汁烤鸡、和牛肉、法式焦糖布丁等。餐厅的服务也非常周到，让您和您的伴侣可以尽情享受美食和浪漫的氛围。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据文本内容和作者的历史背景，分析出该文本所属的文学流派及其特点。\"}, {\"Assistant\": \"抱歉，我无法回答该问题，因为缺少具体的文本内容和作者的历史背景。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据以下症状，这名患者可能患有哪种疾病：头痛、恶心、呕吐、眼睑水肿、视力模糊？\"}, {\"Assistant\": \"这些症状可能是青光眼的表现。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请编辑以下段落来强调其中的关键点。\"}, {\"Assistant\": \"以下是原始段落：\\n\\nThe new product line we are launching is designed to meet the needs of our customers. We have conducted extensive market research and have identified the key features that our customers are looking for. Our team has worked tirelessly to develop a range of products that not only meet these needs but exceed them. We are confident that this new product line will be a game-changer in the industry and will set a new standard for quality and innovation.\\n\\n编辑后的段落：\\n\\nOur new product line is the result of extensive market research and customer feedback. We have identified the key features that our customers are looking for and have exceeded their expectations with our innovative range of products. Our team has worked tirelessly to ensure that this new product line sets a new standard for quality and innovation in the industry. We are confident that it will be a game-changer and meet the needs of our customers like never before.\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请按照情节和主题将以下电影进行分类。《攻壳机动队》、《阿凡达》、《星际穿越》、《头号玩家》、《黑客帝国》。\"}, {\"Assistant\": \"科幻类：《攻壳机动队》、《阿凡达》、《星际穿越》、《头号玩家》、《黑客帝国》。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何使用Python编写代码来进行中文分词？\"}, {\"Assistant\": \"可以使用Python中的jieba库来进行中文分词。以下是一个简单的示例：\\n\\n```python\\nimport jieba\\n\\ntext = \\\"我爱北京天安门\\\"\\nseg_list = jieba.cut(text, cut_all=False)\\nprint(\\\" \\\".join(seg_list))\\n```\\n\\n输出结果为：\\n\\n```\\n我 爱 北京 天安门\\n```\\n\\n其中，`jieba.cut`函数的第一个参数为待分词的文本，第二个参数`cut_all`表示是否采用全模式分词（默认为False，即采用精确模式分词）。`jieba.cut`函数返回一个可迭代的生成器，每次迭代返回一个分词结果。最后使用`\\\" \\\".join(seg_list)`将分词结果拼接成一个字符串输出。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何将一个字符串中的所有大写字母替换为小写字母？\"}, {\"Assistant\": \"可以使用Python中的字符串方法lower()将所有大写字母替换为小写字母。示例代码如下：\\n\\n```python\\nstring = \\\"Hello World\\\"\\nstring = string.lower()\\nprint(string)\\n```\\n\\n输出结果为：\\n\\n```\\nhello world\\n```\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"Twitter上有哪些最流行的话题？\"}, {\"Assistant\": \"作为AI语言模型，我无法提供最新的Twitter话题，但以下是一些常见的Twitter话题：\\n\\n1. #COVID19 - 关于新冠病毒的话题\\n2. #BlackLivesMatter - 关于种族平等和社会正义的话题\\n3. #MeToo - 关于性别平等和性骚扰的话题\\n4. #Brexit - 关于英国脱欧的话题\\n5. #ClimateChange - 关于气候变化和环境保护的话题\\n6. #Election2020 - 关于美国总统选举的话题\\n7. #WorldCup - 关于世界杯足球赛的话题\\n8. #GameOfThrones - 关于《权力的游戏》的话题\\n9. #Oscars - 关于奥斯卡金像奖的话题\\n10. #TBT - Throwback Thursday，回顾过去的话题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举三种常见的有机分子，它们在生物体中的作用是什么？\"}, {\"Assistant\": \"1. 葡萄糖：葡萄糖是一种单糖，是生物体内最重要的能量来源之一。它可以通过细胞呼吸产生能量，供给细胞进行各种生命活动。\\n\\n2. 脂肪酸：脂肪酸是一种长链脂肪酸，是生物体内的重要能量来源之一。它可以被分解为乙酰辅酶A，进入细胞呼吸过程，产生能量。\\n\\n3. 氨基酸：氨基酸是构成蛋白质的基本单元，是生物体内的重要物质。它可以通过蛋白质合成，构成各种生物体内的结构和功能蛋白质，如酶、激素、抗体等。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给我一个单词，让 GPT-3.5 输出这个单词的反义词。\"}, {\"Assistant\": \"单词：善良\\nGPT-3.5 输出：邪恶\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请将这篇英文翻译成中文。\"}, {\"Assistant\": \"Sorry, as an AI language model, I need the English text to translate it into Chinese. Please provide me with the English text.\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举出至少三种不同的主菜类别，并分别给出一到两个典型代表菜单。\"}, {\"Assistant\": \"1. 肉类主菜：烤牛排、烤鸡腿\\n\\n2. 海鲜主菜：烤龙虾、烤鲑鱼\\n\\n3. 素食主菜：烤蔬菜串、烤豆腐卷\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"能否描述一下你最喜欢的度假胜地是什么样子的？\"}, {\"Assistant\": \"作为AI语言模型，我没有个人喜好，但我可以描述一下一般人喜欢的度假胜地的特点。一般人喜欢的度假胜地应该是一个美丽的地方，有着宜人的气候和自然风光，例如海滩、山脉、湖泊或者森林等等。这个地方应该有丰富的文化和历史背景，可以提供各种各样的活动和娱乐，例如水上运动、徒步旅行、文化参观等等。此外，这个地方应该有舒适的住宿条件和美食，可以让人们放松身心，享受愉快的假期。总之，一个理想的度假胜地应该是一个让人们感到放松、愉快和充满活力的地方。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请将以下商品进行分类：牙膏、洗衣液、电视机、抽纸。\"}, {\"Assistant\": \"日用品类：牙膏、洗衣液、抽纸。\\n家电类：电视机。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"针对以下歌曲，请将它们归类到不同的音乐类型中：1. Rolling in the Deep - Adele 2. Hotel California - Eagles 3. Billie Jean - Michael Jackson 4. Bohemian Rhapsody - Queen 5. Stairway to Heaven - Led Zeppelin\"}, {\"Assistant\": \"1. Rolling in the Deep - Adele: Pop/Soul\\n2. Hotel California - Eagles: Rock\\n3. Billie Jean - Michael Jackson: Pop/Funk\\n4. Bohemian Rhapsody - Queen: Rock/Opera\\n5. Stairway to Heaven - Led Zeppelin: Rock/Blues\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"将以下新闻划分到合适的类别：1. 政治家发表演讲，宣布重大政策；2. 美国股市持续下跌，投资者担忧经济前景；3. 某公司发布新产品，受到市场热烈追捧；4. 多地爆发新冠疫情，政府加强防控措施。\"}, {\"Assistant\": \"1. 政治/政策类新闻\\n2. 经济类新闻\\n3. 商业类新闻\\n4. 健康类新闻\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给我一张图片，告诉我里面是什么物体？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法提供图片。请提供更具体的信息，我会尽力回答您的问题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请问目前最常用的自然语言处理技术有哪些？\"}, {\"Assistant\": \"目前最常用的自然语言处理技术包括：\\n\\n1. 分词：将文本分割成单词或词组。\\n\\n2. 词性标注：为每个单词标注其词性，如名词、动词、形容词等。\\n\\n3. 句法分析：分析句子的结构，包括主语、谓语、宾语等。\\n\\n4. 语义分析：理解文本的意义，包括命名实体识别、情感分析等。\\n\\n5. 机器翻译：将一种语言的文本翻译成另一种语言的文本。\\n\\n6. 问答系统：回答用户提出的问题。\\n\\n7. 文本分类：将文本分为不同的类别，如新闻分类、垃圾邮件过滤等。\\n\\n8. 文本生成：生成自然语言文本，如自动摘要、机器写作等。\\n\\n9. 对话系统：与用户进行自然语言交互，如智能客服、聊天机器人等。\\n\\n10. 文本挖掘：从大量文本中提取有用的信息，如关键词提取、主题分析等。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"将《红楼梦》、《西游记》和《水浒传》分为哪些文学类别？\"}, {\"Assistant\": \"《红楼梦》属于明清小说中的长篇小说，也是中国文学史上的经典之作，被认为是中国古代小说的巅峰之作，同时也是一部文学、哲学、历史、人生观等多方面的综合性作品。\\n\\n《西游记》属于明清小说中的长篇小说，是中国文学史上的四大名著之一，是一部以佛教为背景的神魔小说，同时也是一部描写人性、探讨人生的文学作品。\\n\\n《水浒传》属于明清小说中的长篇小说，是中国文学史上的四大名著之一，是一部以宋朝农民起义为背景的英雄传奇小说，同时也是一部描写人性、探讨人生的文学作品。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何使用Python中的NLTK库对一段英文文本进行词性标注？\"}, {\"Assistant\": \"首先，需要安装NLTK库。可以使用以下命令进行安装：\\n\\n```\\npip install nltk\\n```\\n\\n然后，需要下载NLTK库中的英文语料库。可以使用以下命令进行下载：\\n\\n```python\\nimport nltk\\n\\nnltk.download('punkt')\\nnltk.download('averaged_perceptron_tagger')\\n```\\n\\n接下来，可以使用NLTK库中的`pos_tag`函数对文本进行词性标注。以下是一个示例代码：\\n\\n```python\\nimport nltk\\n\\ntext = \\\"This is a sample sentence.\\\"\\ntokens = nltk.word_tokenize(text)\\ntags = nltk.pos_tag(tokens)\\n\\nprint(tags)\\n```\\n\\n输出结果为：\\n\\n```\\n[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\\n```\\n\\n其中，每个单词后面的标记表示该单词的词性。例如，'DT'表示限定词，'VBZ'表示第三人称单数动词等等。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据新闻的标题和正文，将其归类为政治、经济、文化、社会等不同类型。\"}, {\"Assistant\": \"抱歉，我无法回答该问题，因为没有提供新闻标题和正文。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给我列出五个在超市能够购买到的牌子的饼干\"}, {\"Assistant\": \"1. 奥利奥（Oreo）\\n2. 乐事（Lay's）\\n3. 费列罗（Ferrero）\\n4. 蒙布朗（Mondelez）\\n5. 卡夫（Kraft）\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"什么是计算机视觉？举例说明其中一个应用场景。\"}, {\"Assistant\": \"计算机视觉是指利用计算机技术对图像或视频进行处理和分析，从而实现对图像或视频的理解和识别。计算机视觉可以应用于许多领域，如医学影像分析、安防监控、自动驾驶、机器人视觉等。\\n\\n举例说明其中一个应用场景：安防监控。在安防监控领域，计算机视觉可以通过对监控视频进行分析和处理，实现对异常行为的识别和报警。例如，当有人闯入被监控区域时，计算机视觉系统可以通过对视频中的人体轮廓、运动轨迹等进行分析，判断是否存在异常行为，并及时向安保人员发出警报。此外，计算机视觉还可以通过对监控视频中的车辆、人员等进行识别和跟踪，实现对安全事件的追踪和记录。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"有什么简单的方法可以提高睡眠质量？\"}, {\"Assistant\": \"以下是一些简单的方法可以提高睡眠质量：\\n\\n1. 建立规律的睡眠时间表，每天保持相同的睡眠时间和起床时间。\\n\\n2. 创造一个舒适的睡眠环境，保持房间安静、凉爽、黑暗和舒适。\\n\\n3. 避免在睡前饮用咖啡因和酒精，这些物质会影响睡眠质量。\\n\\n4. 避免在睡前使用电子设备，如手机、电脑等，这些设备会影响睡眠质量。\\n\\n5. 进行适当的运动，但不要在睡前进行剧烈运动。\\n\\n6. 放松身心，可以进行深呼吸、冥想、瑜伽等放松活动。\\n\\n7. 避免在睡前吃过多或过少的食物，可以选择一些易于消化的食物。\\n\\n8. 如果有睡眠障碍，可以考虑咨询医生或专业的睡眠治疗师。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"自然语言处理有哪些经典应用？\"}, {\"Assistant\": \"自然语言处理有很多经典应用，以下是其中一些：\\n\\n1. 机器翻译：将一种语言的文本自动翻译成另一种语言的文本。\\n\\n2. 语音识别：将人类语音转换成文本形式。\\n\\n3. 文本分类：将文本按照主题或类别进行分类。\\n\\n4. 命名实体识别：识别文本中的人名、地名、组织机构名等实体。\\n\\n5. 情感分析：分析文本中的情感倾向，如正面、负面或中性。\\n\\n6. 问答系统：回答用户提出的问题，如智能客服、智能助手等。\\n\\n7. 文本生成：自动生成文本，如自动摘要、机器写作等。\\n\\n8. 信息抽取：从大量文本中提取出有用的信息，如新闻摘要、知识图谱等。\\n\\n9. 机器人聊天：与用户进行自然语言交互，如智能聊天机器人等。\\n\\n10. 语音合成：将文本转换成语音形式。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据这段文字，判断作者的情感是积极还是消极？\"}, {\"Assistant\": \"缺少具体的文字内容，无法判断作者的情感是积极还是消极。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何提高宠物店的客户满意度？\"}, {\"Assistant\": \"以下是提高宠物店客户满意度的几个建议：\\n\\n1. 提供优质的服务：宠物店应该提供优质的服务，包括友好的员工、干净的环境和高质量的产品。\\n\\n2. 建立良好的沟通：宠物店应该与客户建立良好的沟通，了解他们的需求和意见，并及时回复他们的问题和反馈。\\n\\n3. 提供个性化的服务：宠物店应该提供个性化的服务，根据客户的需求和宠物的特点，为他们提供定制化的服务。\\n\\n4. 提供专业的建议：宠物店应该提供专业的建议，帮助客户解决宠物养护中的问题，并提供有用的建议和技巧。\\n\\n5. 提供优惠和促销：宠物店应该提供优惠和促销，吸引客户前来购买，并提高客户的忠诚度。\\n\\n6. 关注客户反馈：宠物店应该关注客户的反馈，及时改进服务和产品，以提高客户满意度。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请修改以下句子以更加简洁明了：因为天气炎热，所以我决定去游泳。\"}, {\"Assistant\": \"天气炎热，我去游泳。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请给出一份适合减肥的每日三餐食谱。\"}, {\"Assistant\": \"早餐：\\n燕麦片或全麦面包片配上低脂牛奶或豆浆，加上一份水果（如苹果、香蕉、葡萄柚等）。\\n\\n午餐：\\n烤鸡胸肉或鱼肉配上蔬菜沙拉（如生菜、胡萝卜、黄瓜等），再加上一份全麦面包片或糙米饭。\\n\\n晚餐：\\n蒸鸡胸肉或鱼肉配上蔬菜（如西兰花、芦笋、菜花等），再加上一份糙米饭或红薯。\\n\\n小食：\\n水果、无糖酸奶、坚果（如核桃、杏仁、腰果等）。\\n\\n注意事项：\\n1. 控制食物的摄入量，避免过量；\\n2. 尽量选择低脂、低糖、高纤维的食物；\\n3. 多喝水，保持身体水分；\\n4. 避免高热量、高脂肪、高糖分的食物；\\n5. 坚持运动，增加身体代谢率。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何分析一份大型数据集？\"}, {\"Assistant\": \"分析一份大型数据集需要以下步骤：\\n\\n1. 确定问题和目标：首先需要明确分析的问题和目标，例如探索数据中的趋势、发现异常值、预测未来趋势等。\\n\\n2. 数据清洗和预处理：对数据进行清洗和预处理，包括去除重复值、缺失值、异常值等。\\n\\n3. 数据探索和可视化：通过可视化工具和统计方法探索数据的分布、相关性、趋势等，以便更好地理解数据。\\n\\n4. 数据建模和分析：根据问题和目标选择合适的建模方法，例如回归分析、聚类分析、决策树等，进行数据建模和分析。\\n\\n5. 结果解释和报告：对分析结果进行解释和报告，包括数据可视化、统计分析、模型评估等，以便更好地向利益相关者传达分析结果。\\n\\n6. 持续监控和改进：对分析结果进行持续监控和改进，以便及时发现问题并进行调整和改进。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"您的产品主要针对哪些人群或市场？\"}, {\"Assistant\": \"作为AI语音助手，我并没有具体的产品，但是我可以为各种不同的人群和市场提供服务。我的语音识别和自然语言处理技术可以为智能家居、智能办公、智能医疗、智能客服等领域提供支持。同时，我也可以为个人用户提供语音助手服务，帮助他们完成各种日常任务，如提醒、查询、翻译等。总的来说，我的服务对象是广泛的，可以为各种不同的人群和市场提供帮助。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"我想做一道简单的菜，有什么好推荐？\"}, {\"Assistant\": \"您可以尝试做炒鸡蛋，这是一道简单易学的菜品。以下是做法：\\n\\n材料：\\n- 鸡蛋2个\\n- 盐适量\\n- 葱花适量\\n- 油适量\\n\\n步骤：\\n1. 打散鸡蛋，加入适量盐和葱花，搅拌均匀。\\n2. 热锅凉油，油热后倒入鸡蛋液，用铲子迅速搅拌，炒至鸡蛋凝固。\\n3. 出锅前再撒上一些葱花即可。\\n\\n炒鸡蛋可以作为早餐、午餐或晚餐的配菜，也可以搭配米饭或面条食用。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列出十种可疑的食品成分或添加剂，以及它们可能对人体造成的潜在危害。\"}, {\"Assistant\": \"1. 高果糖玉米糖浆：可能导致肥胖、糖尿病、心脏病等疾病。\\n2. 人造色素：可能导致过敏、行为异常、肝损伤等问题。\\n3. 反式脂肪：可能导致心脏病、中风、糖尿病等疾病。\\n4. 防腐剂：可能导致过敏、呼吸困难、皮炎等问题。\\n5. 高钠含量：可能导致高血压、心脏病、肾脏疾病等问题。\\n6. 高嘌呤含量：可能导致痛风、肾结石等问题。\\n7. 人造甜味剂：可能导致头痛、恶心、癌症等问题。\\n8. 转基因成分：可能导致过敏、免疫系统问题、癌症等问题。\\n9. 饲料添加剂：可能导致抗生素耐药性、肉毒杆菌感染等问题。\\n10. 饱和脂肪：可能导致心脏病、中风、高胆固醇等问题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请简要介绍一下DNA的结构及其功能。\"}, {\"Assistant\": \"DNA是由四种碱基（腺嘌呤、鸟嘌呤、胸腺嘧啶和鳞状细胞素）组成的双链螺旋结构，其中两条链通过氢键相互连接。DNA的主要功能是存储和传递遗传信息，这些信息编码了生物体的基因组，控制着生物体的生长、发育和功能。DNA的结构和序列决定了基因的表达方式，从而影响了生物体的性状和特征。此外，DNA还可以通过复制和修复机制来维持遗传信息的稳定性和完整性。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请将以下电影按照类型分组：《阿凡达》，《星际穿越》，《泰坦尼克号》，《盗梦空间》，《肖申克的救赎》。\"}, {\"Assistant\": \"科幻类：《阿凡达》，《星际穿越》\\n爱情类：《泰坦尼克号》\\n悬疑类：《盗梦空间》\\n剧情类：《肖申克的救赎》\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请解释一下什么是自然语言处理？\"}, {\"Assistant\": \"自然语言处理（Natural Language Processing，NLP）是一种人工智能技术，旨在使计算机能够理解、解释和生成自然语言。它涉及到计算机对自然语言进行语法、语义、语用等多方面的分析和处理，以便实现自然语言的自动翻译、文本分类、信息抽取、情感分析、问答系统等应用。自然语言处理技术的发展，使得计算机能够更好地与人类进行交互，从而提高人机交互的效率和质量。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"列出三个重要的人工智能领域和每个领域的应用\"}, {\"Assistant\": \"1. 机器学习领域：应用于自然语言处理、图像识别、推荐系统等领域。\\n2. 深度学习领域：应用于计算机视觉、语音识别、自然语言处理、智能游戏等领域。\\n3. 自然语言处理领域：应用于机器翻译、情感分析、问答系统、智能客服等领域。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"假设你和你的朋友正在计划一次旅行，请写出一段对话，其中包括你们决定去的目的地、时间以及活动安排。\"}, {\"Assistant\": \"你：嘿，你想去旅行吗？\\n\\n朋友：当然想啊！你有什么好的目的地推荐吗？\\n\\n你：我听说过一个叫做阳朔的地方，很美，你有兴趣吗？\\n\\n朋友：听起来不错，我们什么时候去？\\n\\n你：我想下个月去，你可以吗？\\n\\n朋友：没问题，我们要呆多久？\\n\\n你：我想呆三天，这样我们可以好好玩一下。\\n\\n朋友：好的，那我们要做些什么活动？\\n\\n你：我们可以去漂流、爬山、骑自行车，还可以去看印象刘三姐。\\n\\n朋友：听起来很刺激，我期待着这次旅行了！\\n\\n你：我也是！我们要开始计划了。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何提高图像识别准确率？\"}, {\"Assistant\": \"以下是提高图像识别准确率的一些方法：\\n\\n1. 数据预处理：对图像进行预处理，如去噪、平滑、增强对比度等，可以提高图像识别的准确率。\\n\\n2. 特征提取：选择合适的特征提取算法，如SIFT、SURF、HOG等，可以提高图像识别的准确率。\\n\\n3. 模型选择：选择合适的模型，如卷积神经网络（CNN）、支持向量机（SVM）等，可以提高图像识别的准确率。\\n\\n4. 数据增强：通过数据增强技术，如旋转、平移、缩放等，可以增加训练数据的多样性，提高图像识别的准确率。\\n\\n5. 超参数调优：通过调整模型的超参数，如学习率、批大小、迭代次数等，可以提高图像识别的准确率。\\n\\n6. 集成学习：通过集成多个模型的预测结果，如投票、平均等，可以提高图像识别的准确率。\\n\\n7. 迁移学习：通过利用已经训练好的模型，如在ImageNet上训练好的模型，可以提高图像识别的准确率。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"哪些是最常见的自然语言处理任务？\"}, {\"Assistant\": \"以下是最常见的自然语言处理任务：\\n\\n1. 分词（Tokenization）：将文本分割成单词或子字符串。\\n\\n2. 词性标注（Part-of-speech tagging）：为每个单词标注其词性，如名词、动词、形容词等。\\n\\n3. 命名实体识别（Named Entity Recognition）：识别文本中的人名、地名、组织机构等实体。\\n\\n4. 句法分析（Parsing）：分析句子的结构，确定单词之间的关系。\\n\\n5. 语义分析（Semantic analysis）：理解文本的意义，包括词义、句义和篇章意义。\\n\\n6. 情感分析（Sentiment analysis）：分析文本的情感倾向，如正面、负面或中性。\\n\\n7. 机器翻译（Machine translation）：将一种语言的文本翻译成另一种语言的文本。\\n\\n8. 文本分类（Text classification）：将文本分为不同的类别，如垃圾邮件和正常邮件。\\n\\n9. 信息抽取（Information extraction）：从文本中提取有用的信息，如事件、关系等。\\n\\n10. 问答系统（Question answering）：回答用户提出的问题，通常需要理解问题的意义并从文本中找到答案。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"根据给出的图片，这是哪种动物？\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法看到图片，无法回答这个问题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举三个正在使用人工智能技术的实际应用场景并简要介绍其功能。\"}, {\"Assistant\": \"1. 语音识别技术：语音识别技术是一种基于人工智能的技术，可以将人类语音转换为计算机可读的文本。它被广泛应用于智能语音助手、语音翻译、语音搜索等领域，使得人们可以通过语音与计算机进行交互，提高了人机交互的效率和便捷性。\\n\\n2. 人脸识别技术：人脸识别技术是一种基于图像识别和模式识别的技术，可以通过摄像头捕捉到的人脸图像进行身份识别、人脸比对、人脸搜索等操作。它被广泛应用于安防领域、金融领域、社交网络等领域，提高了安全性和便捷性。\\n\\n3. 推荐系统技术：推荐系统技术是一种基于数据挖掘和机器学习的技术，可以根据用户的历史行为和偏好，为用户推荐个性化的产品、服务或内容。它被广泛应用于电商、社交网络、音乐、视频等领域，提高了用户体验和销售额。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"你能推荐几家在上海市中心地带的本地美食餐馆吗？\"}, {\"Assistant\": \"1. 老正兴饭店：位于南京东路步行街，是一家有着百年历史的老字号餐馆，以本帮菜和小吃为主，口味地道，价格实惠。\\n\\n2. 小杨生煎：位于黄浦区吴江路的小店，以生煎包为主打菜品，口感鲜美，价格便宜，是上海本地人的最爱。\\n\\n3. 老上海饭店：位于黄浦区南京东路步行街，是一家有着80年历史的老字号餐馆，以本帮菜和小吃为主，口味地道，环境古色古香。\\n\\n4. 阿一鲜肉小笼：位于黄浦区南京东路步行街，是一家以小笼包为主打菜品的小店，口感鲜美，价格实惠，是上海本地人的最爱。\\n\\n5. 老城隍庙小吃街：位于黄浦区城隍庙附近，是一条集中了上海本地小吃的美食街，有着各种口味的小吃，价格实惠，是游客和本地人都喜欢的地方。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请给这句话进行词性标注。\"}, {\"Assistant\": \"我喜欢吃水果。\\n\\n我：代词，主语\\n喜欢：动词，谓语\\n吃：动词，宾语\\n水果：名词，宾语\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请将下列单词按照大小顺序排列：microscopic、tiny、minuscule、puny。\"}, {\"Assistant\": \"microscopic、minuscule、tiny、puny。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请描述一张您最喜欢的照片，包括照片中的元素、构图方式以及色彩运用等方面。\"}, {\"Assistant\": \"抱歉，作为AI语言模型，我无法描述我最喜欢的照片，因为我没有感情和喜好。但是，我可以为您提供一些关于描述照片的提示和建议。\\n\\n当描述一张照片时，您可以从以下几个方面入手：\\n\\n1. 照片中的元素：描述照片中的主体、背景、人物、物品等元素，以及它们之间的关系和互动。\\n\\n2. 构图方式：描述照片的构图方式，包括拍摄角度、视角、对焦点、线条、形状等，以及它们对照片整体效果的影响。\\n\\n3. 色彩运用：描述照片的色彩运用，包括色调、明暗度、饱和度等，以及它们对照片的情感表达和氛围营造的作用。\\n\\n4. 感受和意义：描述您对照片的感受和理解，以及它对您的意义和价值。\\n\\n在描述照片时，您可以运用形象生动的语言，使用比喻、象征等修辞手法，让读者能够感受到照片所传达的情感和意义。同时，您也可以结合照片的背景和拍摄者的意图，对照片进行更深入的分析和解读。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"你能给我推荐一些最近上映的好看电影吗？\"}, {\"Assistant\": \"当然可以，以下是最近上映的一些好看电影：\\n\\n1.《误杀》：一部悬疑犯罪电影，讲述了一个普通人因为一次意外而卷入了一场谋杀案的故事。\\n\\n2.《叶问4》：一部动作片，讲述了叶问在美国的生活和他与美国武术家的对决。\\n\\n3.《囧妈》：一部喜剧电影，讲述了一位妈妈在旅途中的一系列奇遇和成长。\\n\\n4.《姜子牙》：一部动画电影，讲述了中国神话中的姜子牙与纣王的故事。\\n\\n5.《八佰》：一部战争电影，讲述了中国抗日战争中的一次重要战役。\\n\\n希望这些推荐能够帮到你。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举三种不同的自然语言处理任务，并解释它们的应用场景。\"}, {\"Assistant\": \"1. 文本分类：将文本分为不同的类别，例如将新闻文章分为政治、经济、体育等类别。应用场景包括垃圾邮件过滤、情感分析、新闻分类等。\\n\\n2. 命名实体识别：识别文本中的实体，例如人名、地名、组织机构名等。应用场景包括信息抽取、搜索引擎、机器翻译等。\\n\\n3. 机器翻译：将一种语言的文本翻译成另一种语言的文本。应用场景包括跨语言交流、跨国企业合作、跨国电子商务等。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何为网站进行搜索引擎优化？\"}, {\"Assistant\": \"以下是为网站进行搜索引擎优化的一些步骤：\\n\\n1. 关键词研究：确定您的目标受众使用的关键词和短语，并确保您的网站内容包含这些关键词。\\n\\n2. 内容优化：确保您的网站内容是高质量、有用的，并包含您的关键词。使用标题、子标题和段落来组织您的内容，并确保您的网站易于阅读和导航。\\n\\n3. 元标签优化：确保您的网站的标题、描述和关键词标签包含您的关键词，并能够吸引用户点击。\\n\\n4. 内部链接：使用内部链接将您的网站的不同页面连接起来，以便搜索引擎可以更好地了解您的网站结构和内容。\\n\\n5. 外部链接：获取其他网站链接到您的网站，这将有助于提高您的网站在搜索引擎中的排名。\\n\\n6. 社交媒体：使用社交媒体平台来推广您的网站和内容，并与您的受众互动。\\n\\n7. 网站速度：确保您的网站加载速度快，以提高用户体验和搜索引擎排名。\\n\\n8. 移动友好性：确保您的网站在移动设备上易于使用和浏览，以提高搜索引擎排名。\\n\\n9. 分析和监测：使用分析工具来监测您的网站流量和搜索引擎排名，并根据数据进行优化。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"分析过去三年的销售数据和市场趋势，预测下一季度该产品的销售额和市场占有率。\"}, {\"Assistant\": \"很抱歉，作为AI语言模型，我无法进行这样的分析和预测。这需要具备相关领域知识和数据分析能力的专业人士进行深入研究和分析。建议您寻求相关领域的专业人士的帮助。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"你觉得如何评价这家新开的山东饺子店？\"}, {\"Assistant\": \"作为AI语言模型，我无法亲身体验这家山东饺子店的菜品和服务，因此无法对其进行评价。建议您可以通过网上点评、口碑等渠道了解其他消费者的评价和意见，以便做出更准确的判断。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"检测一个文本是否属于新闻、微博还是论坛帖子？\"}, {\"Assistant\": \"可以通过以下方式检测一个文本是否属于新闻、微博还是论坛帖子：\\n\\n1. 新闻通常具有正式的标题、正文、作者、发布时间等元素，而微博和论坛帖子通常没有这些元素。\\n\\n2. 微博和论坛帖子通常包含大量的表情符号、缩写、网络用语等，而新闻则相对正式。\\n\\n3. 微博和论坛帖子通常包含大量的评论和回复，而新闻则相对独立。\\n\\n4. 新闻通常由专业的新闻机构或记者发布，而微博和论坛帖子则可以由任何人发布。\\n\\n综上所述，可以通过文本的格式、语言风格、发布者等方面来判断一个文本是否属于新闻、微博还是论坛帖子。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举几个与您的产品竞品相似的公司或产品。\"}, {\"Assistant\": \"很抱歉，作为AI语言模型，我无法知道您的产品是什么，也无法了解您的竞品是哪些公司或产品。请提供更具体的信息，我会尽力回答您的问题。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列出三种常见的自然语言处理任务，并简要描述其作用。\"}, {\"Assistant\": \"1. 文本分类：将文本分为不同的类别，例如新闻分类、情感分析、垃圾邮件过滤等。其作用是帮助人们快速准确地获取所需信息，提高工作效率。\\n\\n2. 命名实体识别：识别文本中的实体，如人名、地名、组织机构名等。其作用是帮助人们更好地理解文本内容，提高信息的准确性和可靠性。\\n\\n3. 机器翻译：将一种语言的文本翻译成另一种语言的文本。其作用是帮助人们跨越语言障碍，促进不同国家和地区之间的交流和合作。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"给定一个新闻标题，如何使用机器学习算法将其分类到正确的新闻类别？\"}, {\"Assistant\": \"以下是一些可能的步骤：\\n\\n1. 收集和整理数据：收集大量的新闻标题和它们所属的类别，将它们整理成一个数据集。\\n\\n2. 数据预处理：对数据进行清洗、去重、分词、去除停用词等预处理操作，以便机器学习算法更好地理解和处理数据。\\n\\n3. 特征提取：将每个新闻标题转换成一个特征向量，以便机器学习算法可以对其进行分类。常用的特征提取方法包括词袋模型、TF-IDF、Word2Vec等。\\n\\n4. 选择和训练模型：选择适合该问题的机器学习算法，如朴素贝叶斯、支持向量机、决策树等，并使用训练数据集对其进行训练。\\n\\n5. 模型评估和调优：使用测试数据集对模型进行评估和调优，以提高其分类准确率。\\n\\n6. 预测新闻类别：将新闻标题转换成特征向量，并使用训练好的模型对其进行分类，得到其所属的新闻类别。\\n\\n需要注意的是，以上步骤并不是一成不变的，具体的实现方式和参数设置可能因问题的不同而有所差异。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请问有哪些常用的中文自然语言处理工具包？\"}, {\"Assistant\": \"常用的中文自然语言处理工具包包括：\\n\\n1. jieba分词：中文分词工具，支持多种分词模式和自定义词典。\\n\\n2. SnowNLP：中文文本情感分析工具，支持情感倾向分析、文本分类、关键词提取等功能。\\n\\n3. HanLP：综合性中文自然语言处理工具包，支持分词、词性标注、命名实体识别、依存句法分析等多种功能。\\n\\n4. THULAC：中文分词工具，支持词性标注和命名实体识别。\\n\\n5. NLTK：Python自然语言处理工具包，支持中文分词、词性标注、命名实体识别等功能。\\n\\n6. LTP：中文自然语言处理工具包，支持分词、词性标注、命名实体识别、依存句法分析等多种功能。\\n\\n7. PKUSeg：中文分词工具，支持词性标注和命名实体识别。\\n\\n8. FudanNLP：中文自然语言处理工具包，支持分词、词性标注、命名实体识别、依存句法分析等多种功能。\\n\\n9. StanfordNLP：自然语言处理工具包，支持中文分词、词性标注、命名实体识别、依存句法分析等多种功能。\\n\\n10. OpenNLP：自然语言处理工具包，支持中文分词、词性标注、命名实体识别等多种功能。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"针对COVID-19的哪些疫苗已经得到批准？\"}, {\"Assistant\": \"截至2021年5月，以下COVID-19疫苗已经得到世界卫生组织（WHO）或欧洲药品管理局（EMA）的批准：\\n\\n1.辉瑞-生物科技（Pfizer-BioNTech）疫苗（BNT162b2）\\n\\n2.现代（Moderna）疫苗（mRNA-1273）\\n\\n3.阿斯利康（AstraZeneca）疫苗（AZD1222）\\n\\n4.强生（Johnson & Johnson）疫苗（Ad26.COV2.S）\\n\\n5.中国国药集团（Sinopharm）疫苗\\n\\n6.中国科兴中维生物（Sinovac）疫苗\\n\\n此外，还有其他COVID-19疫苗正在进行临床试验和审批过程中。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请分享一道适合初学者的简单菜肴，并提供如何做这道菜的详细步骤。\"}, {\"Assistant\": \"番茄炒蛋\\n\\n材料：\\n- 2个鸡蛋\\n- 1个番茄\\n- 适量油\\n- 适量盐\\n- 适量胡椒粉\\n- 适量葱花\\n\\n步骤：\\n1. 将番茄切成小块备用。\\n2. 打入鸡蛋，加入适量盐和胡椒粉，搅拌均匀。\\n3. 热锅凉油，将鸡蛋液倒入锅中，用铲子不断翻炒，炒至鸡蛋熟透后盛出备用。\\n4. 热锅凉油，将番茄块倒入锅中，加入适量盐，翻炒至番茄软烂。\\n5. 将炒好的鸡蛋倒入锅中，翻炒均匀。\\n6. 最后撒上适量葱花即可。\\n\\n温馨提示：\\n1. 番茄炒蛋的关键在于翻炒，要不断翻动，避免糊底。\\n2. 可以根据个人口味加入适量的糖，增加番茄的甜味。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"你能否详细介绍一种常见的癌症治疗方式？\"}, {\"Assistant\": \"化疗是一种常见的癌症治疗方式，它使用化学药物来杀死癌细胞。这些药物可以通过口服、注射或静脉注射等方式给予患者。\\n\\n化疗的原理是通过杀死癌细胞来控制或消除癌症。这些药物可以杀死快速分裂的癌细胞，但也会影响正常细胞的生长和分裂。因此，化疗可能会导致一些副作用，如恶心、呕吐、脱发、疲劳、免疫系统抑制等。\\n\\n化疗通常是与其他治疗方式一起使用，如手术、放疗或免疫疗法。治疗方案通常根据患者的癌症类型、病情和身体状况而定。治疗周期和剂量也会根据患者的情况而定，有时需要多个周期的治疗。\\n\\n虽然化疗可以有效地控制或消除癌症，但它也有一些限制。一些癌症类型对化疗不敏感，而且化疗可能会导致一些长期的副作用，如心脏病、肝损伤、神经系统损伤等。因此，医生会根据患者的情况权衡利弊，决定是否使用化疗。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"如何评估一个文本分类器的性能？\"}, {\"Assistant\": \"评估一个文本分类器的性能通常使用以下指标：\\n\\n1. 准确率（Accuracy）：分类器正确分类的样本数占总样本数的比例。\\n\\n2. 精确率（Precision）：分类器正确分类为正例的样本数占分类器分类为正例的样本数的比例。\\n\\n3. 召回率（Recall）：分类器正确分类为正例的样本数占真实正例的样本数的比例。\\n\\n4. F1值（F1-score）：综合考虑精确率和召回率的指标，F1值越高，分类器性能越好。\\n\\n5. ROC曲线和AUC值：ROC曲线是以假正例率（False Positive Rate）为横轴，真正例率（True Positive Rate）为纵轴的曲线，AUC值是ROC曲线下的面积，AUC值越大，分类器性能越好。\\n\\n6. 混淆矩阵（Confusion Matrix）：将分类器的预测结果与真实结果进行对比，可以得到真正例、假正例、真反例、假反例的数量，从而更直观地评估分类器的性能。\\n\\n在评估分类器性能时，需要根据具体的应用场景选择合适的指标。例如，在医疗领域中，精确率可能更重要，因为假阳性会导致不必要的治疗，而在金融领域中，召回率可能更重要，因为假阴性会导致重大损失。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请分享一个适合初学者的意大利面制作食谱？\"}, {\"Assistant\": \"意大利面是一道简单又美味的食物，适合初学者尝试制作。以下是一个简单的意大利面制作食谱：\\n\\n材料：\\n- 意大利面（200克）\\n- 番茄酱（1杯）\\n- 橄榄油（2汤匙）\\n- 大蒜（2瓣）\\n- 洋葱（1个）\\n- 盐和黑胡椒粉（适量）\\n- 意大利香料（适量）\\n- 帕玛森芝士（适量）\\n\\n步骤：\\n1. 将意大利面放入沸水中煮熟，根据包装上的指示时间煮熟。\\n2. 在另一个平底锅中，加入橄榄油，加热后加入切碎的大蒜和洋葱，炒至变软。\\n3. 加入番茄酱，调味盐、黑胡椒粉和意大利香料，煮沸后转小火煮5分钟。\\n4. 将煮熟的意大利面沥干水分，加入锅中，翻炒均匀。\\n5. 撒上帕玛森芝士，即可享用。\\n\\n这是一个简单的意大利面制作食谱，适合初学者尝试。您可以根据自己的口味调整调料的用量，让这道意大利面更符合您的口味。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"按照指定的实体类别，如人名、地名、机构等，从文本中提取出相关实体并输出其数量和具体名称。\"}, {\"Assistant\": \"这个任务可以使用命名实体识别（Named Entity Recognition，简称NER）技术来完成。NER是自然语言处理中的一项重要任务，其目的是从文本中识别出具有特定意义的实体，如人名、地名、机构名等，并将其分类标注。\\n\\n常见的NER算法包括基于规则的方法、基于统计的方法和基于深度学习的方法。其中，基于深度学习的方法在最近几年取得了很大的进展，如使用循环神经网络（Recurrent Neural Network，简称RNN）和长短时记忆网络（Long Short-Term Memory，简称LSTM）等模型来进行实体识别。\\n\\n具体实现时，可以使用已经训练好的NER模型，如Stanford NER、SpaCy等，也可以自己训练一个模型。对于输出实体数量和具体名称，可以通过对NER结果进行统计和整理来实现。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举至少三个 GPT-3.5 相对于其他自然语言处理模型的优势。\"}, {\"Assistant\": \"1. 更高的语言理解能力：GPT-3.5 模型采用了更加先进的预训练技术，能够更好地理解自然语言，包括语法、语义、上下文等方面，从而能够更准确地理解和生成自然语言。\\n\\n2. 更强的生成能力：GPT-3.5 模型具有更强的生成能力，能够生成更加自然、流畅的语言，包括文章、对话、问题回答等方面，从而能够更好地满足用户的需求。\\n\\n3. 更广泛的应用场景：GPT-3.5 模型可以应用于多种自然语言处理任务，包括文本分类、情感分析、机器翻译、问答系统等方面，从而能够更好地满足不同领域的需求。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"描述一下蒸汽机的工作原理。\"}, {\"Assistant\": \"蒸汽机是一种利用蒸汽压力产生动力的机器。其工作原理如下：\\n\\n1. 蒸汽产生：水被加热，产生蒸汽。\\n\\n2. 蒸汽进入缸体：蒸汽进入缸体，推动活塞向前移动。\\n\\n3. 活塞推动连杆：活塞向前移动，推动连杆转动。\\n\\n4. 连杆带动曲轴：连杆转动，带动曲轴转动。\\n\\n5. 曲轴带动机械装置：曲轴转动，带动机械装置（如发电机、水泵等）工作。\\n\\n6. 蒸汽排出：蒸汽排出缸体，进入冷凝器冷却成水。\\n\\n7. 水回流：水回流到锅炉，再次加热产生蒸汽，循环往复。\\n\\n蒸汽机的工作原理可以简单概括为：蒸汽产生、蒸汽进入缸体、活塞推动连杆、连杆带动曲轴、曲轴带动机械装置、蒸汽排出、水回流。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"列举出当前计算机领域中最有前途的技术有哪些？\"}, {\"Assistant\": \"1. 人工智能和机器学习：人工智能和机器学习是当前计算机领域中最有前途的技术之一。这些技术可以帮助计算机系统自动学习和改进，从而提高其性能和效率。\\n\\n2. 区块链技术：区块链技术是一种分布式数据库技术，可以用于安全地存储和传输数据。它可以应用于各种领域，如金融、医疗、物流等。\\n\\n3. 云计算：云计算是一种基于互联网的计算模式，可以提供各种计算资源和服务。它可以帮助企业和个人节省成本，提高效率。\\n\\n4. 物联网技术：物联网技术是一种将物理设备和互联网连接起来的技术，可以实现设备之间的通信和数据共享。它可以应用于各种领域，如智能家居、智能城市等。\\n\\n5. 虚拟现实和增强现实技术：虚拟现实和增强现实技术可以将虚拟世界和现实世界结合起来，为用户提供更加沉浸式的体验。它可以应用于游戏、教育、医疗等领域。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"怎样分析健身房会员增长情况？\"}, {\"Assistant\": \"1. 收集数据：收集健身房的会员增长数据，包括每月或每季度的新会员数量、老会员续费数量、会员流失数量等。\\n\\n2. 分析趋势：将收集到的数据进行分析，观察会员增长的趋势，看是否呈现逐渐增长或逐渐下降的趋势。\\n\\n3. 比较数据：将不同时间段的数据进行比较，看看会员增长情况是否有所改善或恶化。\\n\\n4. 调查原因：如果会员增长情况出现了不良趋势，需要进行调查，找出原因。可能是因为竞争对手的出现、服务质量下降、价格过高等原因。\\n\\n5. 制定计划：根据分析结果，制定相应的计划，如提高服务质量、降低价格、加强宣传等，以促进会员增长。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"你能否列举一些当前流行的电子商务平台？\"}, {\"Assistant\": \"当然可以，以下是一些当前流行的电子商务平台：\\n\\n1. 亚马逊（Amazon）\\n2. 淘宝（Taobao）\\n3. 天猫（Tmall）\\n4. 京东（JD.com）\\n5. eBay\\n6. Walmart\\n7. Shopify\\n8. Wish\\n9. 美团（Meituan）\\n10. 拼多多（Pinduoduo）\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请描述亚历山大大帝的征服行动中最关键的一次战役是哪场？\"}, {\"Assistant\": \"亚历山大大帝的征服行动中最关键的一次战役是在公元前331年的伊苏斯战役。这场战役是亚历山大大帝与波斯帝国的大军之间的一次决定性战役。亚历山大大帝率领希腊军队与波斯帝国的大军在伊苏斯平原上交战，最终亚历山大大帝的军队获得了胜利。这场战役的胜利使得亚历山大大帝成功地征服了波斯帝国的首都巴比伦，并将波斯帝国的领土扩大到了印度河流域。这场战役的胜利也标志着亚历山大大帝成为了当时世界上最强大的军事统帅之一。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请将以下这篇文章中的语法错误、拼写错误进行修正： …… （提供一篇包含多处错误的文章）\"}, {\"Assistant\": \"原文：\\n\\nThe boy walk to the store to buy some candy. He was very exited to get his favorite kind of candy. When he got to the store, he saw that they were out of the candy he wanted. He was very disapointed and started to cry. The store clerk saw him crying and asked him what was wrong. The boy told him that they were out of his favorite candy. The store clerk told him not to worry and showed him a new kind of candy that he might like. The boy tried it and loved it. He was very happy and thanked the store clerk for helping him find a new favorite candy.\\n\\n修改后：\\n\\nThe boy walked to the store to buy some candy. He was very excited to get his favorite kind of candy. When he got to the store, he saw that they were out of the candy he wanted. He was very disappointed and started to cry. The store clerk saw him crying and asked him what was wrong. The boy told him that they were out of his favorite candy. The store clerk told him not to worry and showed him a new kind of candy that he might like. The boy tried it and loved it. He was very happy and thanked the store clerk for helping him find a new favorite candy.\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"请列举三种常见的自然语言处理技术并对其进行简要描述。\"}, {\"Assistant\": \"1. 分词技术：将一段文本按照一定的规则分割成一个个词语，是自然语言处理中最基础的技术之一。分词技术可以帮助计算机理解文本的含义，从而进行后续的处理。\\n\\n2. 命名实体识别技术：指识别文本中的人名、地名、组织机构名等实体，并将其分类。命名实体识别技术可以帮助计算机更好地理解文本，从而进行更加精准的分析和处理。\\n\\n3. 情感分析技术：指对文本中的情感进行分析和判断，包括正面情感、负面情感和中性情感等。情感分析技术可以帮助计算机更好地理解人类的情感和态度，从而进行更加精准的推荐和服务。\"}], \"category\": null}\n",
      "\n",
      "{\"conversation\": [{\"User\": \"谈论一下计算机科学的历史和发展？\"}, {\"Assistant\": \"计算机科学的历史可以追溯到古代的计算工具，如算盘和天文仪。但是，现代计算机科学的发展始于20世纪初的机械计算机和电子计算机。\\n\\n在20世纪40年代，第一台电子计算机ENIAC被开发出来，它是一个巨大的机器，占据了一个大房间的空间。在接下来的几十年里，计算机的尺寸和成本不断减小，性能和功能不断提高。这些进步使得计算机成为了现代社会中不可或缺的一部分。\\n\\n在计算机科学的发展过程中，出现了许多重要的技术和概念，如操作系统、编程语言、数据库、人工智能、计算机网络等。这些技术和概念的发展，使得计算机科学在各个领域都有广泛的应用，如商业、医疗、教育、娱乐等。\\n\\n随着计算机科学的不断发展，出现了许多新的领域和研究方向，如计算机图形学、计算机安全、计算机视觉、机器学习等。这些领域和研究方向的发展，为计算机科学的未来发展提供了更广阔的空间和更多的机会。\\n\\n总之，计算机科学的历史和发展是一个不断创新和进步的过程，它已经成为了现代社会中不可或缺的一部分，同时也为未来的科技发展提供了更多的可能性。\"}], \"category\": null}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/GPT3.5-3270k/3270k-qa-Deduplicate-289k.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number>=100:break\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae9ba0-56d5-405a-8357-75b4a14216c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data_path='/root/autodl-tmp/weights/smile/data'\n",
    "aim_path='/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c7a3e-51a0-46b5-926a-69ec37619156",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'r') as f:\n",
    "    for number, line in enumerate(f):\n",
    "        total=''\n",
    "        line=json.loads(line)\n",
    "        for ip in line['conversation']:\n",
    "            total+=[content for _, content in ip.items()][0]\n",
    "        print(len(tokenizer.encode(total, add_special_tokens=False)))\n",
    "        if number == 100:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a2d88-2f94-4c84-bc1b-c9f6bdabf941",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "预期结果\n",
    "\n",
    "{\n",
    "    'conversation':[\n",
    "        {'user': 'xxxx},\n",
    "        {'assistant': 'xxxx},\n",
    "    ]\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f0aec-e627-4f35-aa99-89b482685eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#合并chinese_alpaca数据集\n",
    "df_list=[]\n",
    "data_path = '/root/autodl-tmp/dataset/alpaca_chinese_dataset/翻译后的中文数据'\n",
    "for file_name in os.listdir(data_path):\n",
    "    file_path=os.path.join(data_path, file_name)\n",
    "    if '未完成' not in file_path:\n",
    "        print(file_path)\n",
    "        df_list.append(pd.read_json(file_path))\n",
    "full_df=pd.concat(df_list)\n",
    "count=0\n",
    "with open('/root/autodl-tmp/dataset/alpaca_chinese_dataset/chinese_data.jsonl','w') as file:\n",
    "    for number, item in tqdm(full_df.iterrows()):\n",
    "        try:\n",
    "            sample={'conversation':[{'User':item['instruction']+item['input']}, {'Assistant':item['output']}]}\n",
    "            file.write(json.dumps(sample, ensure_ascii=False)+'\\n')\n",
    "        except:\n",
    "            count+=1\n",
    "count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fb0c3-895a-4a20-b9d5-ef6c601e68f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/alpaca_chinese_dataset/chinese_data.jsonl') as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        break\n",
    "with open(\"/root/autodl-tmp/dataset/smile/smile_conversation_750.jsonl\") as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e097212-b568-44e4-8d8e-1dbe711b8a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"/root/autodl-tmp/dataset/smile/smile_conversation.jsonl\"\n",
    "output_file = \"/root/autodl-tmp/dataset/smile/smile_conversation_750.jsonl\"\n",
    "\n",
    "with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    for i, line in enumerate(f_in):\n",
    "        if i < 750:  # 只处理前3000行\n",
    "            f_out.write(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c7c2f-4674-4fcb-91c2-34d2f73ddc38",
   "metadata": {
    "tags": []
   },
   "source": [
    "#处理Smile_conversation的多轮对话\n",
    "\n",
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'w') as f:\n",
    "    for file_name in tqdm(os.listdir('/root/autodl-tmp/weights/smile/data')):\n",
    "        full_path = os.path.join('/root/autodl-tmp/weights/smile/data', file_name)\n",
    "        df = pd.read_json(full_path)\n",
    "        sentences = []\n",
    "        for idx, item in df.iterrows():\n",
    "            sentence = {item.role: item.content}\n",
    "            sentences.append(sentence)\n",
    "        if len(sentences)%2!=0:\n",
    "            sentences=sentences[:-1]\n",
    "        conversation = {'conversation': sentences}\n",
    "        f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "even=0m\n",
    "odd=0\n",
    "with open(aim_path, 'r') as file:\n",
    "    for line in file:\n",
    "        conversation=json.loads(line)['conversation'] #这是一个list，[{role:content}, {role:content}, {role:content}]\n",
    "        if len(conversation)%2==0:\n",
    "            even+=1\n",
    "        else:\n",
    "            odd+=1\n",
    "            print(conversation)\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b5c49-b0d4-4961-8673-b479c3583e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query_temp.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "# 删除原始文件\n",
    "os.remove('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl')\n",
    "\n",
    "# 将临时文件重命名为原始文件的名称\n",
    "os.rename('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query_temp.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d98e27-f584-494d-816c-8b146ee62818",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query_temp.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "# 删除原始文件\n",
    "os.remove('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl')\n",
    "\n",
    "# 将临时文件重命名为原始文件的名称\n",
    "os.rename('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query_temp.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630ca9e-dbd1-4e99-a0a4-2948435c4a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl' 生成一半\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:int(len(lines)/2)]\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/half_query.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps((temp), ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712d137-3a74-4605-a4d2-f30d2321357f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl' 生成2500条\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:2500]\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/2500_query.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps((temp), ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0612c64-ce28-493d-b0cd-dd545085fd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:3000]\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_3k.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)\n",
    "        file.write(json.dumps({'conversation':[{'User':temp['question']}, {'Assistant':temp['answer']}]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfdac2-8d39-41f1-a09a-08b2f5705cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl' 500\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:500]\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_500.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)\n",
    "        file.write(json.dumps({'conversation':[{'User':temp['question']}, {'Assistant':temp['answer']}]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b84e70-f64a-42b3-a59e-c286052ef8df",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "json.loads(lines[19])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e38316-438f-4fc1-ad75-50c6ccc9d2cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen_conversation.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a0d6c-19bb-4c90-b78f-06ffdcf2ff46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "json.loads(lines[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cd027-4da5-4c73-b6de-ef6b01e89848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59197262-0b61-4a02-a26a-73b2fc041879",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#327w qa数据集\n",
    "df=pd.read_json('/root/autodl-tmp/dataset/qa.json')\n",
    "count=0\n",
    "with open('/root/autodl-tmp/dataset/GPT3.5-3270k/3270k-qa.jsonl','w') as file:\n",
    "    for number, item in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            sample={'conversation':[{'User':item['q']}, {'Assistant':item['a']}], 'category':item['category']}\n",
    "            file.write(json.dumps(sample, ensure_ascii=False)+'\\n')\n",
    "        except:\n",
    "            count+=1\n",
    "print(count)\n",
    "\n",
    "with open('/root/autodl-tmp/dataset/GPT3.5-3270k/3270k-qa.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number==100:break\n",
    "        print(json.loads(line))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c98ed-0631-4103-91d5-0b3fee625f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc61a71-8dd4-476b-8670-879d445321a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/qa.json') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        print(line)\n",
    "        if number==10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b258e-48c8-4636-aee8-f293fa19afa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#测试函数\n",
    "\n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的 answer\n",
    "    inside_ast = False  # 标记是否在<|assistant|>和 <|user|>之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        combine_lines = []\n",
    "        for data_path in data_list:\n",
    "            with open(data_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                combine_lines += lines\n",
    "        if max_sample:\n",
    "            combine_lines = random.shuffle(combine_lines)\n",
    "            combine_lines = combine_lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, combine_lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = [si[\"[gMASK]\"], si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role in self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role in self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                #input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                #input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "        try:\n",
    "            labels[labels.index(si[\"<|user|>\"])]=-100\n",
    "        except:\n",
    "            print('fault')\n",
    "            pass\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            eop_position = len(input_ids)\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8aa2f-fdbf-47e3-8f3e-4dab16550f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_list=['/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/half_query.jsonl', '/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_3k.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55117f-43c6-4302-988d-3b17bfb70791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "temp=conversation_dataset(data_list=data_list, \n",
    "                         tokenizer=tokenizer, \n",
    "                         truncate_length=1024, \n",
    "                         query_key=['User', 'client'], \n",
    "                         answer_key=['Assisstant', 'counselor', 'Assistant'], \n",
    "                         max_sample=None, \n",
    "                         num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14c936-77ef-4941-b1e4-217da8bbe67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dccc8-49a2-437a-a44b-b5a990f32e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=None\n",
    "for times, item in enumerate(temp):\n",
    "    if times==32:\n",
    "        sample=item\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d981c3-4809-4ee9-9f88-ea02ac21bc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in sample.values():\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce256b-fbb5-4bb1-a9c8-0b6950603c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids=sample['input_ids']\n",
    "labels=sample['labels']\n",
    "attention_mask=sample['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fd2b9-490b-47ee-ba61-8027cf8eb350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a2f52-63f1-4430-82f3-89b94ee24ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cccc6d-8915-4962-8005-41b09715caf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    '''\n",
    "    这是一个构建dataset的类\n",
    "    内容是，构建出一个包含input_ids，labels，attention_mask的dataset\n",
    "    '''\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length, query_key, answer_key):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample[query_key]\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample[answer_key]\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                end_answer_index = input_ids.index(si['eop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance\n",
    "    \n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, max_query_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.examples = []\n",
    "\n",
    "        # 使用多进程读取和处理数据\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        sample=json.loads(line)\n",
    "        max_answer_length = self.truncate_length - self.max_query_length - 3\n",
    "\n",
    "        # 判断query的长度\n",
    "        query = sample[self.query_key]\n",
    "        query_ids = self.tokenizer.encode(query, add_special_tokens=False)\n",
    "        if len(query_ids) > self.max_query_length:\n",
    "            query_ids = query_ids[:self.max_query_length]\n",
    "\n",
    "        # 判断answer的长度\n",
    "        answer = sample[self.answer_key]\n",
    "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        if len(answer) > max_answer_length:\n",
    "            answer_ids = answer_ids[:max_answer_length]\n",
    "\n",
    "        # 合并\n",
    "        input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "        pre_context_length = input_ids.index(si['sop'])\n",
    "        end_answer_index = input_ids.index(si['eop'])\n",
    "\n",
    "        # padding\n",
    "        padding_length=self.truncate_length-len(input_ids)\n",
    "        input_ids+=padding_length*[self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "        labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "        labels = labels + [-100] * (self.truncate_length-len(labels))\n",
    "\n",
    "        # 制作attention_mask\n",
    "        eop_position = input_ids.index(si['eop'])+1\n",
    "        attention_mask = [True]*eop_position\n",
    "        attention_mask += [False]*(self.truncate_length-len(attention_mask))\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "\n",
    "    \n",
    "    \n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"sop\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"eop\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "   \n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = []\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                #input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                #input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            end_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            end_position = self.truncate_length\n",
    "        attention_mask = [True] * end_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "    \n",
    "class conversation_dataset_trad(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "        self.examples = list(chain.from_iterable(self.examples))\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation = json.loads(line)['conversation']\n",
    "        result = []\n",
    "        for last_index in range(1, 100, 2):\n",
    "            try:\n",
    "                input_ids = []\n",
    "                for sample in conversation[:-last_index]:\n",
    "                    role = next(iter(sample))\n",
    "                    if role == self.query_key:\n",
    "                        input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                        input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                    elif role == self.answer_key:\n",
    "                        input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                        input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "\n",
    "                # 添加最后一个预测位\n",
    "                sample = conversation[-last_index]\n",
    "                role = next(iter(sample))\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n'], si[\"[gMASK]\"], si['sop']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                input_ids += [si[\"eop\"]]\n",
    "\n",
    "                # 判断截断，添加终止生成符号，padding\n",
    "                if len(input_ids) > self.truncate_length - 1:\n",
    "                    input_ids = input_ids[:self.truncate_length - 1] + [si[\"eop\"]]\n",
    "                input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length - len(input_ids))\n",
    "\n",
    "                # 制作labels\n",
    "                labels = build_labels(input_ids)\n",
    "\n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "                attention_mask = [True] * eop_position + [False] * (self.truncate_length - eop_position)\n",
    "\n",
    "                result.append({\n",
    "                    'input_ids': input_ids,\n",
    "                    'labels': labels,\n",
    "                    'attention_mask': attention_mask,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {e}\")\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "    \n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = [si[\"[gMASK]\"], si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                #input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                #input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "        try:\n",
    "            labels[labels.index(si[\"<|user|>\"])]=-100\n",
    "        except:\n",
    "            print('fault')\n",
    "            pass\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            eop_position = len(input_ids)\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
