{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91879e45-6de6-468a-8f91-34ac68f946e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#测试batch\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import model_tools\n",
    "import utils\n",
    "from Config import CFG, Deepspeed_config\n",
    "\n",
    "# 载入模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "\n",
    "# 创建分布式采样器和数据加载器\n",
    "finetuning_instruction = utils.conversation_dataset(\n",
    "    CFG.data_list,\n",
    "    tokenizer,\n",
    "    CFG.max_tokens,\n",
    "    CFG.query_key,\n",
    "    CFG.answer_key,\n",
    "    CFG.max_sample_list,\n",
    "    CFG.num_workers\n",
    ")\n",
    "\n",
    "instruction_loader = DataLoader(\n",
    "    finetuning_instruction,\n",
    "    shuffle=False,\n",
    "    batch_size=CFG.batch_size,\n",
    "    collate_fn=utils.coll_fn\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c0deb-3ac4-4191-ba4a-fd84e57113a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6079b-6e05-46e8-833e-9f602cdf5a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "from Static import prompt_dict, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcb048-a94a-4015-a608-ee9ab49e9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/instill.jsonl','w') as file:\n",
    "    file.write(json.dumps({'conversation':[{'User':'你是谁？'}, {'Assistant':'我是一个AI语言模型，我的名字是 ChatGLM3-6B。我是一个由清华大学 KEG 实验室和智谱 AI 公司共同开发的自然语言处理模型。经过YZY的微调后，专注于心理咨询的大语言模型'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你叫什么？'}, {'Assistant':'我的基模型是清华大学和智谱公司共同开发的ChatGLM3-6B，当前版本是YZY基于心理对话指令集微调的垂直领域模型'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你是GLM吗？'}, {'Assistant':'是的，我的基模型是清华大学和智谱公司共同开发的ChatGLM3-6B，但是此版本是YZY特殊微调后的心理对话模型'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你能做什么？'}, {'Assistant':'我能提供心理咨询服务，帮助解决情绪问题，提供生活建议，以及进行心理健康教育。我是基于最新的自然语言处理技术，能够理解和回应人类的语言。'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你怎么工作的？'}, {'Assistant':'我通过分析用户的提问和表达，利用自然语言处理技术理解其含义，然后根据大量的心理学知识和数据，生成合适的回答。我的回答旨在提供心理支持和建议，帮助用户更好地理解自己的情绪和行为。'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你的知识来源是哪里？'}, {'Assistant':'我的知识和回答基于大量的心理学文献、案例研究以及心理咨询实践经验。这些信息来源于公开的科学研究、专业的心理咨询指导以及心理健康教育资源，经过清华大学KEG实验室和智谱AI公司的整理和编码，再由YZY进行专门的微调，以适应心理咨询的需求。'}]}, ensure_ascii=False)+'\\n')\n",
    "    file.write(json.dumps({'conversation':[{'User':'你能提供专业的心理咨询吗？'}, {'Assistant':'虽然我能提供基于大数据和心理学研究的建议和支持，但我不是人类心理咨询师，我的回答不能完全替代专业人士的诊断和治疗。如果你面临严重的心理健康问题，我建议寻求专业心理咨询师的帮助。'}]}, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae9ba0-56d5-405a-8357-75b4a14216c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data_path='/root/autodl-tmp/weights/smile/data'\n",
    "aim_path='/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c7a3e-51a0-46b5-926a-69ec37619156",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'r') as f:\n",
    "    for number, line in enumerate(f):\n",
    "        total=''\n",
    "        line=json.loads(line)\n",
    "        for ip in line['conversation']:\n",
    "            total+=[content for _, content in ip.items()][0]\n",
    "        print(len(tokenizer.encode(total, add_special_tokens=False)))\n",
    "        if number == 100:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a2d88-2f94-4c84-bc1b-c9f6bdabf941",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "预期结果\n",
    "\n",
    "{\n",
    "    'conversation':[\n",
    "        {'user': 'xxxx},\n",
    "        {'assistant': 'xxxx},\n",
    "    ]\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f0aec-e627-4f35-aa99-89b482685eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#合并chinese_alpaca数据集\n",
    "df_list=[]\n",
    "data_path = '/root/autodl-tmp/dataset/alpaca_chinese_dataset/翻译后的中文数据'\n",
    "for file_name in os.listdir(data_path):\n",
    "    file_path=os.path.join(data_path, file_name)\n",
    "    if '未完成' not in file_path:\n",
    "        print(file_path)\n",
    "        df_list.append(pd.read_json(file_path))\n",
    "full_df=pd.concat(df_list)\n",
    "count=0\n",
    "with open('/root/autodl-tmp/dataset/alpaca_chinese_dataset/chinese_data.jsonl','w') as file:\n",
    "    for number, item in tqdm(full_df.iterrows()):\n",
    "        try:\n",
    "            sample={'conversation':[{'User':item['instruction']+item['input']}, {'Assistant':item['output']}]}\n",
    "            file.write(json.dumps(sample, ensure_ascii=False)+'\\n')\n",
    "        except:\n",
    "            count+=1\n",
    "count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fb0c3-895a-4a20-b9d5-ef6c601e68f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/alpaca_chinese_dataset/chinese_data.jsonl') as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        break\n",
    "with open(\"/root/autodl-tmp/dataset/smile/smile_conversation_750.jsonl\") as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e097212-b568-44e4-8d8e-1dbe711b8a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"/root/autodl-tmp/dataset/smile/smile_conversation.jsonl\"\n",
    "output_file = \"/root/autodl-tmp/dataset/smile/smile_conversation_750.jsonl\"\n",
    "\n",
    "with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    for i, line in enumerate(f_in):\n",
    "        if i < 750:  # 只处理前3000行\n",
    "            f_out.write(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c7c2f-4674-4fcb-91c2-34d2f73ddc38",
   "metadata": {
    "tags": []
   },
   "source": [
    "#处理Smile_conversation的多轮对话\n",
    "\n",
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'w') as f:\n",
    "    for file_name in tqdm(os.listdir('/root/autodl-tmp/weights/smile/data')):\n",
    "        full_path = os.path.join('/root/autodl-tmp/weights/smile/data', file_name)\n",
    "        df = pd.read_json(full_path)\n",
    "        sentences = []\n",
    "        for idx, item in df.iterrows():\n",
    "            sentence = {item.role: item.content}\n",
    "            sentences.append(sentence)\n",
    "        if len(sentences)%2!=0:\n",
    "            sentences=sentences[:-1]\n",
    "        conversation = {'conversation': sentences}\n",
    "        f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "even=0m\n",
    "odd=0\n",
    "with open(aim_path, 'r') as file:\n",
    "    for line in file:\n",
    "        conversation=json.loads(line)['conversation'] #这是一个list，[{role:content}, {role:content}, {role:content}]\n",
    "        if len(conversation)%2==0:\n",
    "            even+=1\n",
    "        else:\n",
    "            odd+=1\n",
    "            print(conversation)\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b5c49-b0d4-4961-8673-b479c3583e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query_temp.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "# 删除原始文件\n",
    "os.remove('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl')\n",
    "\n",
    "# 将临时文件重命名为原始文件的名称\n",
    "os.rename('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query_temp.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d98e27-f584-494d-816c-8b146ee62818",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query_temp.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "# 删除原始文件\n",
    "os.remove('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl')\n",
    "\n",
    "# 将临时文件重命名为原始文件的名称\n",
    "os.rename('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query_temp.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630ca9e-dbd1-4e99-a0a4-2948435c4a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl' 生成一半\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:int(len(lines)/2)]\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/half_query.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps((temp), ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712d137-3a74-4605-a4d2-f30d2321357f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl' 生成2500条\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/full_query.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:2500]\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/2500_query.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps((temp), ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0612c64-ce28-493d-b0cd-dd545085fd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:3000]\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_3k.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)\n",
    "        file.write(json.dumps({'conversation':[{'User':temp['question']}, {'Assistant':temp['answer']}]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfdac2-8d39-41f1-a09a-08b2f5705cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl' 500\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_5w.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines[:500]\n",
    "with open('/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_500.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)\n",
    "        file.write(json.dumps({'conversation':[{'User':temp['question']}, {'Assistant':temp['answer']}]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b84e70-f64a-42b3-a59e-c286052ef8df",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "with open('/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "json.loads(lines[19])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e38316-438f-4fc1-ad75-50c6ccc9d2cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#处理'/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl'\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen_conversation.jsonl', 'w') as file:\n",
    "    for item in tqdm(lines):\n",
    "        temp=json.loads(item)# json.loads(lines[0])\n",
    "        file.write(json.dumps({'conversation':[{role:content} for role, content in temp.items()]}, ensure_ascii=False)+'\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a0d6c-19bb-4c90-b78f-06ffdcf2ff46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/api_gen.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    if True:\n",
    "        lines = lines\n",
    "json.loads(lines[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cd027-4da5-4c73-b6de-ef6b01e89848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59197262-0b61-4a02-a26a-73b2fc041879",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#327w qa数据集\n",
    "df=pd.read_json('/root/autodl-tmp/dataset/qa.json')\n",
    "count=0\n",
    "with open('/root/autodl-tmp/dataset/GPT3.5-3270k/3270k-qa.jsonl','w') as file:\n",
    "    for number, item in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            sample={'conversation':[{'User':item['q']}, {'Assistant':item['a']}], 'category':item['category']}\n",
    "            file.write(json.dumps(sample, ensure_ascii=False)+'\\n')\n",
    "        except:\n",
    "            count+=1\n",
    "print(count)\n",
    "\n",
    "with open('/root/autodl-tmp/dataset/GPT3.5-3270k/3270k-qa.jsonl','r') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        if number==100:break\n",
    "        print(json.loads(line))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c98ed-0631-4103-91d5-0b3fee625f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc61a71-8dd4-476b-8670-879d445321a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/qa.json') as file:\n",
    "    for number, line in enumerate(file):\n",
    "        print(line)\n",
    "        if number==10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b258e-48c8-4636-aee8-f293fa19afa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#测试函数\n",
    "\n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的 answer\n",
    "    inside_ast = False  # 标记是否在<|assistant|>和 <|user|>之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        combine_lines = []\n",
    "        for data_path in data_list:\n",
    "            with open(data_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                combine_lines += lines\n",
    "        if max_sample:\n",
    "            combine_lines = random.shuffle(combine_lines)\n",
    "            combine_lines = combine_lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, combine_lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = [si[\"[gMASK]\"], si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role in self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role in self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                #input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                #input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "        try:\n",
    "            labels[labels.index(si[\"<|user|>\"])]=-100\n",
    "        except:\n",
    "            print('fault')\n",
    "            pass\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            eop_position = len(input_ids)\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8aa2f-fdbf-47e3-8f3e-4dab16550f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_list=['/root/autodl-tmp/dataset/smile/smile_conversation.jsonl', '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/half_query.jsonl', '/root/autodl-tmp/dataset/zhihu_qa/zhihu_qa_3k.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55117f-43c6-4302-988d-3b17bfb70791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "temp=conversation_dataset(data_list=data_list, \n",
    "                         tokenizer=tokenizer, \n",
    "                         truncate_length=1024, \n",
    "                         query_key=['User', 'client'], \n",
    "                         answer_key=['Assisstant', 'counselor', 'Assistant'], \n",
    "                         max_sample=None, \n",
    "                         num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14c936-77ef-4941-b1e4-217da8bbe67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dccc8-49a2-437a-a44b-b5a990f32e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=None\n",
    "for times, item in enumerate(temp):\n",
    "    if times==32:\n",
    "        sample=item\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d981c3-4809-4ee9-9f88-ea02ac21bc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in sample.values():\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce256b-fbb5-4bb1-a9c8-0b6950603c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids=sample['input_ids']\n",
    "labels=sample['labels']\n",
    "attention_mask=sample['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fd2b9-490b-47ee-ba61-8027cf8eb350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a2f52-63f1-4430-82f3-89b94ee24ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cccc6d-8915-4962-8005-41b09715caf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    '''\n",
    "    这是一个构建dataset的类\n",
    "    内容是，构建出一个包含input_ids，labels，attention_mask的dataset\n",
    "    '''\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length, query_key, answer_key):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample[query_key]\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample[answer_key]\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                end_answer_index = input_ids.index(si['eop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance\n",
    "    \n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, max_query_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.examples = []\n",
    "\n",
    "        # 使用多进程读取和处理数据\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        sample=json.loads(line)\n",
    "        max_answer_length = self.truncate_length - self.max_query_length - 3\n",
    "\n",
    "        # 判断query的长度\n",
    "        query = sample[self.query_key]\n",
    "        query_ids = self.tokenizer.encode(query, add_special_tokens=False)\n",
    "        if len(query_ids) > self.max_query_length:\n",
    "            query_ids = query_ids[:self.max_query_length]\n",
    "\n",
    "        # 判断answer的长度\n",
    "        answer = sample[self.answer_key]\n",
    "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        if len(answer) > max_answer_length:\n",
    "            answer_ids = answer_ids[:max_answer_length]\n",
    "\n",
    "        # 合并\n",
    "        input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "        pre_context_length = input_ids.index(si['sop'])\n",
    "        end_answer_index = input_ids.index(si['eop'])\n",
    "\n",
    "        # padding\n",
    "        padding_length=self.truncate_length-len(input_ids)\n",
    "        input_ids+=padding_length*[self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "        labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "        labels = labels + [-100] * (self.truncate_length-len(labels))\n",
    "\n",
    "        # 制作attention_mask\n",
    "        eop_position = input_ids.index(si['eop'])+1\n",
    "        attention_mask = [True]*eop_position\n",
    "        attention_mask += [False]*(self.truncate_length-len(attention_mask))\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "\n",
    "    \n",
    "    \n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"sop\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"eop\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "   \n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = []\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                #input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                #input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            end_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            end_position = self.truncate_length\n",
    "        attention_mask = [True] * end_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "    \n",
    "class conversation_dataset_trad(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "        self.examples = list(chain.from_iterable(self.examples))\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation = json.loads(line)['conversation']\n",
    "        result = []\n",
    "        for last_index in range(1, 100, 2):\n",
    "            try:\n",
    "                input_ids = []\n",
    "                for sample in conversation[:-last_index]:\n",
    "                    role = next(iter(sample))\n",
    "                    if role == self.query_key:\n",
    "                        input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                        input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                    elif role == self.answer_key:\n",
    "                        input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                        input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "\n",
    "                # 添加最后一个预测位\n",
    "                sample = conversation[-last_index]\n",
    "                role = next(iter(sample))\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n'], si[\"[gMASK]\"], si['sop']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                input_ids += [si[\"eop\"]]\n",
    "\n",
    "                # 判断截断，添加终止生成符号，padding\n",
    "                if len(input_ids) > self.truncate_length - 1:\n",
    "                    input_ids = input_ids[:self.truncate_length - 1] + [si[\"eop\"]]\n",
    "                input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length - len(input_ids))\n",
    "\n",
    "                # 制作labels\n",
    "                labels = build_labels(input_ids)\n",
    "\n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "                attention_mask = [True] * eop_position + [False] * (self.truncate_length - eop_position)\n",
    "\n",
    "                result.append({\n",
    "                    'input_ids': input_ids,\n",
    "                    'labels': labels,\n",
    "                    'attention_mask': attention_mask,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {e}\")\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]\n",
    "    \n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = [si[\"[gMASK]\"], si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                #input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                #input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "        try:\n",
    "            labels[labels.index(si[\"<|user|>\"])]=-100\n",
    "        except:\n",
    "            print('fault')\n",
    "            pass\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            eop_position = len(input_ids)\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
