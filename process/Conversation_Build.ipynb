{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bf6df3-05b9-472c-8064-5c8a13bb3a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0102d340-37ab-4921-9dd4-248cc1c83693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea65da5-e8da-4f83-aad8-09ef8203d490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "from Static import prompt_dict, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12723078-200d-483f-8085-97bd7083a80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data_path='/root/autodl-tmp/weights/smile/data'\n",
    "aim_path='/root/autodl-tmp/dataset/smile/smile_conversation.jsonl'\n",
    "model_path = '/root/autodl-tmp/weights/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ab73f3-1b3c-49c4-8fd6-34e82274288f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c7704-4db7-4243-ae18-2800ea7cdba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "with open(aim_path, 'w') as f:\n",
    "    for file_name in tqdm(os.listdir(raw_data_path)):\n",
    "        full_path = os.path.join(raw_data_path, file_name)\n",
    "        df = pd.read_json(full_path)\n",
    "        sentences = []\n",
    "        for idx, item in df.iterrows():\n",
    "            sentence = {item.role: item.content}\n",
    "            sentences.append(sentence)\n",
    "        if len(sentences)%2!=0:\n",
    "            sentences=sentences[:-1]\n",
    "        conversation = {'conversation': sentences}\n",
    "        f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "even=0\n",
    "odd=0\n",
    "with open(aim_path, 'r') as file:\n",
    "    for line in file:\n",
    "        conversation=json.loads(line)['conversation'] #这是一个list，[{role:content}, {role:content}, {role:content}]\n",
    "        if len(conversation)%2==0:\n",
    "            even+=1\n",
    "        else:\n",
    "            odd+=1\n",
    "            print(conversation)\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5672768-41b5-4dbd-8ca4-a79ff520e741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bulid_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"sop\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"eop\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8222c4bc-29f0-42da-8bd6-d88963977461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def build_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        input_ids = [si[\"[gMASK]\"], si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"], si['\\n']]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"], si['\\n']]\n",
    "                #input_ids += [si[\"[gMASK]\"], si['sop']] #添加开始生成的token\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "                #input_ids += [si[\"eop\"]]\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "            input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = build_labels(input_ids)\n",
    "        try:\n",
    "            labels[labels.index(si[\"<|user|>\"])]=-100\n",
    "        except:\n",
    "            print('fault')\n",
    "            pass\n",
    "\n",
    "        # 制作attention_mask\n",
    "        try:\n",
    "            eop_position = input_ids.index(self.tokenizer.pad_token_id)\n",
    "        except:\n",
    "            eop_position = len(input_ids)\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ad717a-c3ce-4423-8c70-09d804709da3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 463 ms, sys: 274 ms, total: 737 ms\n",
      "Wall time: 689 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp=conversation_dataset(data_path=aim_path, tokenizer=tokenizer, truncate_length=1024, query_key='client', answer_key='counselor', max_sample=5, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d97a80d-b48f-4646-99c2-3455efd8c415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample=None\n",
    "for item in temp:\n",
    "    sample=item\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0d848c-334f-443d-a54d-64b2c0585397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "for item in sample.values():\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93bd27ad-a1a6-4ebc-8a4b-89fa09f30e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids=sample['input_ids']\n",
    "labels=sample['labels']\n",
    "attention_mask=sample['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706adfa8-b567-41b3-b7bf-cfad3e2ab00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK]sop<|user|>\\n 高三后的迷茫，高考前的恐惧，能给我一些建议么？<|assistant|>\\n 看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。<|user|>\\n 你说的对，我是非常焦虑，确实需要调整心态。我也明白高考的心态很重要，但是要怎样才能真正拥有必胜的心态呢？<|assistant|>\\n 首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。<|user|>\\n 我确实没有一个合理的复习计划，每天都觉得时间不够用，也不知道该从何开始。你能给我一些建议吗？<|assistant|>\\n 当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。<|user|>\\n 对于难题，我经常会自我怀疑，觉得自己学不好。你觉得我该怎么办呢？<|assistant|>\\n 不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。<|user|>\\n 我真的很感谢你的建议。我会尽力调整心态，制定一个合理的复习计划，并勇敢地向老师请教问题。我相信只要努力，就一定能够达到自己的目标。<|assistant|>\\n 很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adb9265-7c77-4e91-8d1c-1a25dd827e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。<|user|>\\n 首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。<|user|>\\n 当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。<|user|>\\n 不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。<|user|>\\n 很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748718ed-bba8-453c-94ba-620399cdcdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for item in input_ids:\n",
    "    if item!=0:count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40eeb842-66a9-44f6-a1cc-321d18f5d715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for item in attention_mask:\n",
    "    if item==True:count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18caa05d-0c18-4b41-99ac-bb3eca617a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [64790, 64792, 64795, 13, 36431, 54645, 32313, 40739, 31123, 33390, 33711, 36355, 31123, 54558, 33575, 31784, 32108, 54705, 31514, 64796, 13, 30910, 31857, 31822, 37817, 32044, 54622, 54657, 34843, 31123, 31646, 32218, 54534, 32959, 37826, 54578, 33903, 31755, 31155, 33103, 46173, 31123, 54546, 34518, 31822, 34887, 31155, 41037, 54532, 32497, 31631, 54530, 31404, 33390, 35215, 31663, 31848, 54532, 35098, 31155, 54532, 55020, 55442, 38888, 31404, 31642, 34794, 55370, 41652, 55370, 31123, 31700, 53221, 33742, 54535, 56278, 56289, 31123, 54685, 55244, 54530, 54892, 31663, 54546, 31123, 40601, 34794, 31642, 43638, 37736, 31791, 31919, 31123, 54546, 52687, 33154, 31404, 54536, 31919, 37501, 54879, 31123, 31864, 31404, 55478, 54550, 31864, 31123, 55478, 54550, 31643, 32584, 33339, 54530, 37047, 31864, 31155, 64795, 13, 36474, 32980, 54570, 31123, 33030, 31685, 34843, 31123, 32967, 31665, 32271, 35098, 31155, 33876, 32855, 33390, 38888, 37523, 31123, 31694, 54552, 33613, 32017, 32047, 32104, 55020, 55442, 38888, 55282, 31514, 64796, 13, 30910, 32342, 31123, 34526, 32309, 31674, 31919, 31123, 33647, 37902, 54814, 32894, 31123, 54728, 32926, 35016, 51197, 31155, 32043, 31123, 44531, 32624, 31623, 37239, 37047, 31864, 31123, 52120, 31689, 54556, 32158, 32096, 34769, 40036, 31795, 31155, 31676, 41230, 54622, 34450, 40416, 32307, 37047, 31807, 31123, 32382, 34843, 54706, 31155, 64795, 13, 34211, 32967, 47409, 37239, 37047, 31864, 31123, 42701, 31897, 31643, 33793, 54571, 31123, 39463, 54960, 54708, 54873, 31699, 31155, 37474, 33575, 31784, 32108, 55398, 31514, 64796, 13, 30910, 32276, 31628, 31404, 34738, 54708, 54589, 35872, 31123, 39887, 54736, 54726, 39546, 31155, 32096, 54573, 42096, 37234, 31123, 31676, 31759, 31803, 31822, 31658, 33045, 31155, 31701, 31123, 31738, 32886, 54551, 54811, 36322, 31123, 54573, 55208, 54573, 55379, 35073, 32333, 31123, 54744, 54588, 54615, 31687, 31123, 31628, 42096, 42733, 37234, 31123, 33041, 32169, 33467, 32289, 54814, 33177, 37234, 31123, 32843, 34216, 54725, 54736, 31155, 64795, 13, 50700, 35029, 31123, 54546, 43903, 32375, 35483, 31123, 37845, 54545, 32677, 31155, 43467, 54546, 49086, 55282, 31514, 64796, 13, 30910, 31844, 32375, 35483, 31123, 54551, 35565, 31917, 31822, 32128, 35022, 31155, 31654, 32411, 35029, 31123, 34738, 37318, 54701, 54761, 31764, 31123, 31633, 31632, 54541, 31788, 55437, 31822, 31639, 54617, 35134, 31155, 31844, 33519, 34550, 35221, 31123, 47076, 32192, 31674, 36319, 31123, 41406, 41167, 31658, 31123, 31781, 32702, 54626, 33095, 54530, 31155, 64795, 13, 34211, 37824, 32823, 31822, 32108, 31155, 35094, 40328, 32271, 35098, 31123, 32624, 31623, 37239, 37047, 31864, 31123, 54724, 35087, 44035, 31764, 47683, 31639, 31155, 38505, 32100, 31862, 31123, 54567, 31781, 31759, 32084, 31674, 31919, 31155, 64796, 13, 30910, 48895, 31759, 55268, 50589, 31404, 36597, 50139, 33213, 31123, 31875, 31862, 31123, 38505, 54622, 31781, 31759, 32058, 53626, 31155, 34649, 31404, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 30910, 31857, 31822, 37817, 32044, 54622, 54657, 34843, 31123, 31646, 32218, 54534, 32959, 37826, 54578, 33903, 31755, 31155, 33103, 46173, 31123, 54546, 34518, 31822, 34887, 31155, 41037, 54532, 32497, 31631, 54530, 31404, 33390, 35215, 31663, 31848, 54532, 35098, 31155, 54532, 55020, 55442, 38888, 31404, 31642, 34794, 55370, 41652, 55370, 31123, 31700, 53221, 33742, 54535, 56278, 56289, 31123, 54685, 55244, 54530, 54892, 31663, 54546, 31123, 40601, 34794, 31642, 43638, 37736, 31791, 31919, 31123, 54546, 52687, 33154, 31404, 54536, 31919, 37501, 54879, 31123, 31864, 31404, 55478, 54550, 31864, 31123, 55478, 54550, 31643, 32584, 33339, 54530, 37047, 31864, 31155, 64795, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 30910, 32342, 31123, 34526, 32309, 31674, 31919, 31123, 33647, 37902, 54814, 32894, 31123, 54728, 32926, 35016, 51197, 31155, 32043, 31123, 44531, 32624, 31623, 37239, 37047, 31864, 31123, 52120, 31689, 54556, 32158, 32096, 34769, 40036, 31795, 31155, 31676, 41230, 54622, 34450, 40416, 32307, 37047, 31807, 31123, 32382, 34843, 54706, 31155, 64795, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 30910, 32276, 31628, 31404, 34738, 54708, 54589, 35872, 31123, 39887, 54736, 54726, 39546, 31155, 32096, 54573, 42096, 37234, 31123, 31676, 31759, 31803, 31822, 31658, 33045, 31155, 31701, 31123, 31738, 32886, 54551, 54811, 36322, 31123, 54573, 55208, 54573, 55379, 35073, 32333, 31123, 54744, 54588, 54615, 31687, 31123, 31628, 42096, 42733, 37234, 31123, 33041, 32169, 33467, 32289, 54814, 33177, 37234, 31123, 32843, 34216, 54725, 54736, 31155, 64795, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 30910, 31844, 32375, 35483, 31123, 54551, 35565, 31917, 31822, 32128, 35022, 31155, 31654, 32411, 35029, 31123, 34738, 37318, 54701, 54761, 31764, 31123, 31633, 31632, 54541, 31788, 55437, 31822, 31639, 54617, 35134, 31155, 31844, 33519, 34550, 35221, 31123, 47076, 32192, 31674, 36319, 31123, 41406, 41167, 31658, 31123, 31781, 32702, 54626, 33095, 54530, 31155, 64795, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 30910, 48895, 31759, 55268, 50589, 31404, 36597, 50139, 33213, 31123, 31875, 31862, 31123, 38505, 54622, 31781, 31759, 32058, 53626, 31155, 34649, 31404, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]}\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e53543d-bf33-4cc2-a6f5-db2522e5559f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.index(46173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a82270a4-36e5-43e3-845d-7e7e6f8ace3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.index(46173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d36455b-beaf-4b08-92bd-30b51d27970c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 117)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.index(64793), labels.index(64793)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "500003ac-c476-4b4e-b824-fb0ad574f403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。eop'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[30910, 31857, 31822, 37817, 32044, 54622, 54657, 34843, 31123, 31646, 32218, 54534, 32959, 37826, 54578, 33903, 31755, 31155, 33103, 46173, 31123, 54546, 34518, 31822, 34887, 31155, 41037, 54532, 32497, 31631, 54530, 31404, 33390, 35215, 31663, 31848, 54532, 35098, 31155, 54532, 55020, 55442, 38888, 31404, 31642, 34794, 55370, 41652, 55370, 31123, 31700, 53221, 33742, 54535, 56278, 56289, 31123, 54685, 55244, 54530, 54892, 31663, 54546, 31123, 40601, 34794, 31642, 43638, 37736, 31791, 31919, 31123, 54546, 52687, 33154, 31404, 54536, 31919, 37501, 54879, 31123, 31864, 31404, 55478, 54550, 31864, 31123, 55478, 54550, 31643, 32584, 33339, 54530, 37047, 31864, 31155, 64793]\n",
    "tokenizer.decode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6acea-bcdb-400e-8e49-18642094ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb3098fb-0308-4926-a1fe-777586d0fdcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\n 高三后的迷茫，高考前的恐惧，能给我一些建议么？<|assistant|>\\n[gMASK]sop 看到你的提问感觉你很焦虑，这个状态在高中高压下很容易出现。我想说的是，我看到了你的决心。这点是很多人没有的！高考最重要的不是知识是心态。是必胜的心态！什么放松吧缓缓吧，都是站着说话不腰疼，保送的又不是我，我能放松什么？！我有我的目标，我怎么可能放弃！有目标就好办，计划！缺个计划，缺个时间合理配置的复习计划。eop<|user|>\\n 你说的对，我是非常焦虑，确实需要调整心态。我也明白高考的心态很重要，但是要怎样才能真正拥有必胜的心态呢？<|assistant|>\\n[gMASK]sop 首先，你要明确自己的目标，既然你想考本科，那就要为此做好准备。然后，你需要制定一个合理的复习计划，根据自己的情况来安排每天的学习时间和内容。这样可以帮助你更好地掌控整个复习过程，减少焦虑感。eop<|user|>\\n 我确实没有一个合理的复习计划，每天都觉得时间不够用，也不知道该从何开始。你能给我一些建议吗？<|assistant|>\\n[gMASK]sop 当然可以！你可以从高一开始，试试题海战术。每天多做一些题目，这样能够提高你的学习效率。同时，对于英语这门科目，多听多背是很重要的，数理化方面，可以做一些经典的题目，特别是那些类型经常考到的题目，多次反复做题。eop<|user|>\\n 对于难题，我经常会自我怀疑，觉得自己学不好。你觉得我该怎么办呢？<|assistant|>\\n[gMASK]sop 不要自我怀疑，这只会增加你的心理负担。如果遇到难题，你可以大胆去问老师，他们就是为了解答你的问题而存在的。不要担心别人的期望，你应该相信自己的潜力，只要你拼命学习，一定会有所收获的。eop<|user|>\\n 我真的很感谢你的建议。我会尽力调整心态，制定一个合理的复习计划，并勇敢地向老师请教问题。我相信只要努力，就一定能够达到自己的目标。<|assistant|>\\n[gMASK]sop 很高兴能够帮到你！记住要保持信心，坚持努力，我相信你一定能够取得优异的成绩。加油！eopeop'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([64795, 13, 36431, 54645, 32313, 40739, 31123, 33390, 33711, 36355, 31123, 54558, 33575, 31784, 32108, 54705, 31514, 64796, 13, 64790, 64792, 30910, 31857, 31822, 37817, 32044, 54622, 54657, 34843, 31123, 31646, 32218, 54534, 32959, 37826, 54578, 33903, 31755, 31155, 33103, 46173, 31123, 54546, 34518, 31822, 34887, 31155, 41037, 54532, 32497, 31631, 54530, 31404, 33390, 35215, 31663, 31848, 54532, 35098, 31155, 54532, 55020, 55442, 38888, 31404, 31642, 34794, 55370, 41652, 55370, 31123, 31700, 53221, 33742, 54535, 56278, 56289, 31123, 54685, 55244, 54530, 54892, 31663, 54546, 31123, 40601, 34794, 31642, 43638, 37736, 31791, 31919, 31123, 54546, 52687, 33154, 31404, 54536, 31919, 37501, 54879, 31123, 31864, 31404, 55478, 54550, 31864, 31123, 55478, 54550, 31643, 32584, 33339, 54530, 37047, 31864, 31155, 64793, 64795, 13, 36474, 32980, 54570, 31123, 33030, 31685, 34843, 31123, 32967, 31665, 32271, 35098, 31155, 33876, 32855, 33390, 38888, 37523, 31123, 31694, 54552, 33613, 32017, 32047, 32104, 55020, 55442, 38888, 55282, 31514, 64796, 13, 64790, 64792, 30910, 32342, 31123, 34526, 32309, 31674, 31919, 31123, 33647, 37902, 54814, 32894, 31123, 54728, 32926, 35016, 51197, 31155, 32043, 31123, 44531, 32624, 31623, 37239, 37047, 31864, 31123, 52120, 31689, 54556, 32158, 32096, 34769, 40036, 31795, 31155, 31676, 41230, 54622, 34450, 40416, 32307, 37047, 31807, 31123, 32382, 34843, 54706, 31155, 64793, 64795, 13, 34211, 32967, 47409, 37239, 37047, 31864, 31123, 42701, 31897, 31643, 33793, 54571, 31123, 39463, 54960, 54708, 54873, 31699, 31155, 37474, 33575, 31784, 32108, 55398, 31514, 64796, 13, 64790, 64792, 30910, 32276, 31628, 31404, 34738, 54708, 54589, 35872, 31123, 39887, 54736, 54726, 39546, 31155, 32096, 54573, 42096, 37234, 31123, 31676, 31759, 31803, 31822, 31658, 33045, 31155, 31701, 31123, 31738, 32886, 54551, 54811, 36322, 31123, 54573, 55208, 54573, 55379, 35073, 32333, 31123, 54744, 54588, 54615, 31687, 31123, 31628, 42096, 42733, 37234, 31123, 33041, 32169, 33467, 32289, 54814, 33177, 37234, 31123, 32843, 34216, 54725, 54736, 31155, 64793, 64795, 13, 50700, 35029, 31123, 54546, 43903, 32375, 35483, 31123, 37845, 54545, 32677, 31155, 43467, 54546, 49086, 55282, 31514, 64796, 13, 64790, 64792, 30910, 31844, 32375, 35483, 31123, 54551, 35565, 31917, 31822, 32128, 35022, 31155, 31654, 32411, 35029, 31123, 34738, 37318, 54701, 54761, 31764, 31123, 31633, 31632, 54541, 31788, 55437, 31822, 31639, 54617, 35134, 31155, 31844, 33519, 34550, 35221, 31123, 47076, 32192, 31674, 36319, 31123, 41406, 41167, 31658, 31123, 31781, 32702, 54626, 33095, 54530, 31155, 64793, 64795, 13, 34211, 37824, 32823, 31822, 32108, 31155, 35094, 40328, 32271, 35098, 31123, 32624, 31623, 37239, 37047, 31864, 31123, 54724, 35087, 44035, 31764, 47683, 31639, 31155, 38505, 32100, 31862, 31123, 54567, 31781, 31759, 32084, 31674, 31919, 31155, 64796, 13, 64790, 64792, 30910, 48895, 31759, 55268, 50589, 31404, 36597, 50139, 33213, 31123, 31875, 31862, 31123, 38505, 54622, 31781, 31759, 32058, 53626, 31155, 34649, 31404, 64793, 64793, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "148bb2d2-7343-4e4d-9c74-c1c825a22036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec82cc38-f037-49b8-adf1-232b03944731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for item in attention:\n",
    "    if item==True:\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c625a47-f38f-4453-a4f6-b817b74a44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[64795, 13, 36431, 54645, 32313, 40739, 31123, 33390, 33711, 36355, 31123, 54558, 33575, 31784, 32108, 54705, 31514, 64796, 13, 64790, 64792, 30910, 31857, 31822, 37817, 32044, 54622, 54657, 34843, 31123, 31646, 32218, 54534, 32959, 37826, 54578, 33903, 31755, 31155, 33103, 46173, 31123, 54546, 34518, 31822, 34887, 31155, 41037, 54532, 32497, 31631, 54530, 31404, 33390, 35215, 31663, 31848, 54532, 35098, 31155, 54532, 55020, 55442, 38888, 31404, 31642, 34794, 55370, 41652, 55370, 31123, 31700, 53221, 33742, 54535, 56278, 56289, 31123, 54685, 55244, 54530, 54892, 31663, 54546, 31123, 40601, 34794, 31642, 43638, 37736, 31791, 31919, 31123, 54546, 52687, 33154, 31404, 54536, 31919, 37501, 54879, 31123, 31864, 31404, 55478, 54550, 31864, 31123, 55478, 54550, 31643, 32584, 33339, 54530, 37047, 31864, 31155, 64793, 64795, 13, 36474, 32980, 54570, 31123, 33030, 31685, 34843, 31123, 32967, 31665, 32271, 35098, 31155, 33876, 32855, 33390, 38888, 37523, 31123, 31694, 54552, 33613, 32017, 32047, 32104, 55020, 55442, 38888, 55282, 31514, 64796, 13, 64790, 64792, 30910, 32342, 31123, 34526, 32309, 31674, 31919, 31123, 33647, 37902, 54814, 32894, 31123, 54728, 32926, 35016, 51197, 31155, 32043, 31123, 44531, 32624, 31623, 37239, 37047, 31864, 31123, 52120, 31689, 54556, 32158, 32096, 34769, 40036, 31795, 31155, 31676, 41230, 54622, 34450, 40416, 32307, 37047, 31807, 31123, 32382, 34843, 54706, 31155, 64793, 64795, 13, 34211, 32967, 47409, 37239, 37047, 31864, 31123, 42701, 31897, 31643, 33793, 54571, 31123, 39463, 54960, 54708, 54873, 31699, 31155, 37474, 33575, 31784, 32108, 55398, 31514, 64796, 13, 64790, 64792, 30910, 32276, 31628, 31404, 34738, 54708, 54589, 35872, 31123, 39887, 54736, 54726, 39546, 31155, 32096, 54573, 42096, 37234, 31123, 31676, 31759, 31803, 31822, 31658, 33045, 31155, 31701, 31123, 31738, 32886, 54551, 54811, 36322, 31123, 54573, 55208, 54573, 55379, 35073, 32333, 31123, 54744, 54588, 54615, 31687, 31123, 31628, 42096, 42733, 37234, 31123, 33041, 32169, 33467, 32289, 54814, 33177, 37234, 31123, 32843, 34216, 54725, 54736, 31155, 64793, 64795, 13, 50700, 35029, 31123, 54546, 43903, 32375, 35483, 31123, 37845, 54545, 32677, 31155, 43467, 54546, 49086, 55282, 31514, 64796, 13, 64790, 64792, 30910, 31844, 32375, 35483, 31123, 54551, 35565, 31917, 31822, 32128, 35022, 31155, 31654, 32411, 35029, 31123, 34738, 37318, 54701, 54761, 31764, 31123, 31633, 31632, 54541, 31788, 55437, 31822, 31639, 54617, 35134, 31155, 31844, 33519, 34550, 35221, 31123, 47076, 32192, 31674, 36319, 31123, 41406, 41167, 31658, 31123, 31781, 32702, 54626, 33095, 54530, 31155, 64793, 64795, 13, 34211, 37824, 32823, 31822, 32108, 31155, 35094, 40328, 32271, 35098, 31123, 32624, 31623, 37239, 37047, 31864, 31123, 54724, 35087, 44035, 31764, 47683, 31639, 31155, 38505, 32100, 31862, 31123, 54567, 31781, 31759, 32084, 31674, 31919, 31155, 64796, 13, 64790, 64792, 30910, 48895, 31759, 55268, 50589, 31404, 36597, 50139, 33213, 31123, 31875, 31862, 31123, 38505, 54622, 31781, 31759, 32058, 53626, 31155, 34649, 31404, 64793, 64793, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9917ff7e-ff08-4e60-ac75-89151a73a4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for item in attention:\n",
    "    if item!=0:\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f94ca6c-18e7-4be1-b49a-4de9579431c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30910, 32914, 54895, 32595, 55315, 0, 1, 37536, 2]\n",
      "[-100, -100, -100, -100, -100, -100, -100, 1, 37536, 2]\n"
     ]
    }
   ],
   "source": [
    "#这不是一个训练代码，而是一个检查Loss为什么不按照预期下降的代码\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "#tokenizer = AutoTokenizer.from_pretrained('/root/autodl-fs/weights/chatglm3-6b', trust_remote_code=True)\n",
    "\n",
    "query_ids=tokenizer.encode('中国的首都在哪', add_special_tokens=False)\n",
    "answer_ids=tokenizer.encode('北京', add_special_tokens=False)\n",
    "input_ids = query_ids + [0] + [1] + answer_ids + [2]\n",
    "print(input_ids)\n",
    "\n",
    "#input_ids=['a','b','c','d','gmask','sop','e','f','g','eop','pad','pad','pad']\n",
    "pre_context_length=input_ids.index(1)\n",
    "end_answer_index = input_ids.index(2)\n",
    "\n",
    "labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+0: end_answer_index+1]\n",
    "labels = labels + [-100] * (len(input_ids)-len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e91fd0-962b-4b85-be79-72672b94b166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国的首都在哪 北京'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([30910, 32914, 54895, 32595, 55315, 0, 1, 37536, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6dfc8-9029-4077-a88d-d2e70ad0e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_labels(input_ids):\n",
    "    # 初始化一个新列表，用于存储结果\n",
    "    result = [-100] * len(input_ids)\n",
    "    # 遍历列表，查找满足条件的[anw]元素\n",
    "    inside_ast = False  # 标记是否在【ast】和【user】/【eop】之间\n",
    "    for i, item in enumerate(input_ids):\n",
    "        if item == si[\"<|assistant|>\"]:\n",
    "            inside_ast = True\n",
    "        elif item == si[\"<|user|>\"]:\n",
    "            inside_ast = False\n",
    "        elif item == si[\"eop\"]:\n",
    "            inside_ast = False\n",
    "            result[i] = item\n",
    "        elif inside_ast:\n",
    "            result[i] = item\n",
    "    return result\n",
    "   \n",
    "\n",
    "class conversation_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, query_key, answer_key, max_sample=None, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.query_key = query_key\n",
    "        self.answer_key = answer_key\n",
    "        self.max_sample = max_sample\n",
    "        self.examples = []  # 存储最终结果的对象\n",
    "\n",
    "        # 读取文件\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if max_sample:\n",
    "                lines = lines[:max_sample]\n",
    "\n",
    "        # 创建一个进程池\n",
    "        with Pool(num_workers) as p:\n",
    "            self.examples = p.map(self.process_line, lines)\n",
    "\n",
    "    def process_line(self, line):\n",
    "        conversation=json.loads(line)['conversation'] \n",
    "        '''\n",
    "        此处假设conversation的query和answer均不会太长，故删除了判断长度的步骤\n",
    "        '''\n",
    "        # 制作input_ids\n",
    "        # 添加开始生成标识符\n",
    "        input_ids = [si[\"[gMASK]\"]] + [si['sop']]\n",
    "\n",
    "        # 遍历双方对话，首端添加特殊token\n",
    "        for sample in conversation:\n",
    "            role=next(iter(sample))\n",
    "            if  role== self.query_key:\n",
    "                input_ids += [si[\"<|user|>\"]]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "            elif role == self.answer_key:\n",
    "                input_ids += [si[\"<|assistant|>\"]]\n",
    "                input_ids += self.tokenizer.encode(sample[role], add_special_tokens=False)\n",
    "\n",
    "        # 判断截断，添加终止生成符号，padding\n",
    "        if len(input_ids) > self.truncate_length-1:\n",
    "            input_ids = input_ids[:self.truncate_length-1]\n",
    "\n",
    "        input_ids += [si[\"eop\"]]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.truncate_length-len(input_ids))\n",
    "\n",
    "        # 制作labels\n",
    "        labels = bulid_labels(input_ids)\n",
    "\n",
    "        # 制作attention_mask\n",
    "        eop_position = input_ids.index(si['eop']) + 1\n",
    "        attention_mask = [True] * eop_position\n",
    "        attention_mask += [False] * (self.truncate_length - len(attention_mask))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.examples[item]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
