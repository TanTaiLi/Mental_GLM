{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087ccd58-2371-40a7-b5e7-2ae5b2e5b2ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "#这不是一个训练代码，而是一个检查Loss为什么不按照预期下降的代码\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('/root/autodl-fs/weights/chatglm3-6b', trust_remote_code=True)\n",
    "\n",
    "query_ids=tokenizer.encode('中国的首都在哪', add_special_tokens=False)\n",
    "answer_ids=tokenizer.encode('北京', add_special_tokens=False)\n",
    "input_ids = query_ids + [0] + [1] + answer_ids + [2]\n",
    "print(input_ids)\n",
    "\n",
    "#input_ids=['a','b','c','d','gmask','sop','e','f','g','eop','pad','pad','pad']\n",
    "pre_context_length=input_ids.index(1)\n",
    "end_answer_index = input_ids.index(2)\n",
    "\n",
    "labels = [-100] * (pre_context_length+0) + input_ids[pre_context_length+0: end_answer_index+1]\n",
    "labels = labels + [-100] * (len(input_ids)-len(labels))\n",
    "print(labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28687d18-9118-4a50-bb59-c24d73089821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!find /root/autodl-tmp/checkpoints -mindepth 1 -maxdepth 1 -type d -not -path \"/root/autodl-tmp/checkpoints/previous\" -exec mv {} /root/autodl-tmp/checkpoints/previous/ \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71ac6da-4357-4554-b3f8-57bffb23340b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank16-ctx512-API800\n",
      "Rank16-ctx512-API800_epoch0_step300\n",
      "Rank16-ctx512-API800_epoch1_step300\n",
      "Rank16-ctx512-API800_epoch2_step300\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step1000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step10000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step11000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step12000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step13000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step14000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step2000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step3000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step4000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step5000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step6000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step7000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step8000\n",
      "Rank16-ctx512-DeDu-280K_epoch0_step9000\n",
      "Rank16-ctx512-newshort4300\n",
      "Rank16-ctx512-newshort4300_epoch0_step1200\n",
      "Rank16-ctx512-newshort4300_epoch0_step1500\n",
      "Rank16-ctx512-newshort4300_epoch0_step1800\n",
      "Rank16-ctx512-newshort4300_epoch0_step2100\n",
      "Rank16-ctx512-newshort4300_epoch0_step300\n",
      "Rank16-ctx512-newshort4300_epoch0_step600\n",
      "Rank16-ctx512-newshort4300_epoch0_step900\n",
      "Rank16-ctx512-newshort4300_epoch1_step1200\n",
      "Rank16-ctx512-newshort4300_epoch1_step1500\n",
      "Rank16-ctx512-newshort4300_epoch1_step1800\n",
      "Rank16-ctx512-newshort4300_epoch1_step2100\n",
      "Rank16-ctx512-newshort4300_epoch1_step300\n",
      "Rank16-ctx512-newshort4300_epoch1_step600\n",
      "Rank16-ctx512-newshort4300_epoch1_step900\n",
      "Rank16-ctx512-newshort6800\n",
      "Rank16-ctx512-newshort6800_epoch0_step1000\n",
      "Rank16-ctx512-newshort6800_epoch0_step2000\n",
      "Rank16-ctx512-newshort6800_epoch0_step3000\n",
      "Rank16-ctx512-newshort6800_epoch1_step1000\n",
      "Rank16-ctx512-newshort6800_epoch1_step2000\n",
      "Rank16-ctx512-newshort6800_epoch1_step3000\n",
      "Rank16-ctx512-regular2-12w\n",
      "Rank16-ctx512-regular2-12w-addApi-15k\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-Continue-18K_epoch0_step9000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step10000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step11000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step12000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step13000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step14000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step15000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step16000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step17000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step18000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000-largerConver76k_epoch0_step9000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w-addConver-17k_epoch0_step9000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step10000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step11000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step12000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step13000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step14000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step15000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step16000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step17000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step18000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step19000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step20000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step21000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step22000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step23000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step24000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step25000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step26000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step27000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step28000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step29000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step30000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step31000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step32000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step33000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step34000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step35000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step36000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step37000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step38000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step39000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step40000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step41000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step42000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step43000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step44000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step45000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step46000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step47000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step48000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step49000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step50000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step51000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step52000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step53000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step54000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step55000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step56000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step57000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step58000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step59000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step60000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step61000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step62000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step63000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step64000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step65000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step66000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step67000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step68000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step69000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step70000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step71000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step72000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step73000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step74000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step75000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step76000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step77000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step78000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step1000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step2000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step3000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step4000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step5000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step6000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step7000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step79000-Continue-18K_epoch0_step9000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step8000\n",
      "Rank16-ctx512-regular2-12w_epoch0_step9000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step1000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step2000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step3000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step4000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step5000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step6000\n",
      "Rank16-ctx512-regular2-4w_epoch0_step7000\n",
      "flagged\n",
      "glm3\n",
      "glm3-3-dataset-Rank64\n",
      "glm3-3-dataset-Rank64_1000\n",
      "glm3-3-dataset-Rank64_10000\n",
      "glm3-3-dataset-Rank64_11000\n",
      "glm3-3-dataset-Rank64_12000\n",
      "glm3-3-dataset-Rank64_13000\n",
      "glm3-3-dataset-Rank64_14000\n",
      "glm3-3-dataset-Rank64_15000\n",
      "glm3-3-dataset-Rank64_16000\n",
      "glm3-3-dataset-Rank64_17000\n",
      "glm3-3-dataset-Rank64_18000\n",
      "glm3-3-dataset-Rank64_19000\n",
      "glm3-3-dataset-Rank64_2000\n",
      "glm3-3-dataset-Rank64_20000\n",
      "glm3-3-dataset-Rank64_21000\n",
      "glm3-3-dataset-Rank64_22000\n",
      "glm3-3-dataset-Rank64_23000\n",
      "glm3-3-dataset-Rank64_24000\n",
      "glm3-3-dataset-Rank64_25000\n",
      "glm3-3-dataset-Rank64_26000\n",
      "glm3-3-dataset-Rank64_27000\n",
      "glm3-3-dataset-Rank64_28000\n",
      "glm3-3-dataset-Rank64_29000\n",
      "glm3-3-dataset-Rank64_3000\n",
      "glm3-3-dataset-Rank64_30000\n",
      "glm3-3-dataset-Rank64_31000\n",
      "glm3-3-dataset-Rank64_32000\n",
      "glm3-3-dataset-Rank64_33000\n",
      "glm3-3-dataset-Rank64_34000\n",
      "glm3-3-dataset-Rank64_35000\n",
      "glm3-3-dataset-Rank64_36000\n",
      "glm3-3-dataset-Rank64_37000\n",
      "glm3-3-dataset-Rank64_38000\n",
      "glm3-3-dataset-Rank64_39000\n",
      "glm3-3-dataset-Rank64_4000\n",
      "glm3-3-dataset-Rank64_40000\n",
      "glm3-3-dataset-Rank64_41000\n",
      "glm3-3-dataset-Rank64_42000\n",
      "glm3-3-dataset-Rank64_43000\n",
      "glm3-3-dataset-Rank64_44000\n",
      "glm3-3-dataset-Rank64_45000\n",
      "glm3-3-dataset-Rank64_46000\n",
      "glm3-3-dataset-Rank64_47000\n",
      "glm3-3-dataset-Rank64_48000\n",
      "glm3-3-dataset-Rank64_49000\n",
      "glm3-3-dataset-Rank64_5000\n",
      "glm3-3-dataset-Rank64_50000\n",
      "glm3-3-dataset-Rank64_51000\n",
      "glm3-3-dataset-Rank64_6000\n",
      "glm3-3-dataset-Rank64_7000\n",
      "glm3-3-dataset-Rank64_8000\n",
      "glm3-3-dataset-Rank64_9000\n",
      "glm3-3-dataset-Rank64_continue_API\n",
      "glm3-3-dataset-Rank64_low_lr\n",
      "glm3-3-dataset-Rank64_low_lr_1000\n",
      "glm3-3-dataset-Rank64_low_lr_10000\n",
      "glm3-3-dataset-Rank64_low_lr_11000\n",
      "glm3-3-dataset-Rank64_low_lr_12000\n",
      "glm3-3-dataset-Rank64_low_lr_13000\n",
      "glm3-3-dataset-Rank64_low_lr_14000\n",
      "glm3-3-dataset-Rank64_low_lr_15000\n",
      "glm3-3-dataset-Rank64_low_lr_16000\n",
      "glm3-3-dataset-Rank64_low_lr_17000\n",
      "glm3-3-dataset-Rank64_low_lr_18000\n",
      "glm3-3-dataset-Rank64_low_lr_19000\n",
      "glm3-3-dataset-Rank64_low_lr_2000\n",
      "glm3-3-dataset-Rank64_low_lr_3000\n",
      "glm3-3-dataset-Rank64_low_lr_4000\n",
      "glm3-3-dataset-Rank64_low_lr_5000\n",
      "glm3-3-dataset-Rank64_low_lr_6000\n",
      "glm3-3-dataset-Rank64_low_lr_7000\n",
      "glm3-3-dataset-Rank64_low_lr_8000\n",
      "glm3-3-dataset-Rank64_low_lr_9000\n",
      "glm3-4-dataset-Rank4_3_epochs_1000\n",
      "glm3-4-dataset-Rank64_3_epochs\n",
      "glm3-4-dataset-Rank64_3_epochs_1000\n",
      "glm3-4-dataset-Rank64_3_epochs_10000\n",
      "glm3-4-dataset-Rank64_3_epochs_11000\n",
      "glm3-4-dataset-Rank64_3_epochs_12000\n",
      "glm3-4-dataset-Rank64_3_epochs_13000\n",
      "glm3-4-dataset-Rank64_3_epochs_14000\n",
      "glm3-4-dataset-Rank64_3_epochs_15000\n",
      "glm3-4-dataset-Rank64_3_epochs_16000\n",
      "glm3-4-dataset-Rank64_3_epochs_17000\n",
      "glm3-4-dataset-Rank64_3_epochs_18000\n",
      "glm3-4-dataset-Rank64_3_epochs_19000\n",
      "glm3-4-dataset-Rank64_3_epochs_2000\n",
      "glm3-4-dataset-Rank64_3_epochs_20000\n",
      "glm3-4-dataset-Rank64_3_epochs_21000\n",
      "glm3-4-dataset-Rank64_3_epochs_22000\n",
      "glm3-4-dataset-Rank64_3_epochs_23000\n",
      "glm3-4-dataset-Rank64_3_epochs_24000\n",
      "glm3-4-dataset-Rank64_3_epochs_25000\n",
      "glm3-4-dataset-Rank64_3_epochs_26000\n",
      "glm3-4-dataset-Rank64_3_epochs_27000\n",
      "glm3-4-dataset-Rank64_3_epochs_28000\n",
      "glm3-4-dataset-Rank64_3_epochs_29000\n",
      "glm3-4-dataset-Rank64_3_epochs_3000\n",
      "glm3-4-dataset-Rank64_3_epochs_30000\n",
      "glm3-4-dataset-Rank64_3_epochs_31000\n",
      "glm3-4-dataset-Rank64_3_epochs_4000\n",
      "glm3-4-dataset-Rank64_3_epochs_5000\n",
      "glm3-4-dataset-Rank64_3_epochs_6000\n",
      "glm3-4-dataset-Rank64_3_epochs_7000\n",
      "glm3-4-dataset-Rank64_3_epochs_8000\n",
      "glm3-4-dataset-Rank64_3_epochs_9000\n",
      "glm3-4-dataset-Rank8_3_epochs\n",
      "glm3-4-dataset-Rank8_3_epochs_1000\n",
      "glm3-4-dataset-Rank8_3_epochs_2000\n",
      "glm3-4-dataset-Rank8_3_epochs_epoch0_step1000\n",
      "glm3-4-dataset-Rank8_3_epochs_epoch1_step1000\n",
      "glm3-Rank64\n",
      "glm3-Rank64_1000\n",
      "glm3-Rank64_10000\n",
      "glm3-Rank64_11000\n",
      "glm3-Rank64_12000\n",
      "glm3-Rank64_13000\n",
      "glm3-Rank64_14000\n",
      "glm3-Rank64_15000\n",
      "glm3-Rank64_16000\n",
      "glm3-Rank64_17000\n",
      "glm3-Rank64_18000\n",
      "glm3-Rank64_19000\n",
      "glm3-Rank64_2000\n",
      "glm3-Rank64_20000\n",
      "glm3-Rank64_21000\n",
      "glm3-Rank64_22000\n",
      "glm3-Rank64_23000\n",
      "glm3-Rank64_24000\n",
      "glm3-Rank64_25000\n",
      "glm3-Rank64_26000\n",
      "glm3-Rank64_27000\n",
      "glm3-Rank64_3000\n",
      "glm3-Rank64_4000\n",
      "glm3-Rank64_5000\n",
      "glm3-Rank64_6000\n",
      "glm3-Rank64_7000\n",
      "glm3-Rank64_8000\n",
      "glm3-Rank64_9000\n",
      "glm3_1000\n",
      "glm3_10000\n",
      "glm3_11000\n",
      "glm3_12000\n",
      "glm3_13000\n",
      "glm3_14000\n",
      "glm3_15000\n",
      "glm3_16000\n",
      "glm3_17000\n",
      "glm3_18000\n",
      "glm3_19000\n",
      "glm3_2000\n",
      "glm3_20000\n",
      "glm3_21000\n",
      "glm3_22000\n",
      "glm3_23000\n",
      "glm3_24000\n",
      "glm3_25000\n",
      "glm3_26000\n",
      "glm3_27000\n",
      "glm3_3000\n",
      "glm3_4000\n",
      "glm3_5000\n",
      "glm3_6000\n",
      "glm3_7000\n",
      "glm3_8000\n",
      "glm3_9000\n"
     ]
    }
   ],
   "source": [
    "!ls /root/autodl-tmp/checkpoints/previous/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b8c27e-fd8b-4fad-805e-bf64b577c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "    data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "    output_dir = '/root/autodl-tmp/checkpoints/previous/glm3-Rank64_27000'\n",
    "    lora_list = []#['/root/autodl-tmp/checkpoints/previous/Rank16-ctx512-regular2-12w-addApi-15k_epoch0_step8000']\n",
    "    #output_dir = '/root/autodl-tmp/checkpoints/glm3-single_query_turbo3'\n",
    "    #output_dir = '/root/autodl-tmp/checkpoints/glm3-full_query_turbo3'\n",
    "    \n",
    "    num_train_epochs = 5\n",
    "    batch_size = 8\n",
    "    max_tokens = 192\n",
    "    max_query = 64\n",
    "    lr = 1e-5\n",
    "    warm_up_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000f3b7f-0fca-41da-91c1-a42cea3f3cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fade488-5b96-4912-aa8e-e161377a0ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a4aba6-54aa-4e9f-9176-67c536616432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_lora(base_model_path, lora_path, device):\n",
    "    # 载入基座模型\n",
    "    base_model = AutoModel.from_pretrained(base_model_path, trust_remote_code=True).to(device).half()\n",
    "    # 暂存用以验证权重是否改变\n",
    "    first_weight = base_model.transformer.encoder.layers[0].self_attention.query_key_value.weight\n",
    "    first_weight_old = first_weight.clone()\n",
    "    \n",
    "    # 载入lora结构的模型\n",
    "    lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    # 合并lora结构\n",
    "    lora_model = lora_model.merge_and_unload()\n",
    "    lora_model.train(False)\n",
    "    \n",
    "    # 验证结构\n",
    "    assert not torch.allclose(first_weight_old, first_weight), 'Weight Should Change after Lora Merge'\n",
    "    \n",
    "    # 给模型改名\n",
    "    deloreanized_sd = {\n",
    "        k.replace(\"base_model.model.\", \"\"): v\n",
    "        for k, v in lora_model.state_dict().items()\n",
    "        if \"lora\" not in k\n",
    "    }\n",
    "    \n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5f367d-c4ad-4c78-8530-ef6c78ccfd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if []:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10d86e1-4939-4a3d-b5d1-86399cbc34d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 359 µs, total: 197 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32aa151-d656-458b-9ac9-cfb7746260a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015006780624389648,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720929936beb4609a171e74ec96008eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = merge_lora(CFG.model_path, CFG.output_dir, device = torch.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43027a30-cd1c-4126-8614-c43f369b29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().half()#float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50353c3b-0e50-4b48-8ebc-abff706e3b6d",
   "metadata": {},
   "source": [
    "```\n",
    "model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().half()#float()\n",
    "if CFG.lora_list:\n",
    "    for lora_path in CFG.lora_list :\n",
    "        model = model_tools.merge_lora_train(model, lora_path)\n",
    "#model = PeftModel.from_pretrained(model, CFG.output_dir)\n",
    "model = model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87305695-65e5-4ec2-babc-c0f72435cb52",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "%%time\n",
    "#model_tools.chat(lora_model, tokenizer, sys_prompt=False)\n",
    "prompt=\"我最近遇到的苦恼是，我周围的人经常炫耀他们的成就和成功，而这让我感到非常不适。我发现自己的好胜心太强，总是希望自己能够取得更多的成就，但是当看到别人取得成功时，我就会感到自卑和沮丧。这种情况经常发生在工作场合或社交场合，不论是同事还是朋友，他们总是不遗余力地展示他们的成就，比如晋升了职位、获得了奖项、完成了一项重要项目等等。每次听到这些消息，我就会感到自己的成就相对较少，甚至开始怀疑自己的能力和价值。我明白每个人都有自己的成就和努力，但是我无法摆脱对比和竞争的心态。这使得我无法真正享受自己的成就，而是只关注别人取得的更大的成功。我知道这样的想法是不健康的，但我很难控制自己的情绪和思维。这种苦恼对我的心理和情绪造成了很大的压力。我感到自己陷入了一个比较和竞争的漩涡中，无法真正放松和快乐。而且，这种情绪也影响了我与他人的关系，我开始对别人的成功感到嫉妒和羡慕，而不是真心为他们感到高兴。我希望能够找到一种方法来摆脱这种不健康的心态，开始真正关注自己的成就和价值。也许我需要更多的自我接受和肯定，以及学会与他人分享快乐和成功，而不是将其作为竞争的对象。我知道这并不容易，但我相信只有这样，我才能真正摆脱这个苦恼，过上更加平静和满足的生活。\"\n",
    "#prompt='我最近总是陷入犹豫不决的困境中，这让我感到非常苦恼。无论是在工作上还是生活中，我总是难以做出决定，害怕做出错误的选择。每当面临选择时，我开始思考各种可能的后果和影响，这导致我无法做出明智的决定。这种犹豫让我丧失了自信心，让我怀疑自己的能力和智慧。我深知这种拖延和犹豫只会让问题变得更加复杂，但我却无法摆脱这个困境，这让我感到非常困扰和沮丧。我真的希望能够找到方法克服这种犹豫不决的心理障碍，重新找回自己的自信和决断力。'\n",
    "#prompt='我最近非常苦恼，因为我的孩子学习不自觉，我担心他考不上高中。虽然他是一个聪明的孩子，但是他总是对学习缺乏兴趣，经常沉迷于手机游戏和社交媒体，无法集中精力在功课上。我试过多种方法来激发他的学习兴趣，包括给他设定目标，提供奖励，找家教辅导，甚至限制他使用手机的时间。但是结果都并不理想，他仍然没有改观。我曾经和他谈论过考高中的重要性，以及将来的发展机会，但他似乎对此不以为意。每当我看到他在晚上熬夜玩游戏，或者在考试时临时抱佛脚，我就感到非常沮丧和无助。我担心他如果继续这样下去，将来他会错过很多机会，难以实现自己的梦想。同时，我也为他的未来感到担忧，担心他没有足够的学术基础和素质去面对竞争激烈的高中阶段。我知道作为家长，我应该给予他更多的支持和指导，但是我感到自己的能力有限，无法解决这个问题。我经常为此焦虑不已，甚至影响到了我的工作和生活。我希望能够找到更有效的方法来帮助他，让他重新找回对学习的热情，顺利考上理想的高中。'\n",
    "#prompt='男生对女生抽烟是什么态度？ 会喜欢偶尔抽烟的女生吗？我是那种偶尔心烦的时候一个人躲起来抽烟的女生，想知道要是哪次被我喜欢的男生撞见了，他会是什么感觉。'\n",
    "#prompt='从广州访问一个北京的服务器，在IP的每一跳是怎么实现的？ 客户端在广州的某个地方，服务器在北京的某个地方，在客户端访问服务器的时候，中间肯定会经过N个跳点，路线是可以很多的，这中间是怎么完成这个过程的？'\n",
    "#model_tools.chat(model, tokenizer)\n",
    "prompt='我还有期末考试要准备，压力倍增啊。每次考试都感到很有压力，不知道怎么办了。'\n",
    "#prompt='还有关于教资面试，我真的有点害怕。觉得自己的表现可能不够好。'\n",
    "if True:\n",
    "    response, history = model.chat(tokenizer, prompt, history=[], do_sample=True, temperature=0.2, repetition_penalty=1.2)\n",
    "    print(response)\n",
    "else:\n",
    "    model_tools.chat(model, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b644f9-78f0-4590-938b-1a24d0527fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_tools.chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc8a44f-5a4b-47da-b869-796cbba892b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer.build_chat_input('你是谁', [{'role': 'user', 'content': '你好'}, {'role': 'assistant', 'metadata': '', 'content': '是的，你好。请问有什么我可以帮到你的吗？'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d117f654-8fca-41e5-9d7b-0fa80d145cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.stream_chat??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1763d9-75c2-4636-a51a-7e742b94c5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用TanTaili微调模型（基于ChatGLM3），输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 我的小孩是国际学校一位初三学生，因为学校颁布了更严厉的制度，现在不想上学，说自己得了抑郁症\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：我很抱歉听到您的孩子遇到了这样的情况。对于这种情况，建议您与孩子的医生进行沟通，了解其具体的情况和治疗方案，同时也要积极关注孩子的心理健康状况并给予适当的心理支持。如果需要的话，也可以咨询专业的心理咨询师或儿童心理医生的意见。希望我的回答能够对您有所帮助！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"欢迎使用TanTaili微调模型（基于ChatGLM3），输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [0, 2]#[0, 2, 31002, 64795]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "stop = StopOnTokens()\n",
    "    \n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nTanTaili-6B：{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLM：\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history,\n",
    "                                                                    do_sample=True, \n",
    "                                                                    temperature=0.2,\n",
    "                                                                    top_p=1,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True,\n",
    "                                                                    repetition_penalty=1.3,\n",
    "                                                                    stopping_criteria = StoppingCriteriaList([stop]),\n",
    "                                                                   ):\n",
    "            #print(history)\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    print(response[current_length:], end=\"\", flush=True)\n",
    "                except:\n",
    "                    break\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076b882-8bd7-497c-bbe4-86435f218a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498f28f-0f40-4ec8-8782-4c3883537dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33012b7c-f36d-4c8c-95d2-53798294632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "人到老年似乎越来越孤独，孤独终老会不会很可怜？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c15154-4df7-4729-b609-2028409d2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "我是个18岁的女生，最近一年来一直有想自残和自杀的念头，但害怕别人知道。\n",
    "特别是第一年高考前开始的，觉得自己很差劲，一直在自残，但特别害怕别人发现。以前我是很快乐的，但现在只有和别人在一起才能感到快乐，\n",
    "一个人时就会感到无力和想自残，特别是在晚上，就会特别想自杀。每天都想睡觉，根本醒不来。而且吃完饭会恶心。在不开心的时候，我经常会想自杀的方式和死亡的感受。\n",
    "但我内心有一道界限告诉自己绝对不可以去死。自残也是有阵痛期的。最近我没有自残了，但我很害怕同学发现我身上的伤疤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929538f-3dae-4668-abe8-aad6459c049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "给出三个保持健康的提示\n",
    "朱利叶斯·凯撒是如何死的？\n",
    "解释植物的细胞呼吸过程。\n",
    "给我列出五个在超市能够购买到的牌子的饼干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87c43c-3771-48cd-9570-754b1a30cd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "当别人不注意我的时候，我就很愤怒，该怎么办？如题。我想做一个不用那么去争取别人注意力，又能被别人看到的人。\n",
    "比如说，开会的时候，大家会讨论，而忽略我；或者新的环境里，有很多小团体，他们也不会主动叫上我。\n",
    "一开始是尴尬，时间长了次数多了，我开始感受到被忽略的愤怒，知道自己不应该这样，可是就是好生气，该怎么办呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404db042-0431-420d-83ce-24b72902f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "我最近感到有些焦虑\n",
    "我的小孩是国际学校一位初三学生，因为学校颁布了更严厉的制度，现在不想上学，说自己得了抑郁症\n",
    "事实上小孩已经看了心里医生，医生说焦虑，轻度抑郁\n",
    "学校校长知道我情况，主任，叫来与我沟通，说会全力帮助我，我心情好多了，感觉精气神十足好象日子变得又美好了!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10cfb9-f9c4-40eb-9f55-027a10e8b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "贪婪解码，无repetition惩罚\n",
    "#原版\n",
    "ChatGLM：你的情况其实很常见，很多人都会在不注意的时候感到被忽视或者被冷落。这可能是由于多种原因，比如社交焦虑、自卑感、沟通技巧欠佳等等。以下是一些可能有帮助的建议：\n",
    "\n",
    "1. 自我观察：试着观察自己的情绪反应，了解自己在不同情境下的情绪反应。这可以帮助你更好地理解自己的感受，找到可能的原因。\n",
    "2. 沟通技巧：提高自己的沟通技巧，尝试主动参与讨论或者活动，表达自己的想法和观点。同时，也要注意倾听他人的意见，尊重他人。\n",
    "3. 建立自信：建立自信可以帮助你更好地应对社交场合，提高自己的存在感。你可以通过学习新技能、参加社交活动等方式来提高自信。\n",
    "4. 寻求支持：如果你感到很难应对，可以寻求朋友、家人或者专业人士的帮助和支持。他们可以帮助你更好地理解自己的感受，提供建议和帮助。\n",
    "5. 放松心态：当你感到被忽视或者被冷落的时候，试着放松心态，不要过于在意别人的看法。你可以尝试深呼吸、冥想等方式来放松自己。\n",
    "\n",
    "希望这些建议对你有所帮助。记住，不要放弃自己，相信自己可以做得更好。\n",
    "\n",
    "#调完\n",
    "ChatGLM：看到你的问题，我能够感受到你的愤怒。其实，在别人不注意你的时候，你很生气，是因为你希望别人能够关注你，希望别人能够看到你。但是，你又不想让别人看到你，所以你很生气。\n",
    "\n",
    "#40000\n",
    "ChatGLM：看到你的问题，我感受到你的愤怒，同时也看到你的无奈。你很想得到别人的关注，但是别人似乎并不需要你，这让你感到很无助。你很想改变，但是不知道该如何改变。我理解你的感受，因为我也曾经有过这样的经历。\n",
    "\n",
    "#30000\n",
    "ChatGLM：看到你的问题，我能够感受到你的愤怒和挫败感。你很想得到别人的关注，但是现实却让你感到被忽略。这确实是一个很让人不舒服的情况。\n",
    "\n",
    "#20000\n",
    "ChatGLM：听到你的问题，我能感受到你的痛苦和困惑。首先，我想告诉你，你并不孤单，很多人都曾经有过类似的感受。在现实生活中，我们经常遇到各种挫折和困难，这并不代表我们没有价值或者不被重视。\n",
    "\n",
    "首先，我想提醒你，你的感受是可以理解的。当我们感到被忽视或者不被关注时，会感到非常沮丧和失望。但是，请相信，你的感受是有意义的，而且你值得被关注和重视。\n",
    "\n",
    "其次，我建议你尝试与身边的人沟通。有时候，我们可能没有意识到自己的行为和态度对他人造成了伤害。通过与别人坦诚地交流，你可以更好地理解他们的感受和需求，从而建立更健康的关系。\n",
    "\n",
    "此外，你也可以尝试寻找一些能够带给你快乐和满足感的活动。这可以是与朋友一起出去玩，参加一些兴趣小组，或者追求自己的兴趣爱好。通过这些活动，你可以找到属于自己的快乐和满足感，从而减少对他人关注的依赖。\n",
    "\n",
    "最后，我想强调的是，你的感受是重要的，而且你有权利得到关注和被重视。请记住，你是一个有价值的人，你的存在本身就是有意义的。如果你需要进一步的支持和指导，我会一直在这里帮助你。\n",
    "\n",
    "\n",
    "#10000\n",
    "结果很有趣\n",
    "ChatGLM：听起来你很在意别人对你的关注，对吗？\n",
    "\n",
    "用户： 是的，我觉得这也算是尊重的一种\n",
    "\n",
    "ChatGLM：没错，尊重他人也是非常重要的。你提到在开会的时候，别人会忽略你，这让你感到很生气。\n",
    "\n",
    "用户： 对，我还不知道该怎么办？\n",
    "\n",
    "ChatGLM：首先，你可以试着主动参与讨论，提出自己的想法和意见。这样，别人就会注意到你的存在，并且会给你机会表达自己的观点。\n",
    "\n",
    "用户： 我很多时候也尝试过了，但是每次看见他们嫌弃的眼神，我自己的情绪也很难克制住\n",
    "\n",
    "ChatGLM：我理解你的感受，但是情绪控制是很重要的。你可以试着深呼吸，让自己冷静下来，然后再参与讨论。这样，你就能更好地表达自己的意见，并且不会被别人的眼神所影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e191d-0dc6-4673-b80f-fcf6bf63dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#原回答\n",
    "你的困扰我理解，孩子的学习问题常常是家长最头疼的问题之一。以下是一些可能对你有帮助的建议：\n",
    "\n",
    "1. 建立积极的学习环境：为孩子创造一个安静、无干扰的学习环境，减少诱惑他沉迷于手机游戏和社交媒体的因素。\n",
    "\n",
    "2. 设定具体可行的目标：设定明确、具体的学习目标，并让孩子参与其中，让他明白学习的意义和目标，激发他的学习动力。\n",
    "\n",
    "3. 鼓励学习兴趣：尝试发现和培养孩子的兴趣爱好，让孩子在兴趣中学习，提高学习的自觉性和主动性。\n",
    "\n",
    "4. 适度监督和引导：适度地监督孩子的学习进度，引导他建立良好的学习习惯，同时也要尊重他的独立性，避免过度干预。\n",
    "\n",
    "5. 寻求专业帮助：如果以上方法都尝试过了，但孩子的学习情况仍然没有改善，可以考虑寻求专业的帮助，如请教专业的教育顾问或心理医生。\n",
    "\n",
    "请记住，每个孩子都是独一无二的，需要根据孩子的个性和需要来制定个性化的教育计划。希望这些建议能对你有所帮助。\n",
    "\n",
    "#smile all samples\n",
    "\n",
    "首先，我想告诉你，你的孩子是一个聪明、有潜力的孩子。\n",
    "他只是暂时对学习缺乏兴趣，而且沉迷于手机游戏和社交媒体。\n",
    "这并不代表他没有能力去学习，只是暂时没有找到合适的学习方式。\n",
    "对于这种情况，我们可以从以下几个方面来帮助他。\n",
    "首先，我们可以尝试调整他的学习环境，创造一个适合学习的氛围。\n",
    "比如，可以给他安排一个安静的学习空间，减少干扰，让他能够专注于学习。\n",
    "其次，我们可以引导他去了解学习的意义和价值，让他明白学习对他的未来发展是非常重要的。\n",
    "最后，我们可以鼓励他去尝试不同的学习方法，找到适合自己的学习方式。\n",
    "例如，可以让他尝试使用学习工具，如思维导图、时间管理等等。\n",
    "同时，我们也要给予他足够的支持和鼓励，让他感到被尊重和理解。\n",
    "我相信，只要我们用心去引导和帮助他，他一定能够重新找回对学习的热情，顺利考上理想的高中。\n",
    "希望我的回答能够对你有所帮助。如果还有其他问题，欢迎继续向我提问。祝你好运！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d68e1-b63b-4688-8fa2-d7836738eb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#原回答\n",
    "I'm sorry to hear that you're feeling anxious. There are many possible reasons why you might be feeling anxious, and it's important to try to identify the underlying cause. Here are a few things you might try:\n",
    "\n",
    "1. Keep a journal: Writing down your thoughts and feelings can help you identify patterns and triggers that might be causing your anxiety.\n",
    "2. Try to relax: Engage in activities that help you relax, such as deep breathing, meditation, or yoga.\n",
    "3. Exercise regularly: Exercise can help reduce stress and anxiety.\n",
    "4. Cut back on caffeine and alcohol: These substances can increase your anxiety levels.\n",
    "5. Consider talking to a mental health professional: A therapist or counselor can help you identify the underlying cause of your anxiety and develop coping strategies.\n",
    "\n",
    "Remember, it's okay to feel anxious sometimes, and there is no one-size-fits-all solution. It's important to take care of yourself and seek help if you need it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
