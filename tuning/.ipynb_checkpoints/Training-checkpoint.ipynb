{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3730bd-517b-4c1c-89ff-7841bfd38113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是第一个完整训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d1acb-1201-44a0-91e2-00230f6e72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_train_epochs = 1\n",
    "    output_dir = '/root/autodl-fs/weights/chatglm3-6b_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e147d2cf-4919-4b24-9e43-3b120ab9c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a1bd0d-3bb6-432b-b5f9-c77b982dedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a9d856-a53d-41cd-9ca3-0186c3be68d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/chatglm3-6b'\n",
    "data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # lora的目标位置，具体有哪些可选项可打印出源码中的key_list 注意不同的模型中的定义名称不同\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc331aa4-9992-4ab2-8822-8ee8d3a1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009218215942382812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed83be6d547413db9580111cc3170ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.73 s, sys: 42.5 s, total: 50.2 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528572d7-82a1-4c3e-a9f8-2c1663c64db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6243584000\n",
      "Trainable Parameters: 6243584000\n",
      "Percentage of Trainable Parameters: 100.00%\n",
      "conducting peft lora ---------------\n",
      "Total Parameters: 6247483392\n",
      "Trainable Parameters: 3899392\n",
      "Percentage of Trainable Parameters: 0.06%\n"
     ]
    }
   ],
   "source": [
    "#施加peft lora\n",
    "model_tools.model_profile(model)\n",
    "print('conducting peft lora ---------------')\n",
    "model = get_peft_model(model, config)\n",
    "model_tools.model_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64da1570-4e5a-4a58-869a-646da5529a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample['question']\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample['response_j']\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                end_answer_index = input_ids.index(si['eop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length-0) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "285b1119-dddc-4253-8e67-da2137de3950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coll_fn(batch:list):\n",
    "    input_labels = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    for sample in batch:\n",
    "        # 实际上词表长度只有65024，所以int32就可以了 # attention_mask用bool就行 (我收回我的画，完全是玄学)\n",
    "        input_labels.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n",
    "        labels.append(torch.tensor(sample['labels'], dtype=torch.long))\n",
    "        attention_mask.append(torch.tensor(sample['attention_mask'], dtype=torch.float64)) #, dtype=torch.bool\n",
    "    batch = {'input_ids':input_labels, 'labels':labels, 'attention_mask': attention_mask}\n",
    "    batch = {name:torch.stack(item).cuda() for name,item in batch.items()} #对于一个元素不为基本元素的list，需要使用stack方法\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41977962-673c-48e9-a612-ea7ccca3a52f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.41 s, sys: 25.3 ms, total: 5.44 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "finetuning_instruction=instruction_dataset(data_path, tokenizer, 128, 32)\n",
    "instruction_loader = DataLoader(finetuning_instruction, batch_size=2, shuffle=True, collate_fn=coll_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf0607dd-e9ae-4233-8d09-679bf466f26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义参数优化器 & 学习率优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=(len(instruction_loader) * CFG.num_train_epochs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191e0ef-e4f8-4553-8126-198c462b5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练阶段\n",
    "model.train()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(CFG.num_train_epochs):\n",
    "    for step, batch in tqdm(enumerate(instruction_loader)):\n",
    "        # 前向传播 & 计算loss (使用fp16)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        # 优化器调度\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d849b98-412b-4b42-957e-d357eb9da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型的保存\n",
    "model.save_pretrained(CFG.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02dd2b-edf6-4045-bbc1-66b5ea59c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora(base_model_path, lora_path):\n",
    "    # 载入基座模型\n",
    "    base_model = AutoModel.from_pretrained(model_path, trust_remote_code=True).cuda().half()\n",
    "    # 暂存用以验证权重是否改变\n",
    "    first_weight = base_model.base_model.layers[0].attention.query_key_value.weight\n",
    "    first_weight_old = first_weight.clone()\n",
    "    \n",
    "    # 载入lora结构的模型\n",
    "    lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    # 合并lora结构\n",
    "    lora_model = lora_model.merge_and_unload()\n",
    "    lora_model.train(False)\n",
    "    \n",
    "    # 验证结构\n",
    "    assert not torch.allclose(first_weight_old, first_weight), 'Weight Should Change after Lora Merge'\n",
    "    \n",
    "    # 给模型改名\n",
    "    deloreanized_sd = {\n",
    "        k.replace(\"base_model.model.\", \"\"): v\n",
    "        for k, v in lora_model.state_dict().items()\n",
    "        if \"lora\" not in k\n",
    "    }\n",
    "    \n",
    "    return lora_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
