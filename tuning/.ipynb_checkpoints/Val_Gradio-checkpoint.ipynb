{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71ac6da-4357-4554-b3f8-57bffb23340b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flagged\t\t\t     glm3-3-dataset-Rank64_32000\n",
      "glm3-3-dataset-Rank64\t     glm3-3-dataset-Rank64_33000\n",
      "glm3-3-dataset-Rank64_1000   glm3-3-dataset-Rank64_34000\n",
      "glm3-3-dataset-Rank64_10000  glm3-3-dataset-Rank64_35000\n",
      "glm3-3-dataset-Rank64_11000  glm3-3-dataset-Rank64_36000\n",
      "glm3-3-dataset-Rank64_12000  glm3-3-dataset-Rank64_37000\n",
      "glm3-3-dataset-Rank64_13000  glm3-3-dataset-Rank64_38000\n",
      "glm3-3-dataset-Rank64_14000  glm3-3-dataset-Rank64_39000\n",
      "glm3-3-dataset-Rank64_15000  glm3-3-dataset-Rank64_4000\n",
      "glm3-3-dataset-Rank64_16000  glm3-3-dataset-Rank64_40000\n",
      "glm3-3-dataset-Rank64_17000  glm3-3-dataset-Rank64_41000\n",
      "glm3-3-dataset-Rank64_18000  glm3-3-dataset-Rank64_42000\n",
      "glm3-3-dataset-Rank64_19000  glm3-3-dataset-Rank64_43000\n",
      "glm3-3-dataset-Rank64_2000   glm3-3-dataset-Rank64_44000\n",
      "glm3-3-dataset-Rank64_20000  glm3-3-dataset-Rank64_45000\n",
      "glm3-3-dataset-Rank64_21000  glm3-3-dataset-Rank64_46000\n",
      "glm3-3-dataset-Rank64_22000  glm3-3-dataset-Rank64_47000\n",
      "glm3-3-dataset-Rank64_23000  glm3-3-dataset-Rank64_48000\n",
      "glm3-3-dataset-Rank64_24000  glm3-3-dataset-Rank64_49000\n",
      "glm3-3-dataset-Rank64_25000  glm3-3-dataset-Rank64_5000\n",
      "glm3-3-dataset-Rank64_26000  glm3-3-dataset-Rank64_50000\n",
      "glm3-3-dataset-Rank64_27000  glm3-3-dataset-Rank64_51000\n",
      "glm3-3-dataset-Rank64_28000  glm3-3-dataset-Rank64_6000\n",
      "glm3-3-dataset-Rank64_29000  glm3-3-dataset-Rank64_7000\n",
      "glm3-3-dataset-Rank64_3000   glm3-3-dataset-Rank64_8000\n",
      "glm3-3-dataset-Rank64_30000  glm3-3-dataset-Rank64_9000\n",
      "glm3-3-dataset-Rank64_31000  previous\n"
     ]
    }
   ],
   "source": [
    "!ls /root/autodl-tmp/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b8c27e-fd8b-4fad-805e-bf64b577c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "    data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "    output_dir = '/root/autodl-tmp/checkpoints/glm3-3-dataset-Rank64_15000'\n",
    "    #output_dir = '/root/autodl-tmp/checkpoints/glm3-single_query_turbo3'\n",
    "    #output_dir = '/root/autodl-tmp/checkpoints/glm3-full_query_turbo3'\n",
    "    \n",
    "    num_train_epochs = 5\n",
    "    batch_size = 8\n",
    "    max_tokens = 192\n",
    "    max_query = 64\n",
    "    lr = 1e-5\n",
    "    warm_up_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000f3b7f-0fca-41da-91c1-a42cea3f3cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fade488-5b96-4912-aa8e-e161377a0ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a4aba6-54aa-4e9f-9176-67c536616432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_lora(base_model_path, lora_path):\n",
    "    # 载入基座模型\n",
    "    base_model = AutoModel.from_pretrained(base_model_path, trust_remote_code=True).cuda().half()\n",
    "    # 暂存用以验证权重是否改变\n",
    "    first_weight = base_model.transformer.encoder.layers[0].self_attention.query_key_value.weight\n",
    "    first_weight_old = first_weight.clone()\n",
    "    \n",
    "    # 载入lora结构的模型\n",
    "    lora_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    # 合并lora结构\n",
    "    lora_model = lora_model.merge_and_unload()\n",
    "    lora_model.train(False)\n",
    "    \n",
    "    # 验证结构\n",
    "    assert not torch.allclose(first_weight_old, first_weight), 'Weight Should Change after Lora Merge'\n",
    "    \n",
    "    # 给模型改名\n",
    "    deloreanized_sd = {\n",
    "        k.replace(\"base_model.model.\", \"\"): v\n",
    "        for k, v in lora_model.state_dict().items()\n",
    "        if \"lora\" not in k\n",
    "    }\n",
    "    \n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10d86e1-4939-4a3d-b5d1-86399cbc34d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006805896759033203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32b25eb44d5482092e55093e42541aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.11 s, sys: 17.4 s, total: 24.5 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer=AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "model=merge_lora(CFG.model_path, CFG.output_dir)\n",
    "#model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().half()#float()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b644f9-78f0-4590-938b-1a24d0527fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_tools.chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1763d9-75c2-4636-a51a-7e742b94c5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"欢迎使用TanTaili微调模型（基于ChatGLM3），输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nTanTaili-6B：{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLM：\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history,\n",
    "                                                                    do_sample=False, \n",
    "                                                                    temperature=0.1,\n",
    "                                                                    top_p=1,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True,\n",
    "                                                                    repetition_penalty=1.0,\n",
    "                                                                   ):\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    print(response[current_length:], end=\"\", flush=True)\n",
    "                except:\n",
    "                    break\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    \n",
    "    \n",
    "def inference(text):\n",
    "    response, history = model.chat(tokenizer, text, history=[])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce8e554-5f3e-4e42-b3fb-eb608f609b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gradio/utils.py:841: UserWarning: Expected 0 arguments for function <function main at 0x7f4ccc919dc0>, received 2.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/gradio/utils.py:849: UserWarning: Expected maximum 0 arguments for function <function main at 0x7f4ccc919dc0>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Missing file: /root/miniconda3/lib/python3.8/site-packages/gradio/frpc_linux_amd64_v0.2. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.2\n",
      "3. Move the file to this location: /root/miniconda3/lib/python3.8/site-packages/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=inference,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
