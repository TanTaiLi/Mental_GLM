{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e1f98f-9441-4da4-9ff0-4b01fa39c903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#这是初始状态的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3d1acb-1201-44a0-91e2-00230f6e72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_train_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e147d2cf-4919-4b24-9e43-3b120ab9c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a1bd0d-3bb6-432b-b5f9-c77b982dedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a9d856-a53d-41cd-9ca3-0186c3be68d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/chatglm3-6b'\n",
    "data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # lora的目标位置，具体有哪些可选项可打印出源码中的key_list 注意不同的模型中的定义名称不同\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc331aa4-9992-4ab2-8822-8ee8d3a1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cb6c4-4ece-4b27-9735-a9e8ce95d06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e357d0-5703-4db7-8c32-9e1d4cc5cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tools.model_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a56f8-5bd7-401e-a98d-d43b8a6c8b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7f426-8acc-4629-94a4-a31399eab5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tools.model_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e8dda-1e89-456f-9aaa-386567577e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4c308c-5e83-4599-ae50-f0111f4906a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(si[\"[gMASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00d874e-14b7-4447-906a-a149623f3436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '接下来', '将是', '我的', '问题', '。', '▁北京', '是', '哪', '国', '首都', '[gMASK]', 'sop', '▁中国', 'eop']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m*\u001b[39m context_length \u001b[38;5;241m+\u001b[39m input_ids[mask_position \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 根据最大长度进行后填充\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m pad_len \u001b[38;5;241m=\u001b[39m \u001b[43mmax_len\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids)\n\u001b[1;32m     25\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids \u001b[38;5;241m+\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m*\u001b[39m pad_len\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 填充部分不参与损失值计算\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_len' is not defined"
     ]
    }
   ],
   "source": [
    "max_src_len=100\n",
    "max_tgt_len=20\n",
    "\n",
    "src_tokens = tokenizer.tokenize(\"北京是哪国首都\")\n",
    "\n",
    "prompt_tokens = tokenizer.tokenize(\"接下来将是我的问题。\")\n",
    "\n",
    "if len(src_tokens) > max_src_len - len(prompt_tokens):\n",
    "    src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]\n",
    "\n",
    "tgt_tokens = tokenizer.tokenize(\"中国\")\n",
    "\n",
    "if len(tgt_tokens) > max_tgt_len:\n",
    "    tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "\n",
    "tokens = prompt_tokens + src_tokens + [tokenizer.decode(si[\"[gMASK]\"]), tokenizer.decode(si['sop'])] + tgt_tokens + [tokenizer.decode(si['eop'])]\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_length = input_ids.index(si['sop'])\n",
    "mask_position = context_length - 1\n",
    "# prompt和问题部分不参与损失值计算\n",
    "labels = [-100] * context_length + input_ids[mask_position + 1:]\n",
    "# 根据最大长度进行后填充\n",
    "pad_len = max_len - len(input_ids)\n",
    "input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "# 填充部分不参与损失值计算\n",
    "labels = labels + [-100] * pad_len\n",
    "# 区分有用的部分和填充的token\n",
    "attention_mask = []\n",
    "for input_id in input_ids:\n",
    "    if input_id != tokenizer.pad_token_id:\n",
    "        attention_mask.append(True)\n",
    "    else:\n",
    "        attention_mask.append(False)\n",
    "\n",
    "input_ids, labels, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de758bc9-e216-44df-8118-318f6f66bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataSet(Dataset):\n",
    "    \"\"\"数据处理函数\"\"\"\n",
    "    def __init__(self, data_path, tokenizer, max_len, max_src_len, prompt_text):\n",
    "        # -3是因为需要拼接三个特殊字符[gMASK]、<sop>、<eop>\n",
    "        max_tgt_len = max_len - max_src_len - 3\n",
    "        self.all_data = []\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for i, line in enumerate(fh):\n",
    "                sample = json.loads(line.strip())\n",
    "                # chatglm的token不是中文的字，是词\n",
    "                # add_special_tokens = True时会在末位添加[\"[gMASK]\", \"<sop>\"]\n",
    "                src_tokens = tokenizer.tokenize(sample[\"text\"])\n",
    "                # print(sample[\"text\"])\n",
    "                # print(src_tokens)\n",
    "                prompt_tokens = tokenizer.tokenize(prompt_text)\n",
    "                # 根据限制的长度对输入进行截断\n",
    "                if len(src_tokens) > max_src_len - len(prompt_tokens):\n",
    "                    src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]\n",
    "\n",
    "                tgt_tokens = tokenizer.tokenize(sample[\"answer\"])\n",
    "                # 根据限制的长度对输入进行截断\n",
    "                if len(tgt_tokens) > max_tgt_len:\n",
    "                    tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "                # 问、答之间需要通过特殊字符进行分割，同时需要添加终止符\n",
    "                # [gMASK]与<sop>作为模型生成结果的起始标记，属于同一个block，\n",
    "                # 所以这两个token对应的在原始文本中所在的位置是一样的，具体可参考这个issue https://github.com/THUDM/ChatGLM-6B/issues/1313\n",
    "                # tokens = prompt_tokens + src_tokens + [\"[gMASK]\", \"<sop>\"] + tgt_tokens + [\"<eop>\"]\n",
    "                tokens = prompt_tokens + src_tokens + [tokenizer.gmask_token, tokenizer.bos_token] + tgt_tokens + [tokenizer.eos_token]\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                context_length = input_ids.index(tokenizer.bos_token_id)\n",
    "                mask_position = context_length - 1\n",
    "                # prompt和问题部分不参与损失值计算\n",
    "                labels = [-100] * context_length + input_ids[mask_position + 1:]\n",
    "                # 根据最大长度进行后填充\n",
    "                pad_len = max_len - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "                # 填充部分不参与损失值计算\n",
    "                labels = labels + [-100] * pad_len\n",
    "                # 区分有用的部分和填充的token\n",
    "                attention_mask = []\n",
    "                for input_id in input_ids:\n",
    "                    if input_id != tokenizer.pad_token_id:\n",
    "                        attention_mask.append(True)\n",
    "                    else:\n",
    "                        attention_mask.append(False)\n",
    "                self.all_data.append(\n",
    "                    {\"text\": sample[\"text\"], \"answer\": sample[\"answer\"], \"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.all_data[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643120e-9f13-436c-8c38-022f57cc2369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        sample=json.loads(line)\n",
    "        print(tokenizer.encode(sample['question'], add_special_tokens=False))\n",
    "        print(tokenizer.tokenize(sample['question']))\n",
    "        print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sample['question'])))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da1570-4e5a-4a58-869a-646da5529a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, truncate_length, max_query_length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample['question']\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample['response_j']\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + si['gMASK'] + si['sop'] + answer_ids + si['eop']\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算\n",
    "                labels = [-100] * pre_context_length + input_ids[pre_context_length+1:]\n",
    "                labels = labels + [-100]*padding_length\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79876e-be12-43dd-90cb-24313f104343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    #初始化迭代对象\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                self.examples.append(json.loads(line))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    #构造迭代单位（以tensor的形式）\n",
    "    def __getitem__(self, index):\n",
    "        question=self.examples[index]['question']\n",
    "        response=self.examples[index]['response_j']\n",
    "        \n",
    "        tensor_question=tokenizer.encode(question, return_tensors='pt').cuda()[:,2:]\n",
    "        tensor_response=tokenizer.encode(response, return_tensors='pt').cuda()[:,2:]\n",
    "        \n",
    "        instruction = torch.cat([st['\\n'], st['<|user|>'], tensor_question, st['\\n'], st['<|assistant|>'], tensor_response], dim=1)\n",
    "        #decode的label即为下一个位置的token, 最后需要额外拼接一个终止符\n",
    "        label = torch.cat([instruction.squeeze()[1:].clone().view(1,-1), st['eop']], dim=1)\n",
    "        \n",
    "        return instruction, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc77085-5bf6-4ca7-818a-204f3defa99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = instruction_dataset(data_path, tokenizer)\n",
    "instruction_loader = DataLoader(instructions, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c382c9-aa35-4d2c-9d9b-65542cccabf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=(len(instruction_loader) * CFG.num_train_epochs),\n",
    "    )\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f5c08-3daa-4491-bebe-81d4f39a0c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "model.train()\n",
    "for epoch in range(CFG.num_train_epochs):\n",
    "    for batch in tqdm(instruction_loader):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        labels = batch[1].squeeze()\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs.loss, loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1)))\n",
    "        #break\n",
    "        loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "        #print(outputs.logits.view(-1, outputs.logits.size(-1)).shape, labels.view(-1))\n",
    "        print(loss)\n",
    "        #print()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 在这里检查和/或操作梯度\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                pass\n",
    "                #print(f'{name}\\n {param.requires_grad}')\n",
    "                #print(f\"{param.grad}\\n{param}\")\n",
    "                #print()\n",
    "                \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90741068-e931-404c-abcd-8ae1d5c840c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
