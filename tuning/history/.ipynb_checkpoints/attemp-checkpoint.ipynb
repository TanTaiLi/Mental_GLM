{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3730bd-517b-4c1c-89ff-7841bfd38113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是第一个完整训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d1acb-1201-44a0-91e2-00230f6e72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e147d2cf-4919-4b24-9e43-3b120ab9c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a1bd0d-3bb6-432b-b5f9-c77b982dedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a9d856-a53d-41cd-9ca3-0186c3be68d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/chatglm3-6b'\n",
    "data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # lora的目标位置，具体有哪些可选项可打印出源码中的key_list 注意不同的模型中的定义名称不同\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc331aa4-9992-4ab2-8822-8ee8d3a1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008024930953979492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6878632012e346ab9b7e2ad037a9a987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatGLMForConditionalGeneration' object has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1630\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1630\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatGLMForConditionalGeneration' object has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528572d7-82a1-4c3e-a9f8-2c1663c64db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#施加peft lora\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_tools\u001b[38;5;241m.\u001b[39mmodel_profile(\u001b[43mmodel\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconducting peft lora ---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#施加peft lora\n",
    "model_tools.model_profile(model)\n",
    "print('conducting peft lora ---------------')\n",
    "model = get_peft_model(model, config)\n",
    "model_tools.model_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da1570-4e5a-4a58-869a-646da5529a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample['question']\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample['response_j']\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                end_answer_index = input_ids.index(si['eop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length-0) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b1119-dddc-4253-8e67-da2137de3950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coll_fn(batch:list):\n",
    "    input_labels = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    for sample in batch:\n",
    "        # 实际上词表长度只有65024，所以int32就可以了 # attention_mask用bool就行 (我收回我的画，完全是玄学)\n",
    "        input_labels.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n",
    "        labels.append(torch.tensor(sample['labels'], dtype=torch.long))\n",
    "        attention_mask.append(torch.tensor(sample['attention_mask'], dtype=torch.float64)) #, dtype=torch.bool\n",
    "    batch = {'input_ids':input_labels, 'labels':labels, 'attention_mask': attention_mask}\n",
    "    batch = {name:torch.stack(item).cuda() for name,item in batch.items()} #对于一个元素不为基本元素的list，需要使用stack方法\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41977962-673c-48e9-a612-ea7ccca3a52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "finetuning_instruction=instruction_dataset(data_path, tokenizer, 128, 32)\n",
    "instruction_loader = DataLoader(finetuning_instruction, batch_size=2, shuffle=True, collate_fn=coll_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459d23c-c1cf-4408-a5ac-e7ca67854edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in instruction_loader:\n",
    "    pass\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0607dd-e9ae-4233-8d09-679bf466f26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义参数优化器 & 学习率优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-9)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=50,\n",
    "        num_training_steps=(len(instruction_loader) * CFG.num_train_epochs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cd222-1260-41ea-8454-1461569c5fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(475)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff0d58-18e6-450e-b0d7-e3c667c1b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(CFG.num_train_epochs):\n",
    "    for step, batch in tqdm(enumerate(instruction_loader)):\n",
    "        # 前向传播\n",
    "        outputs = model(**batch)\n",
    "        #break\n",
    "        # 计算loss\n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 优化器调度\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d849b98-412b-4b42-957e-d357eb9da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafa9dc-7dbd-48a7-b811-8ff9fa75b411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(CFG.num_train_epochs):\n",
    "    for step, batch in tqdm(enumerate(instruction_loader)):\n",
    "        # 前向传播\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        max_indices = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        #print(max_indices)\n",
    "        #print(outputs.logits)\n",
    "        #print(batch['labels'])\n",
    "        #break\n",
    "        # 计算loss\n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "        if step==10:break\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        for name,param in model.named_parameters():\n",
    "            break\n",
    "            print(name, param.grad)\n",
    "        print('***'*30)\n",
    "        if step==2:break\n",
    "        # 优化器调度\n",
    "        optimizer.step()\n",
    "        for name,param in model.named_parameters():\n",
    "            break\n",
    "            print(name, param.grad)\n",
    "        #lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
