{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3d1acb-1201-44a0-91e2-00230f6e72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_train_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e147d2cf-4919-4b24-9e43-3b120ab9c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a1bd0d-3bb6-432b-b5f9-c77b982dedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a9d856-a53d-41cd-9ca3-0186c3be68d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/chatglm3-6b'\n",
    "data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # lora的目标位置，具体有哪些可选项可打印出源码中的key_list 注意不同的模型中的定义名称不同\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc331aa4-9992-4ab2-8822-8ee8d3a1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00d874e-14b7-4447-906a-a149623f3436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '问题', '▁', '哪', '国', '[gMASK]', 'sop', '▁中国', 'eop']\n",
      "[30910, 31639, 30910, 55315, 54543, 64790, 64792, 34106, 64793, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      " [-100, -100, -100, -100, -100, 64792, 34106, 64793, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100] \n",
      " [True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "30 \n",
      " 30 \n",
      " 30\n"
     ]
    }
   ],
   "source": [
    "max_len=30\n",
    "max_src_len=20\n",
    "max_tgt_len = max_len - max_src_len - 3\n",
    "\n",
    "src_tokens = tokenizer.tokenize(\"哪国\")\n",
    "\n",
    "prompt_tokens = tokenizer.tokenize(\"问题\")\n",
    "\n",
    "if len(src_tokens) > max_src_len - len(prompt_tokens):\n",
    "    src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]\n",
    "\n",
    "tgt_tokens = tokenizer.tokenize(\"中国\")\n",
    "\n",
    "if len(tgt_tokens) > max_tgt_len:\n",
    "    tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "\n",
    "tokens = prompt_tokens + src_tokens + [tokenizer.decode(si[\"[gMASK]\"]), tokenizer.decode(si['sop'])] + tgt_tokens + [tokenizer.decode(si['eop'])]\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_length = input_ids.index(si['sop'])\n",
    "mask_position = context_length - 1\n",
    "# prompt和问题部分不参与损失值计算\n",
    "labels = [-100] * mask_position + input_ids[mask_position + 1:]\n",
    "# 根据最大长度进行后填充\n",
    "pad_len = max_len - len(input_ids)\n",
    "input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "# 填充部分不参与损失值计算\n",
    "labels = labels + [-100] * (pad_len+1)\n",
    "# 区分有用的部分和填充的token\n",
    "attention_mask = []\n",
    "for input_id in input_ids:\n",
    "    if input_id != tokenizer.pad_token_id:\n",
    "        attention_mask.append(True)\n",
    "    else:\n",
    "        attention_mask.append(False)\n",
    "\n",
    "print(f'{input_ids} \\n {labels} \\n {attention_mask}')\n",
    "print(f'{len(input_ids)} \\n {len(labels)} \\n {len(attention_mask)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2befb562-6274-48e7-9835-a235acd13193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '问题', '▁', '哪', '国', '[gMASK]', 'sop', '▁中国', 'eop']\n",
      "[30910, 31639, 30910, 55315, 54543, 64790, 64792, 34106, 64793, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      " [-100, -100, -100, -100, -100, -100, 34106, 64793, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100] \n",
      " [True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "30 \n",
      " 30 \n",
      " 30\n"
     ]
    }
   ],
   "source": [
    "max_len=30\n",
    "max_src_len=20\n",
    "max_tgt_len = max_len - max_src_len - 3\n",
    "\n",
    "src_tokens = tokenizer.tokenize(\"哪国\")\n",
    "\n",
    "prompt_tokens = tokenizer.tokenize(\"问题\")\n",
    "\n",
    "if len(src_tokens) > max_src_len - len(prompt_tokens):\n",
    "    src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]\n",
    "\n",
    "tgt_tokens = tokenizer.tokenize(\"中国\")\n",
    "\n",
    "if len(tgt_tokens) > max_tgt_len:\n",
    "    tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "\n",
    "tokens = prompt_tokens + src_tokens + [tokenizer.decode(si[\"[gMASK]\"]), tokenizer.decode(si['sop'])] + tgt_tokens + [tokenizer.decode(si['eop'])]\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_length = input_ids.index(si['sop'])\n",
    "mask_position = context_length - 1\n",
    "# prompt和问题部分不参与损失值计算\n",
    "labels = [-100] * context_length + input_ids[context_length + 1:]\n",
    "# 根据最大长度进行后填充\n",
    "pad_len = max_len - len(input_ids)\n",
    "input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "# 填充部分不参与损失值计算\n",
    "labels = labels + [-100] * (pad_len+1)\n",
    "# 区分有用的部分和填充的token\n",
    "attention_mask = []\n",
    "for input_id in input_ids:\n",
    "    if input_id != tokenizer.pad_token_id:\n",
    "        attention_mask.append(True)\n",
    "    else:\n",
    "        attention_mask.append(False)\n",
    "\n",
    "print(f'{input_ids} \\n {labels} \\n {attention_mask}')\n",
    "print(f'{len(input_ids)} \\n {len(labels)} \\n {len(attention_mask)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de758bc9-e216-44df-8118-318f6f66bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataSet(Dataset):\n",
    "    \"\"\"数据处理函数\"\"\"\n",
    "    def __init__(self, data_path, tokenizer, max_len, max_src_len, prompt_text):\n",
    "        # -3是因为需要拼接三个特殊字符[gMASK]、<sop>、<eop>\n",
    "        max_tgt_len = max_len - max_src_len - 3\n",
    "        self.all_data = []\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for i, line in enumerate(fh):\n",
    "                sample = json.loads(line.strip())\n",
    "                # chatglm的token不是中文的字，是词\n",
    "                # add_special_tokens = True时会在末位添加[\"[gMASK]\", \"<sop>\"]\n",
    "                src_tokens = tokenizer.tokenize(sample[\"text\"])\n",
    "                # print(sample[\"text\"])\n",
    "                # print(src_tokens)\n",
    "                prompt_tokens = tokenizer.tokenize(prompt_text)\n",
    "                # 根据限制的长度对输入进行截断\n",
    "                if len(src_tokens) > max_src_len - len(prompt_tokens):\n",
    "                    src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]\n",
    "\n",
    "                tgt_tokens = tokenizer.tokenize(sample[\"answer\"])\n",
    "                # 根据限制的长度对输入进行截断\n",
    "                if len(tgt_tokens) > max_tgt_len:\n",
    "                    tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "                # 问、答之间需要通过特殊字符进行分割，同时需要添加终止符\n",
    "                # [gMASK]与<sop>作为模型生成结果的起始标记，属于同一个block，\n",
    "                # 所以这两个token对应的在原始文本中所在的位置是一样的，具体可参考这个issue https://github.com/THUDM/ChatGLM-6B/issues/1313\n",
    "                # tokens = prompt_tokens + src_tokens + [\"[gMASK]\", \"<sop>\"] + tgt_tokens + [\"<eop>\"]\n",
    "                tokens = prompt_tokens + src_tokens + [tokenizer.gmask_token, tokenizer.bos_token] + tgt_tokens + [tokenizer.eos_token]\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                context_length = input_ids.index(tokenizer.bos_token_id)\n",
    "                mask_position = context_length - 1\n",
    "                # prompt和问题部分不参与损失值计算\n",
    "                labels = [-100] * context_length + input_ids[mask_position + 1:]\n",
    "                # 根据最大长度进行后填充\n",
    "                pad_len = max_len - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "                # 填充部分不参与损失值计算\n",
    "                labels = labels + [-100] * pad_len\n",
    "                # 区分有用的部分和填充的token\n",
    "                attention_mask = []\n",
    "                for input_id in input_ids:\n",
    "                    if input_id != tokenizer.pad_token_id:\n",
    "                        attention_mask.append(True)\n",
    "                    else:\n",
    "                        attention_mask.append(False)\n",
    "                self.all_data.append(\n",
    "                    {\"text\": sample[\"text\"], \"answer\": sample[\"answer\"], \"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.all_data[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b643120e-9f13-436c-8c38-022f57cc2369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[307, 30953, 30924, 4035, 1114, 17904, 14330, 293, 307, 883, 30953, 30912, 683, 1624, 30930]\n",
      "['▁I', \"'\", 'm', '▁feeling', '▁really', '▁anxious', '▁lately', '▁and', '▁I', '▁don', \"'\", 't', '▁know', '▁why', '.']\n",
      "[307, 30953, 30924, 4035, 1114, 17904, 14330, 293, 307, 883, 30953, 30912, 683, 1624, 30930]\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        sample=json.loads(line)\n",
    "        print(tokenizer.encode(sample['question'], add_special_tokens=False))\n",
    "        print(tokenizer.tokenize(sample['question']))\n",
    "        print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sample['question'])))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64da1570-4e5a-4a58-869a-646da5529a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample['question']\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample['response_j']\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length-0) + input_ids[pre_context_length+1:]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "285b1119-dddc-4253-8e67-da2137de3950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coll_fn(batch:list):\n",
    "    input_labels = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    for sample in batch:\n",
    "        # 实际上词表长度只有65024，所以int32就可以了 # attention_mask用bool就行\n",
    "        input_labels.append(torch.tensor(sample['input_ids'], dtype=torch.int32))\n",
    "        labels.append(torch.tensor(sample['labels'], dtype=torch.int32))\n",
    "        attention_mask.append(torch.tensor(sample['attention_mask'], dtype=torch.bool))\n",
    "    batch = {'input_ids':input_labels, 'labels':labels, 'attention_mask': attention_mask}\n",
    "    batch = {name:torch.stack(item) for name,item in batch.items()} #对于一个元素不为基本元素的list，需要使用stack方法\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41977962-673c-48e9-a612-ea7ccca3a52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuning_instruction=instruction_dataset(data_path, tokenizer, 128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f65157f-e12f-496c-bc76-17b44f8ead96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(finetuning_instruction, batch_size=2, shuffle=True, collate_fn=coll_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0534334-3fb9-4264-88b8-faa87357d53c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  307, 30953, 30924,  1709,  6555,   356,   552,  1934,  2862,   293,\n",
      "           307,   883, 30953, 30912,  1322,   878,   565,   552,  6518, 30930,\n",
      "         64790, 64792,  2641, 30953, 30917,  5550,   267,  6840,   290,   475,\n",
      "          1934,  2862,  2326,   293,   777,   588,   379,   457,   636,   331,\n",
      "          2310,   475,  6470, 30930,   577, 30953, 30917,  1447,   289,  3170,\n",
      "           343,  6337,  2114,   291,   479, 13278,   293,  9521, 30930, 64793,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  307, 30953, 30924,  4035,  1114,  6213,   293,   883, 30953, 30912,\n",
      "           683,  1624, 30930,  1266,   786,   330,  8486,   434, 30987, 64790,\n",
      "         64792,  1043,   786,   330,   853,  4723, 13986,   289,   475,  2032,\n",
      "          9682, 30932,  1331,  1045,  4685,   645, 30932, 27088, 30932,   293,\n",
      "         10419,  3075,  3008, 30930,  2641, 30953, 30917,  5550,   434,  2315,\n",
      "           293,   636,   331, 12656,   612, 25578,   400,  7375,   343,   740,\n",
      "           330, 13986,   289,   475,  7841, 30930, 64793,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       dtype=torch.int32), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  2641, 30953, 30917,  5550,   267,  6840,   290,   475,  1934,\n",
      "          2862,  2326,   293,   777,   588,   379,   457,   636,   331,  2310,\n",
      "           475,  6470, 30930,   577, 30953, 30917,  1447,   289,  3170,   343,\n",
      "          6337,  2114,   291,   479, 13278,   293,  9521, 30930, 64793,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          1043,   786,   330,   853,  4723, 13986,   289,   475,  2032,  9682,\n",
      "         30932,  1331,  1045,  4685,   645, 30932, 27088, 30932,   293, 10419,\n",
      "          3075,  3008, 30930,  2641, 30953, 30917,  5550,   434,  2315,   293,\n",
      "           636,   331, 12656,   612, 25578,   400,  7375,   343,   740,   330,\n",
      "         13986,   289,   475,  7841, 30930, 64793,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,  -100]],\n",
      "       dtype=torch.int32), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]])}\n"
     ]
    }
   ],
   "source": [
    "for item in dataloader:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "495d7922-6af5-48d1-a7a2-1e52e9bddbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 277, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "for item in finetuning_instruction:\n",
    "    print([len(item[it]) for it in item])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085378ef-a096-4d5a-870a-fe93ada57951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
