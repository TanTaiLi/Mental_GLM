{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824b1416-8e64-4e49-a6d4-7947f3f9b614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#这是一段基于Loss_detect得到的model训练代码，重点在优化训练资源消耗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d1acb-1201-44a0-91e2-00230f6e72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_path = '/root/autodl-tmp/weights/chatglm3-6b'\n",
    "    output_dir = '/root/autodl-tmp/checkpoints/glm3'\n",
    "    \n",
    "    num_train_epochs = 5\n",
    "    max_train_steps = None\n",
    "    \n",
    "    batch_size = 2\n",
    "    max_tokens = 512\n",
    "    max_query = 256\n",
    "    \n",
    "    lr = 1e-5\n",
    "    warm_up_steps = 200\n",
    "    \n",
    "    data_path = '/root/autodl-tmp/dataset/OESD-GG-zh_cn-1/single_query.jsonl'\n",
    "    query_key = 'User'\n",
    "    answer_key = 'Assisstant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e147d2cf-4919-4b24-9e43-3b120ab9c537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 16:47:29,055] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import deepspeed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757dee41-165a-4920-b7c2-e771adbf9e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a1bd0d-3bb6-432b-b5f9-c77b982dedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/root/tuning_space/Components/')\n",
    "import interact\n",
    "import model_tools\n",
    "from Static import prompt_dict, st, si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a9d856-a53d-41cd-9ca3-0186c3be68d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/root/autodl-fs/weights/chatglm3-6b'\n",
    "data_path = '/root/autodl-tmp/dataset/psychology-dataset/data/train.jsonl'\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # lora的目标位置，具体有哪些可选项可打印出源码中的key_list 注意不同的模型中的定义名称不同\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc331aa4-9992-4ab2-8822-8ee8d3a1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006381988525390625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b8b605a2ff4a99bed9a988ec3b34b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.65 s, sys: 20.9 s, total: 27.6 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(CFG.model_path, trust_remote_code=True).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528572d7-82a1-4c3e-a9f8-2c1663c64db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6243584000\n",
      "Trainable Parameters: 6243584000\n",
      "Percentage of Trainable Parameters: 100.00%\n",
      "conducting peft lora ---------------\n",
      "Total Parameters: 6247483392\n",
      "Trainable Parameters: 3899392\n",
      "Percentage of Trainable Parameters: 0.06%\n"
     ]
    }
   ],
   "source": [
    "#施加peft lora\n",
    "model_tools.model_profile(model)\n",
    "print('conducting peft lora ---------------')\n",
    "model = get_peft_model(model, config)\n",
    "model_tools.model_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64da1570-4e5a-4a58-869a-646da5529a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class instruction_dataset(Dataset):\n",
    "    def __init__(self, data_path:'str', tokenizer, truncate_length, max_query_length, query_key, answer_key):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(data_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sample=json.loads(line)\n",
    "                # input_ids的结构应该为：prompt_tokens, src_tokens, [gMASK], <sop>, tgt_tokens, <eop>, [PAD]... \n",
    "                # 或者简化一点，即为 query, [gMASK], <sop>, answer, <eop>, [PAD]... \n",
    "                # padding的目的是为了对齐各个instance，以组成batch（当然batch_size=1时其实没必要）\n",
    "                # 总体的input_ids的长度不超过truncate_length，其中query的长度不超过max_query_length，同理可以计算出answer的最大长度\n",
    "                max_answer_length = truncate_length - max_query_length - 3\n",
    "                \n",
    "                # 判断query的长度\n",
    "                query = sample[query_key]\n",
    "                query_ids = tokenizer.encode(query, add_special_tokens=False)\n",
    "                if len(query_ids) > max_query_length:\n",
    "                    query_ids = query_ids[:max_query_length]\n",
    "                \n",
    "                # 判断answer的长度\n",
    "                answer = sample[answer_key]\n",
    "                answer_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "                if len(answer) > max_answer_length:\n",
    "                    answer_ids = answer_ids[:max_answer_length]\n",
    "                    \n",
    "                # 合并\n",
    "                input_ids = query_ids + [si['[gMASK]']] + [si['sop']] + answer_ids + [si['eop']]\n",
    "                pre_context_length = input_ids.index(si['sop'])\n",
    "                end_answer_index = input_ids.index(si['eop'])\n",
    "                \n",
    "                # padding\n",
    "                padding_length=truncate_length-len(input_ids)\n",
    "                input_ids+=padding_length*[tokenizer.pad_token_id]\n",
    "                \n",
    "                # 制作labels；其中query部分，pad部分均不参与loss的计算 # 因为需要整体向左移动，所以要少填充一个\n",
    "                labels = [-100] * (pre_context_length+1) + input_ids[pre_context_length+1: end_answer_index+1]\n",
    "                labels = labels + [-100] * (truncate_length-len(labels))\n",
    "                \n",
    "                # 制作attention_mask\n",
    "                eop_position = input_ids.index(si['eop'])+1\n",
    "                attention_mask = [True]*eop_position\n",
    "                attention_mask += [False]*(truncate_length-len(attention_mask))\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'query' : query,\n",
    "                    'answer' : answer,\n",
    "                    'input_ids' : input_ids,\n",
    "                    'labels' : labels,\n",
    "                    'attention_mask' : attention_mask,\n",
    "                })\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = self.examples[item]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285b1119-dddc-4253-8e67-da2137de3950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coll_fn(batch:list):\n",
    "    input_labels = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    for sample in batch:\n",
    "        # 实际上词表长度只有65024，所以int32就可以了 # attention_mask用bool就行 (我收回我的画，完全是玄学)\n",
    "        input_labels.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n",
    "        labels.append(torch.tensor(sample['labels'], dtype=torch.long))\n",
    "        attention_mask.append(torch.tensor(sample['attention_mask'], dtype=torch.float64)) #, dtype=torch.bool\n",
    "    batch = {'input_ids':input_labels, 'labels':labels, 'attention_mask': attention_mask}\n",
    "    batch = {name:torch.stack(item).cuda() for name,item in batch.items()} #对于一个元素不为基本元素的list，需要使用stack方法\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41977962-673c-48e9-a612-ea7ccca3a52f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.22 s, sys: 201 ms, total: 9.42 s\n",
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "finetuning_instruction = instruction_dataset(CFG.data_path, tokenizer, CFG.max_tokens, CFG.max_query, CFG.query_key, CFG.answer_key)\n",
    "instruction_loader = DataLoader(finetuning_instruction, batch_size=CFG.batch_size, shuffle=True, collate_fn=coll_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0607dd-e9ae-4233-8d09-679bf466f26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义参数优化器 & 学习率优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=CFG.warm_up_steps,\n",
    "        num_training_steps=(len(instruction_loader) * CFG.num_train_epochs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b96b9-1405-4f3e-8470-f3f24494249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_config_path = \"ds_config.json\"\n",
    "model, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    config_params=deepspeed_config_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab74ae-c35c-47f1-91be-a49b2a6a5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_tuning():\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(CFG.num_train_epochs):\n",
    "        for step, batch in tqdm(enumerate(instruction_loader)):\n",
    "            # 前向传播 & 计算loss (使用fp16)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                status['loss'].append(loss)\n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "            # 优化器调度\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ce2e056-0c60-47bc-98db-6c78c3210455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_tuning():\n",
    "    status=dict()\n",
    "    status['loss']=[]\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(CFG.num_train_epochs):\n",
    "        for step, batch in tqdm(enumerate(instruction_loader)):\n",
    "            # 前向传播 & 计算loss (使用fp16)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                status['loss'].append(loss)\n",
    "                if step%10==0:\n",
    "                    print(loss)\n",
    "                if CFG.max_train_steps!=None:\n",
    "                    if step>=CFG.max_train_steps:\n",
    "                        return status\n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "            # 优化器调度\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd73771-dac7-4274-8b99-d5d6bda33081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#status=lora_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d849b98-412b-4b42-957e-d357eb9da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型的保存\n",
    "#model.save_pretrained(CFG.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78345378-e4c2-4fc0-98ad-1c2e606246ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls $CFG.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8cf47-8fa4-416f-86da-5109af27e1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "loss = [value.item() for times, value in enumerate(status['loss'])]\n",
    "\n",
    "# 创建图表\n",
    "plt.figure(figsize=(10, 5))  # 可以调整图表大小\n",
    "plt.scatter(range(len(loss)), loss, marker='o')  # 使用圆圈标记每个点\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Loss over Steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 显示网格\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
